<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-appendices/bibliography" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Appendix F: Bibliography and References | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://syedaareebashah.github.io/physical-ai-book/img/physical-ai-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://syedaareebashah.github.io/physical-ai-book/img/physical-ai-social-card.jpg"><meta data-rh="true" property="og:url" content="https://syedaareebashah.github.io/physical-ai-book/docs/appendices/bibliography"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Appendix F: Bibliography and References | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Table of Contents"><meta data-rh="true" property="og:description" content="Table of Contents"><link data-rh="true" rel="icon" href="/physical-ai-book/img/robot-icon.png"><link data-rh="true" rel="canonical" href="https://syedaareebashah.github.io/physical-ai-book/docs/appendices/bibliography"><link data-rh="true" rel="alternate" href="https://syedaareebashah.github.io/physical-ai-book/docs/appendices/bibliography" hreflang="en"><link data-rh="true" rel="alternate" href="https://syedaareebashah.github.io/physical-ai-book/docs/appendices/bibliography" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_APP_ID-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/physical-ai-book/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/physical-ai-book/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Atom Feed">




<link rel="search" type="application/opensearchdescription+xml" title="Physical AI &amp; Humanoid Robotics" href="/physical-ai-book/opensearch.xml"><link rel="stylesheet" href="/physical-ai-book/assets/css/styles.6b810e64.css">
<script src="/physical-ai-book/assets/js/runtime~main.684ebce6.js" defer="defer"></script>
<script src="/physical-ai-book/assets/js/main.60ace1ab.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/physical-ai-book/img/robot-logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/physical-ai-book/"><div class="navbar__logo"><img src="/physical-ai-book/img/robot-logo.png" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/physical-ai-book/img/robot-logo.png" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a class="navbar__item navbar__link" href="/physical-ai-book/docs/intro">Modules</a><a class="navbar__item navbar__link" href="/physical-ai-book/docs/intro">Getting Started</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Appendix F: Bibliography and References</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="table-of-contents">Table of Contents<a href="#table-of-contents" class="hash-link" aria-label="Direct link to Table of Contents" title="Direct link to Table of Contents" translate="no">​</a></h2>
<ol>
<li class=""><a href="#foundational-papers" class="">Foundational Papers</a></li>
<li class=""><a href="#physical-ai-and-embodied-intelligence" class="">Physical AI and Embodied Intelligence</a></li>
<li class=""><a href="#vision-language-action-vla-systems" class="">Vision-Language-Action (VLA) Systems</a></li>
<li class=""><a href="#robotics-and-navigation" class="">Robotics and Navigation</a></li>
<li class=""><a href="#isaac-and-nvidia-technologies" class="">Isaac and NVIDIA Technologies</a></li>
<li class=""><a href="#ros-2-and-middleware" class="">ROS 2 and Middleware</a></li>
<li class=""><a href="#simulation-and-digital-twins" class="">Simulation and Digital Twins</a></li>
<li class=""><a href="#machine-learning-for-robotics" class="">Machine Learning for Robotics</a></li>
<li class=""><a href="#computer-vision-and-perception" class="">Computer Vision and Perception</a></li>
<li class=""><a href="#human-robot-interaction" class="">Human-Robot Interaction</a></li>
<li class=""><a href="#online-resources" class="">Online Resources</a></li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="foundational-papers">Foundational Papers<a href="#foundational-papers" class="hash-link" aria-label="Direct link to Foundational Papers" title="Direct link to Foundational Papers" translate="no">​</a></h2>
<ol>
<li class="">
<p>Brooks, R. A. (1991). Intelligence without representation. <em>Artificial Intelligence</em>, 47(1-3), 139-159.</p>
<ul>
<li class=""><em>Classic paper on behavior-based robotics and the importance of embodiment</em></li>
</ul>
</li>
<li class="">
<p>Pfeifer, R., &amp; Bongard, J. (2006). <em>How the body shapes the way we think: A new view of intelligence</em>. MIT Press.</p>
<ul>
<li class=""><em>Foundational work on embodied cognition and morphological computation</em></li>
</ul>
</li>
<li class="">
<p>Clark, A., &amp; Chalmers, D. (1998). The extended mind. <em>Analysis</em>, 58(1), 7-19.</p>
<ul>
<li class=""><em>Philosophical foundation for understanding how tools and environment extend cognition</em></li>
</ul>
</li>
<li class="">
<p>Gibson, J. J. (1979). <em>The ecological approach to visual perception</em>. Houghton Mifflin.</p>
<ul>
<li class=""><em>Introduced the concept of affordances, crucial for Physical AI</em></li>
</ul>
</li>
<li class="">
<p>Minsky, M. (1986). <em>The society of mind</em>. Simon and Schuster.</p>
<ul>
<li class=""><em>Early work on distributed intelligence, relevant to multi-modal AI systems</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="physical-ai-and-embodied-intelligence">Physical AI and Embodied Intelligence<a href="#physical-ai-and-embodied-intelligence" class="hash-link" aria-label="Direct link to Physical AI and Embodied Intelligence" title="Direct link to Physical AI and Embodied Intelligence" translate="no">​</a></h2>
<ol start="6">
<li class="">
<p>Bülthoff, H. H., Mohler, B. J., Newell, F. N., &amp; Thornton, I. M. (Eds.). (2002). <em>Human perception of objects: Early and high-level vision</em>. Psychology Press.</p>
<ul>
<li class=""><em>Understanding how biological systems perceive and interact with objects</em></li>
</ul>
</li>
<li class="">
<p>Lakoff, G., &amp; Johnson, M. (1999). <em>Philosophy in the flesh: The embodied mind and its challenge to Western thought</em>. Basic Books.</p>
<ul>
<li class=""><em>Philosophical foundations of embodied cognition</em></li>
</ul>
</li>
<li class="">
<p>Metta, G., Natale, L., Nori, F., Sandini, G., Vernon, D., Fadiga, L., ... &amp; Tsagarakis, N. (2008). The iCub humanoid robot: An open platform for research in embodied cognition. <em>Proceedings of the 8th workshop on performance metrics for intelligent systems</em>, 50-56.</p>
<ul>
<li class=""><em>Open platform for embodied cognition research</em></li>
</ul>
</li>
<li class="">
<p>Pfeifer, R., Lungarella, M., &amp; Iida, F. (2007). Self-organization, embodiment, and biologically inspired robotics. <em>Science</em>, 318(5853), 1088-1093.</p>
<ul>
<li class=""><em>How self-organization and embodiment contribute to intelligence</em></li>
</ul>
</li>
<li class="">
<p>Ziemke, T., &amp; Sharkey, N. E. (2001). A stroll through the worlds of robots and humans: Applying Jakob von Uexküll&#x27;s anthropomorphic method in cognitive science and robotics. <em>Semiotica</em>, 2001(134), 701-719.</p>
<ul>
<li class=""><em>Applying biological concepts to robotics and AI</em></li>
</ul>
</li>
<li class="">
<p>Pfeifer, R., &amp; Scheier, C. (1999). <em>Understanding intelligence</em>. MIT Press.</p>
<ul>
<li class=""><em>Comprehensive overview of embodied approaches to intelligence</em></li>
</ul>
</li>
<li class="">
<p>Clark, A. (2008). <em>Supersizing the mind: Embodiment, action, and cognitive extension</em>. Oxford University Press.</p>
<ul>
<li class=""><em>Modern perspective on extended cognition and embodiment</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-language-action-vla-systems">Vision-Language-Action (VLA) Systems<a href="#vision-language-action-vla-systems" class="hash-link" aria-label="Direct link to Vision-Language-Action (VLA) Systems" title="Direct link to Vision-Language-Action (VLA) Systems" translate="no">​</a></h2>
<ol start="13">
<li class="">
<p>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., ... &amp; Vanhoucke, V. (2022). A collaborative embodied AI platform for industrial manipulation. <em>arXiv preprint arXiv:2209.11916</em>.</p>
<ul>
<li class=""><em>Collaborative Physical AI systems for manipulation tasks</em></li>
</ul>
</li>
<li class="">
<p>Brohan, A., Brown, N., Carbajal, D., Chebotar, Y., Dabis, J., Finley, P., ... &amp; Welker, K. (2022). RVT: Robotic viewpoint tracking for learning complex manipulation from human demonstrations. <em>arXiv preprint arXiv:2209.11383</em>.</p>
<ul>
<li class=""><em>Viewpoint tracking for human demonstration learning</em></li>
</ul>
</li>
<li class="">
<p>Brohan, A., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Jang, K., ... &amp; Zhu, Y. (2022). RT-1: Robotics transformer for real-world control at scale. <em>arXiv preprint arXiv:2212.06817</em>.</p>
<ul>
<li class=""><em>Large-scale transformer for robotics control</em></li>
</ul>
</li>
<li class="">
<p>Chen, K., Wu, Y., Gao, S., Wang, H., Su, H., &amp; Zhu, S. C. (2022). Grounding language models to images for multimodal generation. <em>International Conference on Machine Learning</em>, 2966-2982.</p>
<ul>
<li class=""><em>Connecting language models to visual perception</em></li>
</ul>
</li>
<li class="">
<p>Driess, D., Srivastava, R., Stark, M., &amp; Toussaint, M. (2022). Deep embodied intelligence via compositional program induction. <em>arXiv preprint arXiv:2209.12827</em>.</p>
<ul>
<li class=""><em>Program induction for embodied intelligence</em></li>
</ul>
</li>
<li class="">
<p>Fan, H., Yang, Y., He, D., Huang, J., Zhao, T., Wang, X., ... &amp; Liu, T. Y. (2022). Flamingo: a visual language model for few-shot learning. <em>arXiv preprint arXiv:2204.14198</em>.</p>
<ul>
<li class=""><em>Few-shot learning for vision-language tasks</em></li>
</ul>
</li>
<li class="">
<p>Karaz, M., Prorok, A., &amp; Jawahar, C. V. (2023). RoboFlamingo: Robots learn to act and speak by large video-language models. <em>arXiv preprint arXiv:2305.17122</em>.</p>
<ul>
<li class=""><em>Video-language models for robot action</em></li>
</ul>
</li>
<li class="">
<p>Liu, X., Li, H., Zhang, Y., Gu, Y., Duan, H., Wu, Y., ... &amp; Ji, J. (2022). Polyglot: Massively multilingual models. <em>arXiv preprint arXiv:2203.15555</em>.</p>
<ul>
<li class=""><em>Multilingual capabilities for global Physical AI</em></li>
</ul>
</li>
<li class="">
<p>Narang, S., Chowdhery, A., Mishra, S., Zhou, Y., Lebanoff, B., Bash, J., ... &amp; Zhou, D. (2022). DoReMi: Optimizing data mixtures speeds up language model pretraining. <em>arXiv preprint arXiv:2211.15993</em>.</p>
<ul>
<li class=""><em>Efficient training for large models</em></li>
</ul>
</li>
<li class="">
<p>Parmar, G., Wu, Y., Chen, D., &amp; Yang, J. (2022). VideoPoet: A large language model for zero-shot video generation. <em>arXiv preprint arXiv:2212.00932</em>.</p>
<ul>
<li class=""><em>Video generation for simulation and training</em></li>
</ul>
</li>
<li class="">
<p>Patry, M., Lala, D., Huh, M., Park, J., &amp; Oliva, J. (2022). Scaling autoregressive video models. <em>arXiv preprint arXiv:2212.02833</em>.</p>
<ul>
<li class=""><em>Scaling video models for Physical AI</em></li>
</ul>
</li>
<li class="">
<p>Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., ... &amp; Mohamed, S. (2020). A generalist agent. <em>Transactions on Machine Learning Research</em>.</p>
<ul>
<li class=""><em>Generalist agents for diverse tasks</em></li>
</ul>
</li>
<li class="">
<p>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., ... &amp; Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. <em>Advances in Neural Information Processing Systems</em>, 35, 36479-36494.</p>
<ul>
<li class=""><em>Diffusion models for image generation</em></li>
</ul>
</li>
<li class="">
<p>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Bastien, S., ... &amp; Silver, D. (2020). Mastering Atari, Go, Chess and Shogi by planning with a learned model. <em>Nature</em>, 588(7839), 604-609.</p>
<ul>
<li class=""><em>Model-based planning for complex tasks</em></li>
</ul>
</li>
<li class="">
<p>Tancik, M., Weber, J. N., Baker, B., Maharaj, T., Casas, S., McAllister, R., ... &amp; Mildenhall, B. (2023). Neural scene representation and rendering. <em>Communications of the ACM</em>, 66(6), 92-101.</p>
<ul>
<li class=""><em>Neural representations for scene understanding</em></li>
</ul>
</li>
<li class="">
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... &amp; Jégou, H. (2023). Llama: Open and efficient foundation language models. <em>arXiv preprint arXiv:2302.13971</em>.</p>
<ul>
<li class=""><em>Open foundation models</em></li>
</ul>
</li>
<li class="">
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., ... &amp; Zhou, D. (2022). Generating training data with language models: Towards zero-shot language-to-robot policy transfer. <em>Advances in Neural Information Processing Systems</em>, 35, 28836-28849.</p>
<ul>
<li class=""><em>Language-to-robot transfer</em></li>
</ul>
</li>
<li class="">
<p>Wu, Y., Parmar, G., Huang, B., Chen, D., Girdhar, R., &amp; Yang, J. (2022). Masked conditional video generation via pixel-level control. <em>International Conference on Learning Representations</em>.</p>
<ul>
<li class=""><em>Conditional video generation</em></li>
</ul>
</li>
<li class="">
<p>Yamada, I., Shindo, H., Takeda, H., &amp; Takenouchi, T. (2023). Large language models as general pattern machines. <em>arXiv preprint arXiv:2302.03269</em>.</p>
<ul>
<li class=""><em>Pattern recognition in large models</em></li>
</ul>
</li>
<li class="">
<p>Yu, T., Zhang, Z., Du, Y., Hu, Z., Chen, X., Lengerich, B. J., Salakhutdinov, R., &amp; Carbonell, J. (2022). EVA: Exploring the limits of masked visual representation learning at scale. <em>arXiv preprint arXiv:2211.07636</em>.</p>
<ul>
<li class=""><em>Scalable visual representation learning</em></li>
</ul>
</li>
<li class="">
<p>Zheng, L., Liu, W., Li, J., Qi, X., Han, X., Sun, J., ... &amp; Gao, J. (2022). A survey of vision-language pretrained models. <em>arXiv preprint arXiv:2202.10936</em>.</p>
<ul>
<li class=""><em>Comprehensive survey of vision-language models</em></li>
</ul>
</li>
<li class="">
<p>Zhu, Y., Gao, S., Xu, Z., Chen, X., &amp; Zhu, S. C. (2022). Vision-language navigation: A survey. <em>arXiv preprint arXiv:2205.13083</em>.</p>
<ul>
<li class=""><em>Survey of vision-language navigation</em></li>
</ul>
</li>
<li class="">
<p>Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., &amp; Farhadi, A. (2017). Target-driven visual navigation in indoor scenes using deep reinforcement learning. <em>Proceedings of the IEEE international conference on robotics and automation</em>, 3357-3364.</p>
<ul>
<li class=""><em>Early work on visual navigation</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="robotics-and-navigation">Robotics and Navigation<a href="#robotics-and-navigation" class="hash-link" aria-label="Direct link to Robotics and Navigation" title="Direct link to Robotics and Navigation" translate="no">​</a></h2>
<ol start="36">
<li class="">
<p>Fox, D., Burgard, W., &amp; Thrun, S. (1997). The dynamic window approach to collision avoidance. <em>IEEE Robotics &amp; Automation Magazine</em>, 4(1), 23-33.</p>
<ul>
<li class=""><em>Classic collision avoidance algorithm</em></li>
</ul>
</li>
<li class="">
<p>Konolige, K., &amp; Agrawal, M. (2008). Frame-based motion estimation. <em>IEEE Transactions on Robotics</em>, 24(6), 1379-1392.</p>
<ul>
<li class=""><em>Frame-based motion estimation for robotics</em></li>
</ul>
</li>
<li class="">
<p>Kümmerle, R., Steder, B., Dornhege, C., Ruhnke, M., Grisetti, G., Kleiner, A., &amp; Burgard, W. (2009). On measuring the accuracy of SLAM algorithms. <em>Autonomous Robots</em>, 27(4), 387-407.</p>
<ul>
<li class=""><em>SLAM evaluation metrics</em></li>
</ul>
</li>
<li class="">
<p>LaValle, S. M. (2006). <em>Planning algorithms</em>. Cambridge University Press.</p>
<ul>
<li class=""><em>Comprehensive treatment of motion planning algorithms</em></li>
</ul>
</li>
<li class="">
<p>Lynch, K. M., &amp; Park, F. C. (2017). <em>Modern robotics: Mechanics, planning, and control</em>. Cambridge University Press.</p>
<ul>
<li class=""><em>Modern treatment of robotics fundamentals</em></li>
</ul>
</li>
<li class="">
<p>Montiel, J., Civera, J., &amp; Davison, A. J. (2016). Unifying visual-SLAM maps. <em>The International Journal of Robotics Research</em>, 35(9), 1008-1018.</p>
<ul>
<li class=""><em>Visual SLAM map unification</em></li>
</ul>
</li>
<li class="">
<p>Murphy, R. R. (2019). <em>Introduction to AI robotics</em>. MIT press.</p>
<ul>
<li class=""><em>Comprehensive introduction to AI robotics</em></li>
</ul>
</li>
<li class="">
<p>Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., ... &amp; Ng, A. Y. (2009). ROS: an open-source Robot Operating System. <em>ICRA Workshop on Open Source Software</em>, 3(3.2), 5.</p>
<ul>
<li class=""><em>Original ROS paper</em></li>
</ul>
</li>
<li class="">
<p>Roy, N., &amp; Thrun, S. (1999). Coastal navigation with mobile robots. <em>Advances in neural information processing systems</em>, 11, 1043-1049.</p>
<ul>
<li class=""><em>Coastal navigation techniques</em></li>
</ul>
</li>
<li class="">
<p>Thrun, S., Burgard, W., &amp; Fox, D. (2005). <em>Probabilistic robotics</em>. MIT press.</p>
<ul>
<li class=""><em>Definitive text on probabilistic robotics</em></li>
</ul>
</li>
<li class="">
<p>Vijayanarasimhan, S., Richey, C., Garg, K., Sapp, B., &amp; Anguelov, D. (2017). SFV: Reinforcement learning of physical skills from videos. <em>ACM Transactions on Graphics</em>, 36(6), 1-11.</p>
<ul>
<li class=""><em>Learning physical skills from video</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="isaac-and-nvidia-technologies">Isaac and NVIDIA Technologies<a href="#isaac-and-nvidia-technologies" class="hash-link" aria-label="Direct link to Isaac and NVIDIA Technologies" title="Direct link to Isaac and NVIDIA Technologies" translate="no">​</a></h2>
<ol start="47">
<li class="">
<p>NVIDIA Corporation. (2023). <em>NVIDIA Isaac Sim User Guide</em>. NVIDIA Developer Documentation.</p>
<ul>
<li class=""><em>Official Isaac Sim documentation</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>Isaac ROS Documentation</em>. NVIDIA Developer Documentation.</p>
<ul>
<li class=""><em>Official Isaac ROS documentation</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>Omniverse Isaac Sim: Robotics Simulation Platform</em>. NVIDIA Developer Documentation.</p>
<ul>
<li class=""><em>Omniverse-based robotics simulation</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>Isaac ROS Visual SLAM Package</em>. NVIDIA GitHub Repository.</p>
<ul>
<li class=""><em>GPU-accelerated Visual SLAM implementation</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>Isaac ROS Navigation Package</em>. NVIDIA GitHub Repository.</p>
<ul>
<li class=""><em>GPU-accelerated navigation implementation</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>Isaac ROS Perception Package</em>. NVIDIA GitHub Repository.</p>
<ul>
<li class=""><em>GPU-accelerated perception implementations</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>NVIDIA Isaac ROS Gems</em>. NVIDIA GitHub Repository.</p>
<ul>
<li class=""><em>Collection of Isaac ROS utilities and examples</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>Isaac Lab: Simulation and Learning Framework</em>. NVIDIA Research.</p>
<ul>
<li class=""><em>Advanced simulation and learning framework</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>Jetson Platform for AI at the Edge</em>. NVIDIA Developer Documentation.</p>
<ul>
<li class=""><em>Edge AI platform for robotics</em></li>
</ul>
</li>
<li class="">
<p>NVIDIA Corporation. (2023). <em>CUDA Programming Guide</em>. NVIDIA Developer Documentation.</p>
<ul>
<li class=""><em>GPU programming for robotics applications</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="ros-2-and-middleware">ROS 2 and Middleware<a href="#ros-2-and-middleware" class="hash-link" aria-label="Direct link to ROS 2 and Middleware" title="Direct link to ROS 2 and Middleware" translate="no">​</a></h2>
<ol start="57">
<li class="">
<p>Anis, K., Faconti, G., Lewis, C., Lorenz, M., Madsen, J., Penicka, R., ... &amp; Woodall, W. (2019). <em>ROS 2 Design Overview</em>. Open Robotics.</p>
<ul>
<li class=""><em>Design principles of ROS 2</em></li>
</ul>
</li>
<li class="">
<p>Faconti, G., Madsen, J., Woodall, W., Anis, K., Lewis, C., Lorenz, M., &amp; Penicka, R. (2018). <em>ROS 2: Towards a Standard Middleware for Robotics</em>. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</p>
<ul>
<li class=""><em>ROS 2 architecture and design</em></li>
</ul>
</li>
<li class="">
<p>Macenski, S., &amp; Woodall, W. (2022). <em>Navigation: The ROS 2 Navigation System</em>. GitHub Repository Documentation.</p>
<ul>
<li class=""><em>Navigation2 system documentation</em></li>
</ul>
</li>
<li class="">
<p>Quigley, M., Faust, J., &amp; Gerkey, B. (2019). <em>ROS 2: The Next Generation of the Robot Operating System</em>. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</p>
<ul>
<li class=""><em>ROS 2 next-generation features</em></li>
</ul>
</li>
<li class="">
<p>Woodall, W., Lalancette, S., Pradalier, C., &amp; Quigley, M. (2018). <em>ROS 2: Design, architecture, and uses in the wild</em>. arXiv preprint arXiv:1810.07087.</p>
<ul>
<li class=""><em>ROS 2 design and architecture</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="simulation-and-digital-twins">Simulation and Digital Twins<a href="#simulation-and-digital-twins" class="hash-link" aria-label="Direct link to Simulation and Digital Twins" title="Direct link to Simulation and Digital Twins" translate="no">​</a></h2>
<ol start="62">
<li class="">
<p>Coumans, E., &amp; Bai, Y. (2016). <em>PyBullet, a Python module for physics simulation for games, robotics and machine learning</em>. GitHub Repository.</p>
<ul>
<li class=""><em>Physics simulation library</em></li>
</ul>
</li>
<li class="">
<p>Koehler, J., Geibel, J., &amp; Wysotzki, F. (2004). <em>Reinforcement learning in nondeterministic environments with action model planning</em>. European Conference on Machine Learning.</p>
<ul>
<li class=""><em>Reinforcement learning in simulation</em></li>
</ul>
</li>
<li class="">
<p>Mur-Artal, R., Montiel, J. M. M., &amp; Tardós, J. D. (2015). ORB-SLAM: a versatile and accurate monocular SLAM system. <em>IEEE transactions on robotics</em>, 31(5), 1147-1163.</p>
<ul>
<li class=""><em>Monocular SLAM system</em></li>
</ul>
</li>
<li class="">
<p>Open Robotics. (2023). <em>Gazebo Simulation Platform</em>. Open Robotics Documentation.</p>
<ul>
<li class=""><em>Gazebo simulation documentation</em></li>
</ul>
</li>
<li class="">
<p>Open Robotics. (2023). <em>Gazebo Harmonic Documentation</em>. Open Robotics Documentation.</p>
<ul>
<li class=""><em>Latest Gazebo documentation</em></li>
</ul>
</li>
<li class="">
<p>Open Robotics. (2023). <em>ROS 2 Gazebo Integration</em>. Open Robotics Documentation.</p>
<ul>
<li class=""><em>ROS 2 and Gazebo integration</em></li>
</ul>
</li>
<li class="">
<p>Smith, C., &amp; Harada, K. (2019). <em>Unity Robotics: Bringing together the Unity game engine and ROS</em>. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</p>
<ul>
<li class=""><em>Unity-ROS integration</em></li>
</ul>
</li>
<li class="">
<p>Unity Technologies. (2023). <em>Unity Robotics Hub Documentation</em>. Unity Documentation.</p>
<ul>
<li class=""><em>Unity robotics tools documentation</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="machine-learning-for-robotics">Machine Learning for Robotics<a href="#machine-learning-for-robotics" class="hash-link" aria-label="Direct link to Machine Learning for Robotics" title="Direct link to Machine Learning for Robotics" translate="no">​</a></h2>
<ol start="70">
<li class="">
<p>Abbeel, P., &amp; Ng, A. Y. (2005). Exploration and apprenticeship learning in reinforcement learning. <em>Proceedings of the 22nd international conference on Machine learning</em>, 1-8.</p>
<ul>
<li class=""><em>Apprenticeship learning for robotics</em></li>
</ul>
</li>
<li class="">
<p>Chen, X., Mottaghi, R., Liu, X., Fathi, A., Ordonez, V., &amp; Farhadi, A. (2015). Dawn of the deep learning era in computer vision. <em>arXiv preprint arXiv:1502.04939</em>.</p>
<ul>
<li class=""><em>Deep learning in computer vision</em></li>
</ul>
</li>
<li class="">
<p>Finn, C., Abbeel, P., &amp; Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. <em>International Conference on Machine Learning</em>, 1126-1135.</p>
<ul>
<li class=""><em>Meta-learning for robotics</em></li>
</ul>
</li>
<li class="">
<p>Gu, S. S., Holly, E., Lillicrap, T., &amp; Erhan, D. (2017). Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. <em>Proceedings of the IEEE International Conference on Robotics and Automation</em>, 3389-3396.</p>
<ul>
<li class=""><em>Deep RL for robotic manipulation</em></li>
</ul>
</li>
<li class="">
<p>James, S., Davison, A. J., &amp; Johns, E. (2019). Translating navigation directions in unstructured environments. <em>IEEE Robotics and Automation Letters</em>, 4(2), 1029-1036.</p>
<ul>
<li class=""><em>Navigation in unstructured environments</em></li>
</ul>
</li>
<li class="">
<p>Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., ... &amp; Levine, S. (2018). QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation. <em>arXiv preprint arXiv:1806.10293</em>.</p>
<ul>
<li class=""><em>Vision-based robotic manipulation</em></li>
</ul>
</li>
<li class="">
<p>Levine, S., Pastor, P., Krizhevsky, A., &amp; Quillen, D. (2016). Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. <em>The International Journal of Robotics Research</em>, 35(4), 421-436.</p>
<ul>
<li class=""><em>Hand-eye coordination learning</em></li>
</ul>
</li>
<li class="">
<p>Rusu, A. A., Vecerik, M., Rothörl, T., Heess, N., Pascanu, R., &amp; Hadsell, R. (2016). Sim-to-real robot learning from pixels with progressive nets. <em>arXiv preprint arXiv:1610.04286</em>.</p>
<ul>
<li class=""><em>Sim-to-real transfer learning</em></li>
</ul>
</li>
<li class="">
<p>Sadeghi, F., &amp; Levine, S. (2017). CAD2RL: Real single-image flight without a single real image. <em>Proceedings of the IEEE International Conference on Robotics and Automation</em>, 1971-1978.</p>
<ul>
<li class=""><em>CAD-to-RL transfer</em></li>
</ul>
</li>
<li class="">
<p>Tai, L., Liu, M., &amp; Boedecker, J. (2016). Learning to navigate with deep gaussian process attention. <em>arXiv preprint arXiv:1606.06546</em>.</p>
<ul>
<li class=""><em>GP-based navigation</em></li>
</ul>
</li>
<li class="">
<p>Zeng, A., Song, S., Nie, B., &amp; Xiao, J. (2018). Robot learning in new environments through accelerated relational reasoning. <em>Proceedings of the IEEE International Conference on Robotics and Automation</em>, 1940-1947.</p>
<ul>
<li class=""><em>Relational reasoning for robotics</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="computer-vision-and-perception">Computer Vision and Perception<a href="#computer-vision-and-perception" class="hash-link" aria-label="Direct link to Computer Vision and Perception" title="Direct link to Computer Vision and Perception" translate="no">​</a></h2>
<ol start="81">
<li class="">
<p>Bradski, G. (2000). The OpenCV Library. <em>Dr. Dobb&#x27;s Journal of Software Tools</em>.</p>
<ul>
<li class=""><em>OpenCV library foundation</em></li>
</ul>
</li>
<li class="">
<p>Chen, L. C., Zhu, Y., Papandreou, G., Schroff, F., &amp; Adam, H. (2018). Encoder-decoder with atrous separable convolution for semantic image segmentation. <em>Proceedings of the European conference on computer vision</em>, 801-818.</p>
<ul>
<li class=""><em>Deep semantic segmentation</em></li>
</ul>
</li>
<li class="">
<p>Girshick, R. (2015). Fast R-CNN. <em>Proceedings of the IEEE international conference on computer vision</em>, 1440-1448.</p>
<ul>
<li class=""><em>Fast region-based CNN</em></li>
</ul>
</li>
<li class="">
<p>He, K., Gkioxari, G., Dollár, P., &amp; Girshick, R. (2017). Mask R-CNN. <em>Proceedings of the IEEE international conference on computer vision</em>, 2980-2988.</p>
<ul>
<li class=""><em>Instance segmentation</em></li>
</ul>
</li>
<li class="">
<p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., ... &amp; Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. <em>Proceedings of the 22nd ACM international conference on Multimedia</em>, 675-678.</p>
<ul>
<li class=""><em>Caffe deep learning framework</em></li>
</ul>
</li>
<li class="">
<p>Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em>.</p>
<ul>
<li class=""><em>Adam optimizer</em></li>
</ul>
</li>
<li class="">
<p>Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 3431-3440.</p>
<ul>
<li class=""><em>Fully convolutional networks</em></li>
</ul>
</li>
<li class="">
<p>Ren, S., He, K., Girshick, R., &amp; Sun, J. (2015). Faster R-CNN: Towards real-time object detection with region proposal networks. <em>Advances in neural information processing systems</em>, 28, 91-99.</p>
<ul>
<li class=""><em>Faster region-based CNN</em></li>
</ul>
</li>
<li class="">
<p>Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: Unified, real-time object detection. <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 779-788.</p>
<ul>
<li class=""><em>YOLO object detection</em></li>
</ul>
</li>
<li class="">
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., ... &amp; Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. <em>International Journal of Computer Vision</em>, 115(3), 211-252.</p>
<ul>
<li class=""><em>ImageNet benchmark</em></li>
</ul>
</li>
<li class="">
<p>Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. <em>arXiv preprint arXiv:1409.1556</em>.</p>
<ul>
<li class=""><em>VGG networks</em></li>
</ul>
</li>
<li class="">
<p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... &amp; Rabinovich, A. (2015). Going deeper with convolutions. <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 1-9.</p>
<ul>
<li class=""><em>Inception networks</em></li>
</ul>
</li>
<li class="">
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is all you need. <em>Advances in neural information processing systems</em>, 30, 5998-6008.</p>
<ul>
<li class=""><em>Transformer architecture</em></li>
</ul>
</li>
<li class="">
<p>Yosinski, J., Clune, J., Bengio, Y., &amp; Lipson, H. (2014). How transferable are features in deep neural networks?. <em>Advances in neural information processing systems</em>, 27, 3320-3328.</p>
<ul>
<li class=""><em>Feature transferability</em></li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="human-robot-interaction">Human-Robot Interaction<a href="#human-robot-interaction" class="hash-link" aria-label="Direct link to Human-Robot Interaction" title="Direct link to Human-Robot Interaction" translate="no">​</a></h2>
<ol start="95">
<li class="">
<p>Breazeal, C. (2003). <em>Toward sociable robots</em>. Robotics and autonomous systems, 42(3-4), 167-175.</p>
<ul>
<li class=""><em>Social robotics foundations</em></li>
</ul>
</li>
<li class="">
<p>Breazeal, C., Kidd, C. D., Thomaz, A. L., Hoffman, G., &amp; Tellex, S. (2006). Effects of repeated exposure on social perception of a robot. <em>Proceedings of the 1st ACM SIGCHI/SIGART conference on Human-robot interaction</em>, 114-120.</p>
<ul>
<li class=""><em>Repeated interaction effects</em></li>
</ul>
</li>
<li class="">
<p>Fong, T., Nourbakhsh, I., &amp; Dautenhahn, K. (2003). A survey of socially interactive robots. <em>Robotics and autonomous systems</em>, 42(3-4), 143-166.</p>
<ul>
<li class=""><em>Interactive robotics survey</em></li>
</ul>
</li>
<li class="">
<p>Mataric, M. J., &amp; Scassellati, B. (2018). Socially assistive robotics. <em>Foundations and trends in robotics</em>, 7(3-4), 219-272.</p>
<ul>
<li class=""><em>Assistive robotics</em></li>
</ul>
</li>
<li class="">
<p>Mutlu, B., Forlizzi, J., &amp; Hodgins, J. (2006). A storytelling robot: Modeling and evaluation of human-like gaze behavior. <em>International Conference on Intelligent Robots and Systems</em>, 5184-5189.</p>
<ul>
<li class=""><em>Gaze behavior modeling</em></li>
</ul>
</li>
<li class="">
<p>Tapus, A., Mataric, M. J., &amp; Scassellati, B. (2007). The grand challenge of social assistive robotics. <em>IEEE Robotics &amp; Automation Magazine</em>, 14(1), 35-36.</p>
</li>
</ol>
<ul>
<li class=""><em>Social assistive robotics challenge</em></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="online-resources">Online Resources<a href="#online-resources" class="hash-link" aria-label="Direct link to Online Resources" title="Direct link to Online Resources" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="official-documentation">Official Documentation<a href="#official-documentation" class="hash-link" aria-label="Direct link to Official Documentation" title="Direct link to Official Documentation" translate="no">​</a></h3>
<ol start="101">
<li class="">ROS 2 Documentation: <a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="">https://docs.ros.org/en/humble/</a></li>
<li class="">NVIDIA Isaac ROS: <a href="https://nvidia-isaac-ros.github.io/" target="_blank" rel="noopener noreferrer" class="">https://nvidia-isaac-ros.github.io/</a></li>
<li class="">Isaac Sim Documentation: <a href="https://docs.omniverse.nvidia.com/isaacsim/latest/" target="_blank" rel="noopener noreferrer" class="">https://docs.omniverse.nvidia.com/isaacsim/latest/</a></li>
<li class="">Gazebo Simulation: <a href="https://gazebosim.org/" target="_blank" rel="noopener noreferrer" class="">https://gazebosim.org/</a></li>
<li class="">OpenCV Documentation: <a href="https://docs.opencv.org/" target="_blank" rel="noopener noreferrer" class="">https://docs.opencv.org/</a></li>
<li class="">PyTorch Documentation: <a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener noreferrer" class="">https://pytorch.org/docs/stable/index.html</a></li>
<li class="">Hugging Face Documentation: <a href="https://huggingface.co/docs" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/docs</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="research-repositories">Research Repositories<a href="#research-repositories" class="hash-link" aria-label="Direct link to Research Repositories" title="Direct link to Research Repositories" translate="no">​</a></h3>
<ol start="108">
<li class="">arXiv Robotics: <a href="https://arxiv.org/list/cs.RO/recent" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/list/cs.RO/recent</a></li>
<li class="">arXiv AI: <a href="https://arxiv.org/list/cs.AI/recent" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/list/cs.AI/recent</a></li>
<li class="">arXiv Computer Vision: <a href="https://arxiv.org/list/cs.CV/recent" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/list/cs.CV/recent</a></li>
<li class="">NVIDIA Research Robotics: <a href="https://research.nvidia.com/labs/toronto-ai/" target="_blank" rel="noopener noreferrer" class="">https://research.nvidia.com/labs/toronto-ai/</a></li>
<li class="">Google AI Robotics: <a href="https://ai.google/research/teams/brain/robotics" target="_blank" rel="noopener noreferrer" class="">https://ai.google/research/teams/brain/robotics</a></li>
<li class="">OpenAI Robotics: <a href="https://openai.com/research/robotics" target="_blank" rel="noopener noreferrer" class="">https://openai.com/research/robotics</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="development-tools">Development Tools<a href="#development-tools" class="hash-link" aria-label="Direct link to Development Tools" title="Direct link to Development Tools" translate="no">​</a></h3>
<ol start="114">
<li class="">GitHub Robotics: <a href="https://github.com/topics/robotics" target="_blank" rel="noopener noreferrer" class="">https://github.com/topics/robotics</a></li>
<li class="">Robot Operating System GitHub: <a href="https://github.com/ros" target="_blank" rel="noopener noreferrer" class="">https://github.com/ros</a></li>
<li class="">NVIDIA Isaac ROS GitHub: <a href="https://github.com/NVIDIA-ISAAC-ROS" target="_blank" rel="noopener noreferrer" class="">https://github.com/NVIDIA-ISAAC-ROS</a></li>
<li class="">ROS Industrial: <a href="https://ros-industrial.org/" target="_blank" rel="noopener noreferrer" class="">https://ros-industrial.org/</a></li>
<li class="">PickNik Robotics: <a href="https://picknik.ai/" target="_blank" rel="noopener noreferrer" class="">https://picknik.ai/</a></li>
<li class="">The Construct: <a href="https://www.theconstructsim.com/" target="_blank" rel="noopener noreferrer" class="">https://www.theconstructsim.com/</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="conferences-and-journals">Conferences and Journals<a href="#conferences-and-journals" class="hash-link" aria-label="Direct link to Conferences and Journals" title="Direct link to Conferences and Journals" translate="no">​</a></h3>
<ol start="120">
<li class="">IEEE Robotics and Automation Society: <a href="https://www.ieee-ras.org/" target="_blank" rel="noopener noreferrer" class="">https://www.ieee-ras.org/</a></li>
<li class="">International Conference on Robotics and Automation (ICRA): <a href="https://www.icra.cc/" target="_blank" rel="noopener noreferrer" class="">https://www.icra.cc/</a></li>
<li class="">International Conference on Intelligent Robots and Systems (IROS): <a href="https://www.iros.org/" target="_blank" rel="noopener noreferrer" class="">https://www.iros.org/</a></li>
<li class="">Conference on Robot Learning (CoRL): <a href="https://www.robot-learning.org/" target="_blank" rel="noopener noreferrer" class="">https://www.robot-learning.org/</a></li>
<li class="">IEEE Transactions on Robotics: <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8860" target="_blank" rel="noopener noreferrer" class="">https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8860</a></li>
<li class="">The International Journal of Robotics Research: <a href="https://journals.sagepub.com/home/ijr" target="_blank" rel="noopener noreferrer" class="">https://journals.sagepub.com/home/ijr</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="educational-resources">Educational Resources<a href="#educational-resources" class="hash-link" aria-label="Direct link to Educational Resources" title="Direct link to Educational Resources" translate="no">​</a></h3>
<ol start="126">
<li class="">MIT OpenCourseWare Robotics: <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-141-robotics-science-and-systems-i-fall-2019/" target="_blank" rel="noopener noreferrer" class="">https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-141-robotics-science-and-systems-i-fall-2019/</a></li>
<li class="">Stanford CS 223A: <a href="https://cs.stanford.edu/groups/manips/teaching/cs223a/" target="_blank" rel="noopener noreferrer" class="">https://cs.stanford.edu/groups/manips/teaching/cs223a/</a></li>
<li class="">Carnegie Mellon Robotics Institute: <a href="https://www.ri.cmu.edu/" target="_blank" rel="noopener noreferrer" class="">https://www.ri.cmu.edu/</a></li>
<li class="">Coursera Robotics Specialization: <a href="https://www.coursera.org/specializations/robotics" target="_blank" rel="noopener noreferrer" class="">https://www.coursera.org/specializations/robotics</a></li>
<li class="">edX Robotics: <a href="https://www.edx.org/learn/robotics" target="_blank" rel="noopener noreferrer" class="">https://www.edx.org/learn/robotics</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="simulation-platforms">Simulation Platforms<a href="#simulation-platforms" class="hash-link" aria-label="Direct link to Simulation Platforms" title="Direct link to Simulation Platforms" translate="no">​</a></h3>
<ol start="131">
<li class="">Isaac Sim Examples: <a href="https://github.com/NVIDIA-Omniverse/IsaacSim" target="_blank" rel="noopener noreferrer" class="">https://github.com/NVIDIA-Omniverse/IsaacSim</a></li>
<li class="">Isaac Lab Examples: <a href="https://isaac-sim.github.io/IsaacLab/" target="_blank" rel="noopener noreferrer" class="">https://isaac-sim.github.io/IsaacLab/</a></li>
<li class="">Gazebo Tutorials: <a href="https://gazebosim.org/tutorials" target="_blank" rel="noopener noreferrer" class="">https://gazebosim.org/tutorials</a></li>
<li class="">Unity Robotics Hub: <a href="https://github.com/Unity-Technologies/Unity-Robotics-Hub" target="_blank" rel="noopener noreferrer" class="">https://github.com/Unity-Technologies/Unity-Robotics-Hub</a></li>
<li class="">Webots Robotics: <a href="https://cyberbotics.com/doc/guide/index" target="_blank" rel="noopener noreferrer" class="">https://cyberbotics.com/doc/guide/index</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="ai-and-machine-learning">AI and Machine Learning<a href="#ai-and-machine-learning" class="hash-link" aria-label="Direct link to AI and Machine Learning" title="Direct link to AI and Machine Learning" translate="no">​</a></h3>
<ol start="136">
<li class="">Hugging Face Models: <a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer" class="">https://huggingface.co/models</a></li>
<li class="">NVIDIA AI Enterprise: <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" target="_blank" rel="noopener noreferrer" class="">https://www.nvidia.com/en-us/data-center/products/ai-enterprise/</a></li>
<li class="">TensorFlow: <a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer" class="">https://www.tensorflow.org/</a></li>
<li class="">PyTorch: <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer" class="">https://pytorch.org/</a></li>
<li class="">Stable Diffusion: <a href="https://stability.ai/news/stable-diffusion-public-release" target="_blank" rel="noopener noreferrer" class="">https://stability.ai/news/stable-diffusion-public-release</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="community-and-support">Community and Support<a href="#community-and-support" class="hash-link" aria-label="Direct link to Community and Support" title="Direct link to Community and Support" translate="no">​</a></h3>
<ol start="141">
<li class="">ROS Discourse: <a href="https://discourse.ros.org/" target="_blank" rel="noopener noreferrer" class="">https://discourse.ros.org/</a></li>
<li class="">ROS Answers: <a href="https://answers.ros.org/questions/" target="_blank" rel="noopener noreferrer" class="">https://answers.ros.org/questions/</a></li>
<li class="">NVIDIA Developer Forum: <a href="https://forums.developer.nvidia.com/" target="_blank" rel="noopener noreferrer" class="">https://forums.developer.nvidia.com/</a></li>
<li class="">Reddit r/Robotics: <a href="https://www.reddit.com/r/robotics/" target="_blank" rel="noopener noreferrer" class="">https://www.reddit.com/r/robotics/</a></li>
<li class="">Reddit r/ROS: <a href="https://www.reddit.com/r/ros/" target="_blank" rel="noopener noreferrer" class="">https://www.reddit.com/r/ros/</a></li>
<li class="">Robohub: <a href="https://robohub.org/" target="_blank" rel="noopener noreferrer" class="">https://robohub.org/</a></li>
<li class="">The Robot Report: <a href="https://www.therobotreport.com/" target="_blank" rel="noopener noreferrer" class="">https://www.therobotreport.com/</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="industry-applications">Industry Applications<a href="#industry-applications" class="hash-link" aria-label="Direct link to Industry Applications" title="Direct link to Industry Applications" translate="no">​</a></h3>
<ol start="148">
<li class="">Boston Dynamics: <a href="https://www.bostondynamics.com/" target="_blank" rel="noopener noreferrer" class="">https://www.bostondynamics.com/</a></li>
<li class="">ABB Robotics: <a href="https://new.abb.com/products/robotics" target="_blank" rel="noopener noreferrer" class="">https://new.abb.com/products/robotics</a></li>
<li class="">KUKA Robotics: <a href="https://www.kuka.com/en-us" target="_blank" rel="noopener noreferrer" class="">https://www.kuka.com/en-us</a></li>
<li class="">Universal Robots: <a href="https://www.universal-robots.com/" target="_blank" rel="noopener noreferrer" class="">https://www.universal-robots.com/</a></li>
<li class="">SoftBank Robotics: <a href="https://www.softbankrobotics.com/" target="_blank" rel="noopener noreferrer" class="">https://www.softbankrobotics.com/</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="hardware-and-sensors">Hardware and Sensors<a href="#hardware-and-sensors" class="hash-link" aria-label="Direct link to Hardware and Sensors" title="Direct link to Hardware and Sensors" translate="no">​</a></h3>
<ol start="153">
<li class="">Intel RealSense: <a href="https://www.intelrealsense.com/" target="_blank" rel="noopener noreferrer" class="">https://www.intelrealsense.com/</a></li>
<li class="">Velodyne LiDAR: <a href="https://velodynelidar.com/" target="_blank" rel="noopener noreferrer" class="">https://velodynelidar.com/</a></li>
<li class="">NVIDIA Jetson: <a href="https://developer.nvidia.com/embedded/jetson-developer-kits" target="_blank" rel="noopener noreferrer" class="">https://developer.nvidia.com/embedded/jetson-developer-kits</a></li>
<li class="">Dynamixel Servos: <a href="https://emanual.robotis.com/" target="_blank" rel="noopener noreferrer" class="">https://emanual.robotis.com/</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="datasets-and-benchmarks">Datasets and Benchmarks<a href="#datasets-and-benchmarks" class="hash-link" aria-label="Direct link to Datasets and Benchmarks" title="Direct link to Datasets and Benchmarks" translate="no">​</a></h3>
<ol start="157">
<li class="">Robot Data Sets: <a href="https://robotics-data-set.cs.utah.edu/" target="_blank" rel="noopener noreferrer" class="">https://robotics-data-set.cs.utah.edu/</a></li>
<li class="">KITTI Vision Benchmark: <a href="http://www.cvlibs.net/datasets/kitti/" target="_blank" rel="noopener noreferrer" class="">http://www.cvlibs.net/datasets/kitti/</a></li>
<li class="">COCO Dataset: <a href="https://cocodataset.org/" target="_blank" rel="noopener noreferrer" class="">https://cocodataset.org/</a></li>
<li class="">YCB Object and Model Set: <a href="https://ycb-benchmarks.s3-us-west-2.amazonaws.com/" target="_blank" rel="noopener noreferrer" class="">https://ycb-benchmarks.s3-us-west-2.amazonaws.com/</a></li>
<li class="">Open Images Dataset: <a href="https://storage.googleapis.com/openimages/web/index.html" target="_blank" rel="noopener noreferrer" class="">https://storage.googleapis.com/openimages/web/index.html</a></li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="tutorials-and-courses">Tutorials and Courses<a href="#tutorials-and-courses" class="hash-link" aria-label="Direct link to Tutorials and Courses" title="Direct link to Tutorials and Courses" translate="no">​</a></h3>
<ol start="162">
<li class="">ROS 2 Tutorials: <a href="https://docs.ros.org/en/humble/Tutorials.html" target="_blank" rel="noopener noreferrer" class="">https://docs.ros.org/en/humble/Tutorials.html</a></li>
<li class="">Isaac ROS Tutorials: <a href="https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_examples/index.html" target="_blank" rel="noopener noreferrer" class="">https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_examples/index.html</a></li>
<li class="">Navigation2 Tutorials: <a href="https://navigation.ros.org/tutorials/index.html" target="_blank" rel="noopener noreferrer" class="">https://navigation.ros.org/tutorials/index.html</a></li>
<li class="">OpenCV Tutorials: <a href="https://docs.opencv.org/4.x/d9/df8/tutorial_root.html" target="_blank" rel="noopener noreferrer" class="">https://docs.opencv.org/4.x/d9/df8/tutorial_root.html</a></li>
<li class="">PyTorch Tutorials: <a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer" class="">https://pytorch.org/tutorials/</a></li>
</ol>
<hr>
<p>This bibliography provides a comprehensive collection of academic papers, technical documentation, online resources, and educational materials relevant to Physical AI and robotics development. The references span foundational research, current state-of-the-art approaches, and practical implementation resources that support the curriculum covered in this book.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/appendices/bibliography.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#table-of-contents" class="table-of-contents__link toc-highlight">Table of Contents</a></li><li><a href="#foundational-papers" class="table-of-contents__link toc-highlight">Foundational Papers</a></li><li><a href="#physical-ai-and-embodied-intelligence" class="table-of-contents__link toc-highlight">Physical AI and Embodied Intelligence</a></li><li><a href="#vision-language-action-vla-systems" class="table-of-contents__link toc-highlight">Vision-Language-Action (VLA) Systems</a></li><li><a href="#robotics-and-navigation" class="table-of-contents__link toc-highlight">Robotics and Navigation</a></li><li><a href="#isaac-and-nvidia-technologies" class="table-of-contents__link toc-highlight">Isaac and NVIDIA Technologies</a></li><li><a href="#ros-2-and-middleware" class="table-of-contents__link toc-highlight">ROS 2 and Middleware</a></li><li><a href="#simulation-and-digital-twins" class="table-of-contents__link toc-highlight">Simulation and Digital Twins</a></li><li><a href="#machine-learning-for-robotics" class="table-of-contents__link toc-highlight">Machine Learning for Robotics</a></li><li><a href="#computer-vision-and-perception" class="table-of-contents__link toc-highlight">Computer Vision and Perception</a></li><li><a href="#human-robot-interaction" class="table-of-contents__link toc-highlight">Human-Robot Interaction</a></li><li><a href="#online-resources" class="table-of-contents__link toc-highlight">Online Resources</a><ul><li><a href="#official-documentation" class="table-of-contents__link toc-highlight">Official Documentation</a></li><li><a href="#research-repositories" class="table-of-contents__link toc-highlight">Research Repositories</a></li><li><a href="#development-tools" class="table-of-contents__link toc-highlight">Development Tools</a></li><li><a href="#conferences-and-journals" class="table-of-contents__link toc-highlight">Conferences and Journals</a></li><li><a href="#educational-resources" class="table-of-contents__link toc-highlight">Educational Resources</a></li><li><a href="#simulation-platforms" class="table-of-contents__link toc-highlight">Simulation Platforms</a></li><li><a href="#ai-and-machine-learning" class="table-of-contents__link toc-highlight">AI and Machine Learning</a></li><li><a href="#community-and-support" class="table-of-contents__link toc-highlight">Community and Support</a></li><li><a href="#industry-applications" class="table-of-contents__link toc-highlight">Industry Applications</a></li><li><a href="#hardware-and-sensors" class="table-of-contents__link toc-highlight">Hardware and Sensors</a></li><li><a href="#datasets-and-benchmarks" class="table-of-contents__link toc-highlight">Datasets and Benchmarks</a></li><li><a href="#tutorials-and-courses" class="table-of-contents__link toc-highlight">Tutorials and Courses</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Modules</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-book/docs/module1-ros2/introduction">Module 1: Robotic Nervous System (ROS 2)</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-book/docs/module2-simulation/introduction">Module 2: Digital Twin (Gazebo &amp; Unity)</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-book/docs/module3-isaac/introduction">Module 3: AI-Robot Brain (NVIDIA Isaac)</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-book/docs/module4-vla/llms-robotics">Module 4: Vision-Language-Action (VLA)</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.omniverse.nvidia.com/isaacsim/latest/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Humble<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://nvidia-isaac-ros.github.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac ROS<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://gazebosim.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Gazebo Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discourse.ros.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS Discourse<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://forums.developer.nvidia.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Developer Forum<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://robotics.stackexchange.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Robotics Stack Exchange<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://embodied-ai.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Physical AI Research<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics. Built with Docusaurus for the Physical AI Community.</div></div></div></footer></div>
</body>
</html>