"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6296],{5432:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4-vla/voice-action-pipeline","title":"Voice-to-Action Pipeline","description":"Chapter Objectives","source":"@site/docs/module4-vla/02-voice-action-pipeline.md","sourceDirName":"module4-vla","slug":"/module4-vla/voice-action-pipeline","permalink":"/physical-ai-book/docs/module4-vla/voice-action-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module4-vla/02-voice-action-pipeline.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"LLMs Meet Robotics","permalink":"/physical-ai-book/docs/module4-vla/llms-robotics"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/physical-ai-book/docs/module4-vla/cognitive-planning"}}');var a=i(4848),o=i(8453);const r={sidebar_position:2},s="Voice-to-Action Pipeline",l={},c=[{value:"Chapter Objectives",id:"chapter-objectives",level:2},{value:"Voice-to-Action Architecture",id:"voice-to-action-architecture",level:2},{value:"Complete Pipeline Overview",id:"complete-pipeline-overview",level:3},{value:"Pipeline Components",id:"pipeline-components",level:3},{value:"Audio Input and Preprocessing",id:"audio-input-and-preprocessing",level:2},{value:"Audio Capture Configuration",id:"audio-capture-configuration",level:3},{value:"Audio Preprocessing",id:"audio-preprocessing",level:3},{value:"Speech Recognition Integration",id:"speech-recognition-integration",level:2},{value:"Online Speech Recognition",id:"online-speech-recognition",level:3},{value:"Offline Speech Recognition",id:"offline-speech-recognition",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:2},{value:"Command Parsing",id:"command-parsing",level:3},{value:"LLM Integration for Advanced Understanding",id:"llm-integration-for-advanced-understanding",level:2},{value:"LLM Command Interpreter",id:"llm-command-interpreter",level:3},{value:"Action Planning and Execution",id:"action-planning-and-execution",level:2},{value:"Action Planner",id:"action-planner",level:3},{value:"Voice Command Validation",id:"voice-command-validation",level:2},{value:"Safety and Validation System",id:"safety-and-validation-system",level:3},{value:"Complete Voice Pipeline Launch",id:"complete-voice-pipeline-launch",level:2},{value:"Main Launch File",id:"main-launch-file",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Audio Processing Optimization",id:"audio-processing-optimization",level:3},{value:"Best Practices for Voice-to-Action Pipelines",id:"best-practices-for-voice-to-action-pipelines",level:2},{value:"1. Error Handling",id:"1-error-handling",level:3},{value:"2. Performance Optimization",id:"2-performance-optimization",level:3},{value:"3. Safety Considerations",id:"3-safety-considerations",level:3},{value:"4. User Experience",id:"4-user-experience",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"})}),"\n",(0,a.jsx)(e.h2,{id:"chapter-objectives",children:"Chapter Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design and implement a complete voice-to-action pipeline for Physical AI"}),"\n",(0,a.jsx)(e.li,{children:"Integrate speech recognition with LLM processing"}),"\n",(0,a.jsx)(e.li,{children:"Create robust voice command interpretation systems"}),"\n",(0,a.jsx)(e.li,{children:"Implement multi-modal feedback mechanisms"}),"\n",(0,a.jsx)(e.li,{children:"Handle voice command ambiguity and errors gracefully"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"voice-to-action-architecture",children:"Voice-to-Action Architecture"}),"\n",(0,a.jsx)(e.h3,{id:"complete-pipeline-overview",children:"Complete Pipeline Overview"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Voice Input \u2192 Speech Recognition \u2192 Natural Language Processing \u2192 LLM Interpretation \u2192 Action Planning \u2192 Robot Execution \u2192 Feedback\n     \u2191                                                                                                                     \u2193\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(e.h3,{id:"pipeline-components",children:"Pipeline Components"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Audio Input"}),": Microphone array and audio preprocessing"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Speech Recognition"}),": Converting speech to text"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Natural Language Understanding"}),": Parsing and semantic analysis"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"LLM Processing"}),": Advanced interpretation and reasoning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Planning"}),": Converting commands to executable actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Execution"}),": Physical robot action execution"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Feedback"}),": Audio/visual confirmation and status updates"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"audio-input-and-preprocessing",children:"Audio Input and Preprocessing"}),"\n",(0,a.jsx)(e.h3,{id:"audio-capture-configuration",children:"Audio Capture Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/audio_capture.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nimport pyaudio\nimport numpy as np\nimport threading\nimport queue\nimport webrtcvad\nfrom collections import deque\n\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__('audio_capture_node')\n\n        # Audio configuration\n        self.rate = 16000  # Sample rate\n        self.chunk_size = 1024  # Samples per chunk\n        self.channels = 1  # Mono\n        self.format = pyaudio.paInt16  # 16-bit samples\n\n        # Voice Activity Detection\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(2)  # Aggressive VAD\n\n        # Audio buffers\n        self.audio_queue = queue.Queue()\n        self.voice_buffer = deque(maxlen=32000)  # 2 seconds at 16kHz\n        self.listening = False\n\n        # Publishers\n        self.audio_text_pub = self.create_publisher(String, '/audio/text', 10)\n        self.listening_pub = self.create_publisher(Bool, '/audio/listening', 10)\n\n        # Start audio capture thread\n        self.audio_thread = threading.Thread(target=self.capture_audio, daemon=True)\n        self.audio_thread.start()\n\n        # Timer for VAD\n        self.vad_timer = self.create_timer(0.1, self.check_voice_activity)\n\n        self.get_logger().info('Audio Capture Node Started')\n\n    def capture_audio(self):\n        \"\"\"Capture audio from microphone\"\"\"\n        audio = pyaudio.PyAudio()\n\n        stream = audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        self.get_logger().info('Audio capture started')\n\n        while rclpy.ok():\n            try:\n                data = stream.read(self.chunk_size, exception_on_overflow=False)\n                audio_data = np.frombuffer(data, dtype=np.int16)\n\n                # Add to voice buffer for VAD\n                self.voice_buffer.extend(audio_data)\n\n                # Put raw audio in queue for processing\n                self.audio_queue.put(audio_data)\n\n            except Exception as e:\n                self.get_logger().error(f'Audio capture error: {e}')\n                break\n\n        stream.stop_stream()\n        stream.close()\n        audio.terminate()\n\n    def check_voice_activity(self):\n        \"\"\"Check for voice activity and publish listening status\"\"\"\n        if len(self.voice_buffer) >= 320:  # 20ms at 16kHz\n            # Get 20ms chunk for VAD\n            chunk = list(self.voice_buffer)[-320:]\n            chunk_bytes = np.array(chunk, dtype=np.int16).tobytes()\n\n            try:\n                is_speech = self.vad.is_speech(chunk_bytes, self.rate)\n                self.listening = is_speech\n\n                # Publish listening status\n                listening_msg = Bool()\n                listening_msg.data = is_speech\n                self.listening_pub.publish(listening_msg)\n\n                if is_speech:\n                    self.get_logger().debug('Voice activity detected')\n            except Exception as e:\n                self.get_logger().error(f'VAD error: {e}')\n\n    def get_audio_chunk(self):\n        \"\"\"Get audio chunk from queue\"\"\"\n        try:\n            return self.audio_queue.get_nowait()\n        except queue.Empty:\n            return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AudioCaptureNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"audio-preprocessing",children:"Audio Preprocessing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# File: voice_pipeline/audio_preprocessing.py\nimport numpy as np\nfrom scipy import signal\nimport librosa\n\nclass AudioPreprocessor:\n    def __init__(self):\n        self.sample_rate = 16000\n        self.noise_threshold = 0.01  # Threshold for noise detection\n\n    def preprocess_audio(self, audio_data):\n        """Preprocess audio data for better recognition"""\n        # Convert to float32\n        if audio_data.dtype == np.int16:\n            audio_data = audio_data.astype(np.float32) / 32768.0\n\n        # Apply noise reduction\n        audio_data = self.reduce_noise(audio_data)\n\n        # Normalize audio\n        audio_data = self.normalize_audio(audio_data)\n\n        # Apply pre-emphasis filter\n        audio_data = self.pre_emphasis_filter(audio_data)\n\n        return audio_data\n\n    def reduce_noise(self, audio_data):\n        """Simple noise reduction"""\n        # Calculate RMS energy\n        rms = np.sqrt(np.mean(audio_data**2))\n\n        # Apply noise threshold\n        if rms < self.noise_threshold:\n            # Apply noise reduction\n            audio_data = self.spectral_subtraction(audio_data)\n\n        return audio_data\n\n    def spectral_subtraction(self, audio_data, n_fft=512):\n        """Simple spectral subtraction noise reduction"""\n        # This is a simplified version\n        # In practice, you\'d use more sophisticated noise reduction\n        return audio_data\n\n    def normalize_audio(self, audio_data):\n        """Normalize audio to consistent level"""\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            audio_data = audio_data / max_val\n        return audio_data\n\n    def pre_emphasis_filter(self, audio_data, coeff=0.97):\n        """Apply pre-emphasis filter"""\n        return np.append(audio_data[0], audio_data[1:] - coeff * audio_data[:-1])\n\n    def vad_filter(self, audio_data, threshold=0.01):\n        """Voice Activity Detection based on energy"""\n        energy = np.mean(audio_data**2)\n        return energy > threshold\n'})}),"\n",(0,a.jsx)(e.h2,{id:"speech-recognition-integration",children:"Speech Recognition Integration"}),"\n",(0,a.jsx)(e.h3,{id:"online-speech-recognition",children:"Online Speech Recognition"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/speech_recognition.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import AudioData\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\n\nclass SpeechRecognitionNode(Node):\n    def __init__(self):\n        super().__init__('speech_recognition_node')\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Configure recognizer\n        self.recognizer.energy_threshold = 300  # Adjust for environment\n        self.recognizer.dynamic_energy_threshold = True\n\n        # Audio buffer\n        self.audio_queue = queue.Queue()\n        self.listening = False\n\n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, '/audio/raw', self.audio_callback, 10)\n        self.listening_sub = self.create_subscription(\n            Bool, '/audio/listening', self.listening_callback, 10)\n\n        # Publishers\n        self.text_pub = self.create_publisher(String, '/voice/command', 10)\n\n        # Start recognition thread\n        self.recognition_thread = threading.Thread(\n            target=self.recognition_worker, daemon=True)\n        self.recognition_thread.start()\n\n        self.get_logger().info('Speech Recognition Node Started')\n\n    def audio_callback(self, msg):\n        \"\"\"Receive audio data\"\"\"\n        # Convert AudioData message to AudioData object\n        audio_data = sr.AudioData(msg.data, 16000, 2)  # Assuming 16kHz, 16-bit\n        self.audio_queue.put(audio_data)\n\n    def listening_callback(self, msg):\n        \"\"\"Update listening state\"\"\"\n        self.listening = msg.data\n\n    def recognition_worker(self):\n        \"\"\"Process audio in background\"\"\"\n        while rclpy.ok():\n            try:\n                if not self.audio_queue.empty() and self.listening:\n                    audio_data = self.audio_queue.get(timeout=1.0)\n\n                    try:\n                        # Recognize speech\n                        text = self.recognizer.recognize_google(audio_data)\n                        self.get_logger().info(f'Recognized: {text}')\n\n                        # Publish recognized text\n                        text_msg = String()\n                        text_msg.data = text\n                        self.text_pub.publish(text_msg)\n\n                    except sr.UnknownValueError:\n                        self.get_logger().info('Could not understand audio')\n                    except sr.RequestError as e:\n                        self.get_logger().error(f'Could not request results: {e}')\n\n            except queue.Empty:\n                time.sleep(0.1)  # Small delay to prevent busy waiting\n            except Exception as e:\n                self.get_logger().error(f'Recognition error: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SpeechRecognitionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"offline-speech-recognition",children:"Offline Speech Recognition"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/offline_recognition.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport vosk\nimport json\nimport wave\n\nclass OfflineSpeechRecognitionNode(Node):\n    def __init__(self):\n        super().__init__('offline_speech_recognition_node')\n\n        # Initialize Vosk model (requires model download)\n        # Download model from https://alphacephei.com/vosk/models\n        try:\n            self.model = vosk.Model(\"path/to/vosk-model\")  # Download model first\n            self.recognizer = vosk.KaldiRecognizer(self.model, 16000)\n        except:\n            self.get_logger().error('Vosk model not found. Please download and configure.')\n            self.model = None\n            self.recognizer = None\n\n        # Subscribers and publishers\n        self.audio_sub = self.create_subscription(\n            String, '/audio/processed', self.audio_callback, 10)\n        self.text_pub = self.create_publisher(String, '/voice/command', 10)\n\n        self.get_logger().info('Offline Speech Recognition Node Started')\n\n    def audio_callback(self, msg):\n        \"\"\"Process audio for offline recognition\"\"\"\n        if self.recognizer is None:\n            return\n\n        # Process audio chunk\n        if self.recognizer.AcceptWaveform(msg.data):\n            result = self.recognizer.Result()\n            result_dict = json.loads(result)\n\n            if 'text' in result_dict and result_dict['text']:\n                # Publish recognized text\n                text_msg = String()\n                text_msg.data = result_dict['text']\n                self.text_pub.publish(text_msg)\n                self.get_logger().info(f'Recognized (offline): {result_dict[\"text\"]}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = OfflineSpeechRecognitionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,a.jsx)(e.h3,{id:"command-parsing",children:"Command Parsing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/nlu_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport re\nimport json\nfrom typing import Dict, List, Optional\n\nclass NLUProcessorNode(Node):\n    def __init__(self):\n        super().__init__('nlu_processor_node')\n\n        # Subscribers\n        self.voice_sub = self.create_subscription(\n            String, '/voice/command', self.voice_callback, 10)\n\n        # Publishers\n        self.interpretation_pub = self.create_publisher(\n            String, '/command/interpretation', 10)\n\n        # Command patterns\n        self.command_patterns = {\n            'move': [\n                r'move\\s+(forward|backward|ahead|back)',\n                r'go\\s+(forward|backward|ahead|back)',\n                r'forward',\n                r'backward',\n                r'back',\n                r'ahead'\n            ],\n            'turn': [\n                r'turn\\s+(left|right)',\n                r'rotate\\s+(left|right)',\n                r'pivot\\s+(left|right)'\n            ],\n            'stop': [\n                r'stop',\n                r'halt',\n                r'freeze'\n            ],\n            'pickup': [\n                r'pick up',\n                r'grab',\n                r'take',\n                r'collect'\n            ],\n            'place': [\n                r'place',\n                r'put',\n                r'drop',\n                r'release'\n            ]\n        }\n\n        # Distance patterns\n        self.distance_patterns = [\n            r'(\\d+(?:\\.\\d+)?)\\s*(?:meters?|m)',\n            r'(\\d+(?:\\.\\d+)?)\\s*(?:feet|ft)',\n            r'(\\d+(?:\\.\\d+)?)\\s*(?:steps?)'\n        ]\n\n        # Direction patterns\n        self.direction_patterns = [\n            r'(\\d+(?:\\.\\d+)?)\\s*degrees?',\n            r'(\\d+(?:\\.\\d+)?)\\s*deg',\n            r'quarter turn',\n            r'half turn',\n            r'full turn'\n        ]\n\n        self.get_logger().info('NLU Processor Node Started')\n\n    def voice_callback(self, msg):\n        \"\"\"Process voice command\"\"\"\n        command_text = msg.data.lower()\n        self.get_logger().info(f'Processing command: {command_text}')\n\n        # Parse command\n        interpretation = self.parse_command(command_text)\n\n        # Publish interpretation\n        interpretation_msg = String()\n        interpretation_msg.data = json.dumps(interpretation)\n        self.interpretation_pub.publish(interpretation_msg)\n\n    def parse_command(self, text: str) -> Dict:\n        \"\"\"Parse natural language command into structured format\"\"\"\n        interpretation = {\n            'action': None,\n            'parameters': {},\n            'confidence': 0.0,\n            'original_text': text\n        }\n\n        # Find main action\n        for action, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                if re.search(pattern, text):\n                    interpretation['action'] = action\n                    interpretation['confidence'] = 0.9\n                    break\n            if interpretation['action']:\n                break\n\n        # Extract parameters\n        if interpretation['action']:\n            interpretation['parameters'] = self.extract_parameters(text, interpretation['action'])\n\n        return interpretation\n\n    def extract_parameters(self, text: str, action: str) -> Dict:\n        \"\"\"Extract parameters from command text\"\"\"\n        params = {}\n\n        # Extract distance\n        for pattern in self.distance_patterns:\n            match = re.search(pattern, text)\n            if match:\n                distance = float(match.group(1))\n                if 'meters' in match.group(0) or 'm' in match.group(0):\n                    params['distance'] = distance\n                elif 'feet' in match.group(0) or 'ft' in match.group(0):\n                    params['distance'] = distance * 0.3048  # Convert to meters\n                elif 'step' in match.group(0):\n                    params['distance'] = distance * 0.76  # Average step length\n                break\n\n        # Extract direction (for turns)\n        if action == 'turn':\n            for pattern in self.direction_patterns:\n                match = re.search(pattern, text)\n                if match:\n                    if 'quarter turn' in text:\n                        params['angle'] = 90\n                    elif 'half turn' in text:\n                        params['angle'] = 180\n                    elif 'full turn' in text:\n                        params['angle'] = 360\n                    else:\n                        params['angle'] = float(match.group(1))\n                    break\n\n            # Extract direction (left/right)\n            if 'left' in text:\n                params['direction'] = 'left'\n            elif 'right' in text:\n                params['direction'] = 'right'\n\n        # Extract object (for pickup/place)\n        if action in ['pickup', 'place']:\n            # Look for object names in the text\n            object_patterns = [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'take (.+)',\n                r'place (.+)',\n                r'put (.+)'\n            ]\n\n            for pattern in object_patterns:\n                match = re.search(pattern, text)\n                if match:\n                    params['object'] = match.group(1).strip()\n                    break\n\n        return params\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NLUProcessorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"llm-integration-for-advanced-understanding",children:"LLM Integration for Advanced Understanding"}),"\n",(0,a.jsx)(e.h3,{id:"llm-command-interpreter",children:"LLM Command Interpreter"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/llm_interpreter.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport openai\nfrom typing import Dict, Any\n\nclass LLMInterpreterNode(Node):\n    def __init__(self):\n        super().__init__('llm_interpreter_node')\n\n        # Initialize OpenAI client\n        self.client = openai.OpenAI(api_key='your-api-key')  # Replace with actual key\n\n        # Subscribers\n        self.nlu_sub = self.create_subscription(\n            String, '/command/interpretation', self.nlu_callback, 10)\n\n        # Publishers\n        self.action_plan_pub = self.create_publisher(\n            String, '/action/plan', 10)\n\n        self.get_logger().info('LLM Interpreter Node Started')\n\n    def nlu_callback(self, msg):\n        \"\"\"Process NLU output with LLM for advanced interpretation\"\"\"\n        try:\n            interpretation = json.loads(msg.data)\n            original_text = interpretation['original_text']\n\n            # Use LLM to enhance interpretation\n            enhanced_interpretation = self.enhance_interpretation(original_text)\n\n            # Publish enhanced action plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(enhanced_interpretation)\n            self.action_plan_pub.publish(plan_msg)\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in NLU message')\n\n    def enhance_interpretation(self, command: str) -> Dict[str, Any]:\n        \"\"\"Use LLM to enhance command interpretation with context and reasoning\"\"\"\n        prompt = f\"\"\"\n        Interpret this robot command: \"{command}\"\n\n        Provide a structured JSON response with:\n        1. action: the primary action to perform\n        2. parameters: specific parameters for the action\n        3. context: environmental context considerations\n        4. safety: safety considerations for execution\n        5. alternatives: alternative interpretations if ambiguous\n        6. confidence: confidence level (0-1)\n\n        Consider common sense, physics, and safety in your interpretation.\n        \"\"\"\n\n        try:\n            response = self.client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1\n            )\n\n            # Parse response (in practice, ensure proper JSON format)\n            content = response.choices[0].message.content\n\n            # Clean up response if needed\n            if content.startswith('```json'):\n                content = content[7:content.rfind('```')]\n            elif content.startswith('```'):\n                content = content[3:content.rfind('```')]\n\n            return json.loads(content)\n\n        except Exception as e:\n            self.get_logger().error(f'LLM interpretation error: {e}')\n            # Return fallback interpretation\n            return {\n                'action': 'unknown',\n                'parameters': {},\n                'context': 'unknown',\n                'safety': 'unknown',\n                'alternatives': [],\n                'confidence': 0.0\n            }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMInterpreterNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"action-planning-and-execution",children:"Action Planning and Execution"}),"\n",(0,a.jsx)(e.h3,{id:"action-planner",children:"Action Planner"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/action_planner.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Path\nimport json\nfrom typing import List, Dict, Any\n\nclass ActionPlannerNode(Node):\n    def __init__(self):\n        super().__init__('action_planner_node')\n\n        # Subscribers\n        self.action_plan_sub = self.create_subscription(\n            String, '/action/plan', self.action_plan_callback, 10)\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.nav_goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)\n        self.status_pub = self.create_publisher(String, '/voice/status', 10)\n\n        self.get_logger().info('Action Planner Node Started')\n\n    def action_plan_callback(self, msg):\n        \"\"\"Process action plan and execute\"\"\"\n        try:\n            plan = json.loads(msg.data)\n            action = plan.get('action', 'unknown')\n\n            self.get_logger().info(f'Executing action: {action}')\n\n            if action == 'move':\n                self.execute_move_action(plan.get('parameters', {}))\n            elif action == 'turn':\n                self.execute_turn_action(plan.get('parameters', {}))\n            elif action == 'stop':\n                self.execute_stop_action()\n            elif action == 'navigate':\n                self.execute_navigation_action(plan.get('parameters', {}))\n            elif action == 'pickup':\n                self.execute_pickup_action(plan.get('parameters', {}))\n            elif action == 'place':\n                self.execute_place_action(plan.get('parameters', {}))\n            else:\n                self.get_logger().warn(f'Unknown action: {action}')\n                self.publish_status(f'Unknown command: {action}')\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in action plan')\n\n    def execute_move_action(self, params: Dict[str, Any]):\n        \"\"\"Execute move action\"\"\"\n        cmd = Twist()\n\n        distance = params.get('distance', 1.0)  # Default 1 meter\n        duration = distance / 0.5  # Assuming 0.5 m/s speed\n\n        # Simple move forward\n        cmd.linear.x = 0.5  # 0.5 m/s\n        cmd.angular.z = 0.0\n\n        # Publish command for duration\n        self.publish_command_for_duration(cmd, duration)\n        self.publish_status(f'Moving forward {distance} meters')\n\n    def execute_turn_action(self, params: Dict[str, Any]):\n        \"\"\"Execute turn action\"\"\"\n        cmd = Twist()\n\n        angle = params.get('angle', 90)  # Default 90 degrees\n        direction = params.get('direction', 'left')\n\n        # Convert angle to radians and calculate duration\n        angle_rad = angle * 3.14159 / 180.0\n        duration = angle_rad / 0.5  # Assuming 0.5 rad/s angular velocity\n\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.5 if direction == 'left' else -0.5\n\n        self.publish_command_for_duration(cmd, duration)\n        self.publish_status(f'Turning {direction} {angle} degrees')\n\n    def execute_stop_action(self):\n        \"\"\"Execute stop action\"\"\"\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n\n        self.cmd_vel_pub.publish(cmd)\n        self.publish_status('Stopping')\n\n    def execute_navigation_action(self, params: Dict[str, Any]):\n        \"\"\"Execute navigation action\"\"\"\n        # This would integrate with navigation stack\n        goal_x = params.get('x', 0.0)\n        goal_y = params.get('y', 0.0)\n\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = 'map'\n        goal_msg.pose.position.x = goal_x\n        goal_msg.pose.position.y = goal_y\n        goal_msg.pose.position.z = 0.0\n        goal_msg.pose.orientation.w = 1.0\n\n        self.nav_goal_pub.publish(goal_msg)\n        self.publish_status(f'Navigating to ({goal_x}, {goal_y})')\n\n    def execute_pickup_action(self, params: Dict[str, Any]):\n        \"\"\"Execute pickup action\"\"\"\n        # This would integrate with manipulation stack\n        object_name = params.get('object', 'unknown')\n        self.publish_status(f'Attempting to pick up {object_name}')\n        # In real implementation, this would call manipulation services\n\n    def execute_place_action(self, params: Dict[str, Any]):\n        \"\"\"Execute place action\"\"\"\n        # This would integrate with manipulation stack\n        object_name = params.get('object', 'unknown')\n        self.publish_status(f'Attempting to place {object_name}')\n        # In real implementation, this would call manipulation services\n\n    def publish_command_for_duration(self, cmd: Twist, duration: float):\n        \"\"\"Publish command for specified duration\"\"\"\n        start_time = self.get_clock().now()\n        end_time = start_time + rclpy.time.Duration(seconds=duration)\n\n        while self.get_clock().now() < end_time:\n            self.cmd_vel_pub.publish(cmd)\n            rclpy.spin_once(self, timeout_sec=0.1)\n\n        # Stop after duration\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n    def publish_status(self, status: str):\n        \"\"\"Publish status message\"\"\"\n        status_msg = String()\n        status_msg.data = status\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionPlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"voice-command-validation",children:"Voice Command Validation"}),"\n",(0,a.jsx)(e.h3,{id:"safety-and-validation-system",children:"Safety and Validation System"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/command_validator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import LaserScan\nimport json\nfrom typing import Dict, Any, Tuple\n\nclass CommandValidatorNode(Node):\n    def __init__(self):\n        super().__init__('command_validator_node')\n\n        # Subscribers\n        self.action_plan_sub = self.create_subscription(\n            String, '/action/plan', self.action_plan_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10)\n\n        # Publishers\n        self.validated_plan_pub = self.create_publisher(\n            String, '/action/validated_plan', 10)\n        self.warning_pub = self.create_publisher(\n            String, '/voice/warning', 10)\n\n        # State\n        self.laser_data = None\n        self.safety_threshold = 0.5  # 50cm safety distance\n\n        self.get_logger().info('Command Validator Node Started')\n\n    def scan_callback(self, msg):\n        \"\"\"Update laser scan data\"\"\"\n        self.laser_data = msg\n\n    def action_plan_callback(self, msg):\n        \"\"\"Validate action plan against safety constraints\"\"\"\n        try:\n            plan = json.loads(msg.data)\n\n            # Validate plan\n            is_safe, reason = self.validate_plan(plan)\n\n            if is_safe:\n                # Publish validated plan\n                validated_msg = String()\n                validated_msg.data = msg.data\n                self.validated_plan_pub.publish(validated_msg)\n            else:\n                # Issue warning and modify plan if possible\n                self.get_logger().warn(f'Unsafe command: {reason}')\n                self.publish_warning(f'Unsafe command: {reason}')\n\n                # Try to modify plan to be safe\n                safe_plan = self.modify_plan_for_safety(plan)\n                if safe_plan:\n                    validated_msg = String()\n                    validated_msg.data = json.dumps(safe_plan)\n                    self.validated_plan_pub.publish(validated_msg)\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in action plan')\n\n    def validate_plan(self, plan: Dict[str, Any]) -> Tuple[bool, str]:\n        \"\"\"Validate plan against safety constraints\"\"\"\n        action = plan.get('action', 'unknown')\n\n        if action in ['move', 'navigate']:\n            if self.laser_data:\n                # Check if path is clear\n                min_distance = min(self.laser_data.ranges) if self.laser_data.ranges else float('inf')\n\n                if min_distance < self.safety_threshold:\n                    return False, f'Obstacle detected at {min_distance:.2f}m, minimum safe distance is {self.safety_threshold}m'\n\n        elif action == 'turn':\n            # Check if turning would cause collision\n            if self.laser_data:\n                # For left turn, check left side; for right turn, check right side\n                params = plan.get('parameters', {})\n                direction = params.get('direction', 'left')\n\n                if direction == 'left':\n                    # Check left side (first quarter of ranges)\n                    left_ranges = self.laser_data.ranges[:len(self.laser_data.ranges)//4]\n                else:  # right\n                    # Check right side (last quarter of ranges)\n                    right_idx = 3 * len(self.laser_data.ranges) // 4\n                    left_ranges = self.laser_data.ranges[right_idx:]\n\n                min_left_distance = min(left_ranges) if left_ranges else float('inf')\n\n                if min_left_distance < self.safety_threshold:\n                    return False, f'Obstacle on {direction} side at {min_left_distance:.2f}m'\n\n        return True, \"Plan is safe\"\n\n    def modify_plan_for_safety(self, plan: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Modify plan to make it safe, if possible\"\"\"\n        action = plan.get('action', 'unknown')\n\n        if action == 'move':\n            # Reduce distance if obstacle detected\n            if self.laser_data:\n                min_distance = min(self.laser_data.ranges) if self.laser_data.ranges else float('inf')\n\n                if min_distance < self.safety_threshold:\n                    # Modify distance to be safe\n                    safe_distance = max(0.1, min_distance - 0.1)  # 10cm buffer\n                    modified_plan = plan.copy()\n                    if 'parameters' not in modified_plan:\n                        modified_plan['parameters'] = {}\n                    modified_plan['parameters']['distance'] = safe_distance\n                    modified_plan['modified'] = True\n                    modified_plan['modification_reason'] = f'Reduced distance for safety (was too close to obstacle)'\n                    return modified_plan\n\n        return None  # No safe modification possible\n\n    def publish_warning(self, warning: str):\n        \"\"\"Publish warning message\"\"\"\n        warning_msg = String()\n        warning_msg.data = warning\n        self.warning_pub.publish(warning_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandValidatorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"complete-voice-pipeline-launch",children:"Complete Voice Pipeline Launch"}),"\n",(0,a.jsx)(e.h3,{id:"main-launch-file",children:"Main Launch File"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: voice_pipeline/launch/voice_pipeline.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    # Audio capture node\n    audio_capture = Node(\n        package='voice_pipeline',\n        executable='audio_capture',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Speech recognition node\n    speech_recognition = Node(\n        package='voice_pipeline',\n        executable='speech_recognition',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # NLU processor node\n    nlu_processor = Node(\n        package='voice_pipeline',\n        executable='nlu_processor',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # LLM interpreter node\n    llm_interpreter = Node(\n        package='voice_pipeline',\n        executable='llm_interpreter',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Command validator node\n    command_validator = Node(\n        package='voice_pipeline',\n        executable='command_validator',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Action planner node\n    action_planner = Node(\n        package='voice_pipeline',\n        executable='action_planner',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time'\n        ),\n        audio_capture,\n        speech_recognition,\n        nlu_processor,\n        llm_interpreter,\n        command_validator,\n        action_planner\n    ])\n"})}),"\n",(0,a.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"audio-processing-optimization",children:"Audio Processing Optimization"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# File: voice_pipeline/optimization.py\nimport numpy as np\nfrom collections import deque\nimport threading\nimport time\n\nclass OptimizedAudioProcessor:\n    def __init__(self):\n        self.sample_rate = 16000\n        self.buffer_size = 8192  # Process in chunks\n        self.audio_buffer = deque(maxlen=self.buffer_size)\n        self.processing_lock = threading.Lock()\n\n        # Pre-allocated arrays to avoid allocation during processing\n        self.temp_array = np.zeros(self.buffer_size, dtype=np.float32)\n        self.processed_array = np.zeros(self.buffer_size, dtype=np.float32)\n\n    def process_audio_chunk(self, chunk):\n        """Optimized audio processing with minimal allocation"""\n        with self.processing_lock:\n            # Convert to float32 if needed\n            if chunk.dtype != np.float32:\n                chunk = chunk.astype(np.float32) / 32768.0\n\n            # Copy to temp array to avoid modifying original\n            np.copyto(self.temp_array[:len(chunk)], chunk)\n\n            # Apply processing\n            processed = self.apply_processing(self.temp_array[:len(chunk)])\n\n            return processed\n\n    def apply_processing(self, audio_data):\n        """Apply optimized processing steps"""\n        # Normalize\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            audio_data = audio_data / max_val\n\n        # Apply simple noise reduction\n        audio_data = self.simple_noise_reduction(audio_data)\n\n        return audio_data\n\n    def simple_noise_reduction(self, audio_data, threshold=0.01):\n        """Simple noise reduction without complex algorithms"""\n        # Create a simple mask based on threshold\n        mask = np.abs(audio_data) > threshold\n        return audio_data * mask.astype(np.float32)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices-for-voice-to-action-pipelines",children:"Best Practices for Voice-to-Action Pipelines"}),"\n",(0,a.jsx)(e.h3,{id:"1-error-handling",children:"1. Error Handling"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement robust error handling at each pipeline stage"}),"\n",(0,a.jsx)(e.li,{children:"Provide graceful degradation when components fail"}),"\n",(0,a.jsx)(e.li,{children:"Log errors for debugging and improvement"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-performance-optimization",children:"2. Performance Optimization"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use appropriate buffer sizes to balance latency and throughput"}),"\n",(0,a.jsx)(e.li,{children:"Optimize audio processing to run in real-time"}),"\n",(0,a.jsx)(e.li,{children:"Consider using separate threads for different pipeline stages"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-safety-considerations",children:"3. Safety Considerations"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Always validate LLM-generated actions before execution"}),"\n",(0,a.jsx)(e.li,{children:"Implement safety checks based on sensor data"}),"\n",(0,a.jsx)(e.li,{children:"Provide emergency stop capabilities"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"4-user-experience",children:"4. User Experience"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Provide clear audio feedback when listening"}),"\n",(0,a.jsx)(e.li,{children:"Confirm understanding before executing commands"}),"\n",(0,a.jsx)(e.li,{children:"Handle ambiguous commands gracefully"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,a.jsx)(e.p,{children:"The voice-to-action pipeline enables natural human-robot interaction by converting spoken commands into physical robot actions. The pipeline involves audio capture, speech recognition, natural language understanding, LLM processing, action planning, and execution. Proper integration requires attention to real-time performance, safety validation, and user experience considerations."}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement a complete voice-to-action pipeline with speech recognition and command execution."}),"\n",(0,a.jsx)(e.li,{children:"Add voice feedback to confirm command understanding."}),"\n",(0,a.jsx)(e.li,{children:"Implement safety validation that prevents unsafe actions based on sensor data."}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"In the next chapter, we'll explore cognitive planning with LLMs, learning how to use language models for higher-level reasoning and decision making in Physical AI systems."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var t=i(6540);const a={},o=t.createContext(a);function r(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);