"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[3799],{177:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4-vla/multimodal-integration","title":"Multimodal Integration","description":"Chapter Objectives","source":"@site/docs/module4-vla/04-multimodal-integration.md","sourceDirName":"module4-vla","slug":"/module4-vla/multimodal-integration","permalink":"/physical-ai-book/docs/module4-vla/multimodal-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module4-vla/04-multimodal-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning with LLMs","permalink":"/physical-ai-book/docs/module4-vla/cognitive-planning"},"next":{"title":"CAPSTONE: Autonomous Humanoid Assistant","permalink":"/physical-ai-book/docs/module4-vla/capstone"}}');var a=t(4848),o=t(8453);const s={sidebar_position:4},r="Multimodal Integration",l={},c=[{value:"Chapter Objectives",id:"chapter-objectives",level:2},{value:"Multimodal Integration Overview",id:"multimodal-integration-overview",level:2},{value:"What is Multimodal Integration?",id:"what-is-multimodal-integration",level:3},{value:"The VLA Framework",id:"the-vla-framework",level:3},{value:"Cross-Modal Representations",id:"cross-modal-representations",level:2},{value:"Unified Embedding Spaces",id:"unified-embedding-spaces",level:3},{value:"Vision-Language Grounding",id:"vision-language-grounding",level:2},{value:"Object Grounding and Referencing",id:"object-grounding-and-referencing",level:3},{value:"Action-Language Integration",id:"action-language-integration",level:2},{value:"Natural Language Command to Action Mapping",id:"natural-language-command-to-action-mapping",level:3},{value:"Multimodal Decision Making",id:"multimodal-decision-making",level:2},{value:"Cross-Modal Reasoning Engine",id:"cross-modal-reasoning-engine",level:3},{value:"Real-Time Multimodal Fusion",id:"real-time-multimodal-fusion",level:2},{value:"Synchronized Processing Pipeline",id:"synchronized-processing-pipeline",level:3},{value:"Best Practices for Multimodal Integration",id:"best-practices-for-multimodal-integration",level:2},{value:"1. Temporal Synchronization",id:"1-temporal-synchronization",level:3},{value:"2. Uncertainty Management",id:"2-uncertainty-management",level:3},{value:"3. Computational Efficiency",id:"3-computational-efficiency",level:3},{value:"4. Robustness",id:"4-robustness",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"multimodal-integration",children:"Multimodal Integration"})}),"\n",(0,a.jsx)(e.h2,{id:"chapter-objectives",children:"Chapter Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design and implement multimodal systems that combine vision, language, and action"}),"\n",(0,a.jsx)(e.li,{children:"Integrate multiple sensory modalities for enhanced Physical AI capabilities"}),"\n",(0,a.jsx)(e.li,{children:"Create unified representations that bridge different modalities"}),"\n",(0,a.jsx)(e.li,{children:"Build systems that understand and respond to complex multimodal inputs"}),"\n",(0,a.jsx)(e.li,{children:"Implement cross-modal reasoning and decision making"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"multimodal-integration-overview",children:"Multimodal Integration Overview"}),"\n",(0,a.jsx)(e.h3,{id:"what-is-multimodal-integration",children:"What is Multimodal Integration?"}),"\n",(0,a.jsx)(e.p,{children:"Multimodal integration in Physical AI refers to the ability to process, understand, and act upon information from multiple sensory modalities simultaneously. This includes:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Visual Modality"}),": Cameras, LiDAR, depth sensors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Language Modality"}),": Speech, text, natural language commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Modality"}),": Physical manipulation, navigation, interaction"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Other Modalities"}),": Audio, tactile, thermal, etc."]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"the-vla-framework",children:"The VLA Framework"}),"\n",(0,a.jsx)(e.p,{children:"The Vision-Language-Action (VLA) framework provides a unified approach to multimodal integration:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Vision       \u2502    \u2502   Language      \u2502    \u2502     Action      \u2502\n\u2502   (Perception)  \u2502\u25c4\u2500\u2500\u25ba\u2502   (Cognition)   \u2502\u25c4\u2500\u2500\u25ba\u2502   (Execution)   \u2502\n\u2502   \u2022 Cameras     \u2502    \u2502   \u2022 LLMs        \u2502    \u2502   \u2022 Navigation  \u2502\n\u2502   \u2022 LiDAR       \u2502    \u2502   \u2022 NLP         \u2502    \u2502   \u2022 Manipulation\u2502\n\u2502   \u2022 Depth       \u2502    \u2502   \u2022 Dialogue    \u2502    \u2502   \u2022 Control     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Cross-Modal    \u2502\n                    \u2502  Integration    \u2502\n                    \u2502  (Fusion,       \u2502\n                    \u2502   Reasoning,    \u2502\n                    \u2502   Grounding)    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(e.h2,{id:"cross-modal-representations",children:"Cross-Modal Representations"}),"\n",(0,a.jsx)(e.h3,{id:"unified-embedding-spaces",children:"Unified Embedding Spaces"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: multimodal_integration/embedding_fusion.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom transformers import CLIPProcessor, CLIPModel\nimport numpy as np\nfrom typing import Dict, Any, List, Optional\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass MultimodalEmbeddingNode(Node):\n    def __init__(self):\n        super().__init__('multimodal_embedding_node')\n\n        # Initialize CLIP model for vision-language integration\n        try:\n            self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n            self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        except:\n            self.get_logger().warn('CLIP model not available, using placeholder')\n            self.clip_model = None\n            self.clip_processor = None\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.text_sub = self.create_subscription(\n            String, '/user/command', self.text_callback, 10)\n        self.detections_sub = self.create_subscription(\n            Detection2DArray, '/object/detections', self.detections_callback, 10)\n\n        # Publishers\n        self.fusion_pub = self.create_publisher(\n            String, '/multimodal/fusion_output', 10)\n\n        # State\n        self.current_image = None\n        self.current_text = None\n        self.current_detections = None\n\n        self.get_logger().info('Multimodal Embedding Node Started')\n\n    def image_callback(self, msg):\n        \"\"\"Process image and extract visual embeddings\"\"\"\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.current_image = cv_image\n\n            # Extract visual features using CLIP\n            if self.clip_model is not None:\n                inputs = self.clip_processor(images=cv_image, return_tensors=\"pt\", padding=True)\n                visual_features = self.clip_model.get_image_features(**inputs)\n                self.process_visual_features(visual_features, cv_image)\n\n        except Exception as e:\n            self.get_logger().error(f'Image processing error: {e}')\n\n    def text_callback(self, msg):\n        \"\"\"Process text and extract language embeddings\"\"\"\n        self.current_text = msg.data\n\n        # Extract text features using CLIP\n        if self.clip_model is not None:\n            try:\n                inputs = self.clip_processor(text=msg.data, return_tensors=\"pt\", padding=True)\n                text_features = self.clip_model.get_text_features(**inputs)\n                self.process_text_features(text_features, msg.data)\n            except Exception as e:\n                self.get_logger().error(f'Text processing error: {e}')\n\n    def detections_callback(self, msg):\n        \"\"\"Process object detections\"\"\"\n        self.current_detections = msg\n\n    def process_visual_features(self, features, image):\n        \"\"\"Process visual features and prepare for fusion\"\"\"\n        # Convert to numpy for easier handling\n        visual_embedding = features.detach().cpu().numpy()\n\n        # Store with spatial information\n        height, width = image.shape[:2]\n        spatial_features = self.extract_spatial_features(image)\n\n        return {\n            'embedding': visual_embedding,\n            'spatial': spatial_features,\n            'resolution': (height, width)\n        }\n\n    def process_text_features(self, features, text):\n        \"\"\"Process text features and prepare for fusion\"\"\"\n        text_embedding = features.detach().cpu().numpy()\n\n        # Extract semantic information\n        semantic_features = self.extract_semantic_features(text)\n\n        return {\n            'embedding': text_embedding,\n            'semantic': semantic_features,\n            'original_text': text\n        }\n\n    def extract_spatial_features(self, image):\n        \"\"\"Extract spatial features from image\"\"\"\n        # Simple spatial features: edges, corners, regions\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Edge detection\n        edges = cv2.Canny(gray, 50, 150)\n\n        # Corner detection\n        corners = cv2.goodFeaturesToTrack(gray, maxCorners=100, qualityLevel=0.01, minDistance=10)\n\n        return {\n            'edges': edges,\n            'corners': corners,\n            'center_of_mass': self.calculate_center_of_mass(edges)\n        }\n\n    def extract_semantic_features(self, text):\n        \"\"\"Extract semantic features from text\"\"\"\n        # Simple keyword extraction and semantic analysis\n        keywords = self.extract_keywords(text)\n        intent = self.classify_intent(text)\n\n        return {\n            'keywords': keywords,\n            'intent': intent,\n            'entities': self.extract_entities(text)\n        }\n\n    def extract_keywords(self, text):\n        \"\"\"Extract important keywords from text\"\"\"\n        # Simple keyword extraction (in practice, use NLP libraries)\n        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n        words = text.lower().replace(',', ' ').replace('.', ' ').split()\n        return [word for word in words if word not in stop_words]\n\n    def classify_intent(self, text):\n        \"\"\"Classify intent of text command\"\"\"\n        navigation_keywords = ['go', 'move', 'navigate', 'to', 'toward', 'towards']\n        manipulation_keywords = ['pick', 'place', 'grasp', 'take', 'put', 'drop']\n        perception_keywords = ['see', 'find', 'look', 'detect', 'where', 'what']\n\n        text_lower = text.lower()\n\n        if any(keyword in text_lower for keyword in navigation_keywords):\n            return 'navigation'\n        elif any(keyword in text_lower for keyword in manipulation_keywords):\n            return 'manipulation'\n        elif any(keyword in text_lower for keyword in perception_keywords):\n            return 'perception'\n        else:\n            return 'other'\n\n    def extract_entities(self, text):\n        \"\"\"Extract named entities from text\"\"\"\n        # Simple entity extraction (in practice, use NER models)\n        entities = []\n        words = text.split()\n\n        # Look for potential object names\n        object_indicators = ['the', 'a', 'an']\n        for i, word in enumerate(words):\n            if word.lower() in object_indicators and i + 1 < len(words):\n                entities.append(words[i + 1])\n\n        return entities\n\n    def fuse_modalities(self, visual_data, text_data):\n        \"\"\"Fuse visual and text modalities\"\"\"\n        if visual_data is None or text_data is None:\n            return None\n\n        # Simple fusion: compute similarity between visual and text embeddings\n        visual_emb = visual_data['embedding']\n        text_emb = text_data['embedding']\n\n        # Normalize embeddings\n        visual_emb_norm = visual_emb / np.linalg.norm(visual_emb)\n        text_emb_norm = text_emb / np.linalg.norm(text_emb)\n\n        # Compute similarity\n        similarity = np.dot(visual_emb_norm, text_emb_norm.T)\n\n        # Create fused representation\n        fused_data = {\n            'similarity': float(similarity[0][0]) if hasattr(similarity, '__len__') else float(similarity),\n            'visual_context': visual_data,\n            'text_context': text_data,\n            'grounding_confidence': self.calculate_grounding_confidence(visual_data, text_data)\n        }\n\n        return fused_data\n\n    def calculate_grounding_confidence(self, visual_data, text_data):\n        \"\"\"Calculate confidence in visual-language grounding\"\"\"\n        # Simple confidence calculation based on keyword matching\n        keywords = text_data['semantic']['keywords']\n        # In a real system, this would use more sophisticated grounding\n\n        # For now, return a placeholder confidence\n        return 0.8\n\n    def calculate_center_of_mass(self, binary_image):\n        \"\"\"Calculate center of mass of non-zero pixels\"\"\"\n        y, x = np.nonzero(binary_image)\n        if len(x) > 0 and len(y) > 0:\n            return (int(np.mean(x)), int(np.mean(y)))\n        return (0, 0)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultimodalEmbeddingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"vision-language-grounding",children:"Vision-Language Grounding"}),"\n",(0,a.jsx)(e.h3,{id:"object-grounding-and-referencing",children:"Object Grounding and Referencing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: multimodal_integration/vision_language_grounding.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom geometry_msgs.msg import Point\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport json\nfrom typing import Dict, Any, List, Optional\n\nclass VisionLanguageGroundingNode(Node):\n    def __init__(self):\n        super().__init__('vision_language_grounding_node')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, '/user/command', self.command_callback, 10)\n        self.detections_sub = self.create_subscription(\n            Detection2DArray, '/object/detections', self.detections_callback, 10)\n\n        # Publishers\n        self.grounded_objects_pub = self.create_publisher(\n            String, '/multimodal/grounded_objects', 10)\n        self.visualization_pub = self.create_publisher(\n            Image, '/multimodal/grounding_viz', 10)\n\n        # State\n        self.current_image = None\n        self.current_detections = None\n        self.pending_command = None\n\n        self.get_logger().info('Vision-Language Grounding Node Started')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image for grounding\"\"\"\n        try:\n            self.current_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        except Exception as e:\n            self.get_logger().error(f'Image conversion error: {e}')\n\n    def command_callback(self, msg):\n        \"\"\"Process command and perform grounding\"\"\"\n        command = msg.data\n        self.pending_command = command\n\n        # Perform grounding if we have both image and detections\n        if self.current_image is not None and self.current_detections is not None:\n            self.perform_grounding(command)\n\n    def detections_callback(self, msg):\n        \"\"\"Process object detections\"\"\"\n        self.current_detections = msg\n\n        # Process pending command if available\n        if self.pending_command is not None and self.current_image is not None:\n            self.perform_grounding(self.pending_command)\n            self.pending_command = None\n\n    def perform_grounding(self, command: str):\n        \"\"\"Perform vision-language grounding\"\"\"\n        if self.current_detections is None:\n            return\n\n        # Parse command to identify target object\n        target_object = self.parse_command_for_object(command)\n\n        if not target_object:\n            self.get_logger().info('No target object identified in command')\n            return\n\n        # Find matching detection\n        matched_detection = self.find_matching_detection(target_object)\n\n        if matched_detection:\n            # Create grounding result\n            grounding_result = {\n                'command': command,\n                'target_object': target_object,\n                'detection': self.detection_to_dict(matched_detection),\n                'confidence': self.calculate_grounding_confidence(target_object, matched_detection),\n                'spatial_relationship': self.analyze_spatial_relationship(matched_detection)\n            }\n\n            # Publish grounding result\n            grounding_msg = String()\n            grounding_msg.data = json.dumps(grounding_result)\n            self.grounded_objects_pub.publish(grounding_msg)\n\n            # Create visualization\n            viz_image = self.create_grounding_visualization(target_object, matched_detection)\n            viz_msg = self.cv_bridge.cv2_to_imgmsg(viz_image, \"bgr8\")\n            self.visualization_pub.publish(viz_msg)\n\n            self.get_logger().info(f'Grounded \"{target_object}\" with confidence {grounding_result[\"confidence\"]:.2f}')\n\n    def parse_command_for_object(self, command: str) -> Optional[str]:\n        \"\"\"Parse command to identify target object\"\"\"\n        # Simple object extraction (in practice, use NLP)\n        command_lower = command.lower()\n\n        # Look for object references\n        object_indicators = ['the', 'a', 'an']\n        words = command_lower.split()\n\n        # Common object categories that might be in commands\n        object_categories = [\n            'box', 'bottle', 'cup', 'chair', 'table', 'person', 'robot',\n            'object', 'item', 'thing', 'book', 'phone', 'laptop', 'monitor'\n        ]\n\n        for word in words:\n            if word in object_categories:\n                return word\n\n        # Look for color + object patterns\n        color_object_patterns = [\n            'red box', 'blue cup', 'green bottle', 'white chair', 'black table'\n        ]\n\n        for pattern in color_object_patterns:\n            if pattern in command_lower:\n                return pattern\n\n        return None\n\n    def find_matching_detection(self, target_object: str) -> Optional[Detection2D]:\n        \"\"\"Find detection that matches target object\"\"\"\n        if self.current_detections is None:\n            return None\n\n        target_lower = target_object.lower()\n\n        # Score each detection based on similarity to target\n        best_match = None\n        best_score = 0.0\n\n        for detection in self.current_detections.detections:\n            if detection.results:\n                class_name = detection.results[0].hypothesis.class_id.lower()\n                confidence = detection.results[0].hypothesis.score\n\n                # Calculate match score\n                score = self.calculate_match_score(target_lower, class_name, confidence)\n\n                if score > best_score:\n                    best_score = score\n                    best_match = detection\n\n        return best_match if best_score > 0.3 else None  # Threshold\n\n    def calculate_match_score(self, target: str, detected_class: str, confidence: float) -> float:\n        \"\"\"Calculate match score between target and detected object\"\"\"\n        score = 0.0\n\n        # Exact match\n        if target == detected_class:\n            score = confidence\n        # Partial match (e.g., \"box\" vs \"cardboard box\")\n        elif target in detected_class or detected_class in target:\n            score = confidence * 0.8\n        # Semantic similarity could be added here\n        else:\n            # Check for semantic similarity using simple rules\n            semantic_matches = {\n                'person': ['human', 'man', 'woman', 'person'],\n                'bottle': ['container', 'vessel', 'jug'],\n                'cup': ['mug', 'glass', 'vessel'],\n                'box': ['container', 'crate', 'carton']\n            }\n\n            for semantic_target, semantic_variants in semantic_matches.items():\n                if target == semantic_target and detected_class in semantic_variants:\n                    score = confidence * 0.7\n                    break\n                elif detected_class == semantic_target and target in semantic_variants:\n                    score = confidence * 0.7\n                    break\n\n        return score\n\n    def calculate_grounding_confidence(self, target_object: str, detection: Detection2D) -> float:\n        \"\"\"Calculate confidence in the grounding\"\"\"\n        if detection.results:\n            base_confidence = detection.results[0].hypothesis.score\n            # Additional factors could be added here\n            return base_confidence\n        return 0.0\n\n    def analyze_spatial_relationship(self, detection: Detection2D) -> Dict[str, Any]:\n        \"\"\"Analyze spatial relationship of detected object\"\"\"\n        bbox = detection.bbox\n        center_x = bbox.center.x\n        center_y = bbox.center.y\n\n        # Calculate relative position in image\n        img_width, img_height = 640, 480  # Assuming standard resolution\n\n        # Normalize coordinates\n        norm_x = center_x / img_width\n        norm_y = center_y / img_height\n\n        # Determine spatial relationship\n        position = []\n        if norm_x < 0.33:\n            position.append('left')\n        elif norm_x > 0.67:\n            position.append('right')\n        else:\n            position.append('center')\n\n        if norm_y < 0.33:\n            position.append('top')\n        elif norm_y > 0.67:\n            position.append('bottom')\n        else:\n            position.append('middle')\n\n        return {\n            'position': position,\n            'normalized_coordinates': [norm_x, norm_y],\n            'bbox': [bbox.center.x, bbox.center.y, bbox.size_x, bbox.size_y]\n        }\n\n    def create_grounding_visualization(self, target_object: str, detection: Detection2D) -> np.ndarray:\n        \"\"\"Create visualization showing grounding result\"\"\"\n        if self.current_image is None:\n            return np.zeros((480, 640, 3), dtype=np.uint8)\n\n        viz_image = self.current_image.copy()\n\n        # Draw bounding box\n        bbox = detection.bbox\n        x = int(bbox.center.x - bbox.size_x / 2)\n        y = int(bbox.center.y - bbox.size_y / 2)\n        w = int(bbox.size_x)\n        h = int(bbox.size_y)\n\n        cv2.rectangle(viz_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n        # Add label\n        label = f\"{target_object}\"\n        cv2.putText(viz_image, label, (x, y - 10),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        return viz_image\n\n    def detection_to_dict(self, detection: Detection2D) -> Dict[str, Any]:\n        \"\"\"Convert detection to dictionary for JSON serialization\"\"\"\n        result = {\n            'bbox': {\n                'center_x': detection.bbox.center.x,\n                'center_y': detection.bbox.center.y,\n                'size_x': detection.bbox.size_x,\n                'size_y': detection.bbox.size_y\n            },\n            'class': detection.results[0].hypothesis.class_id if detection.results else 'unknown',\n            'confidence': detection.results[0].hypothesis.score if detection.results else 0.0\n        }\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionLanguageGroundingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"action-language-integration",children:"Action-Language Integration"}),"\n",(0,a.jsx)(e.h3,{id:"natural-language-command-to-action-mapping",children:"Natural Language Command to Action Mapping"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: multimodal_integration/action_language_mapping.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import LaserScan\nimport json\nimport openai\nfrom typing import Dict, Any, List, Optional, Tuple\nimport re\n\nclass ActionLanguageMapperNode(Node):\n    def __init__(self):\n        super().__init__('action_language_mapper_node')\n\n        # Initialize OpenAI client\n        self.client = openai.OpenAI(api_key='your-api-key')\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String, '/natural/command', self.command_callback, 10)\n        self.context_sub = self.create_subscription(\n            String, '/multimodal/context', self.context_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10)\n\n        # Publishers\n        self.action_pub = self.create_publisher(\n            String, '/robot/action_sequence', 10)\n        self.feedback_pub = self.create_publisher(\n            String, '/voice/feedback', 10)\n\n        # State\n        self.context = {}\n        self.scan_data = None\n\n        self.get_logger().info('Action-Language Mapper Node Started')\n\n    def context_callback(self, msg):\n        \"\"\"Update context\"\"\"\n        try:\n            self.context = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in context message')\n\n    def scan_callback(self, msg):\n        \"\"\"Update laser scan data\"\"\"\n        self.scan_data = msg\n\n    def command_callback(self, msg):\n        \"\"\"Process natural language command and map to actions\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Processing command: {command}')\n\n        # Map command to action sequence\n        action_sequence = self.map_command_to_actions(command)\n\n        if action_sequence:\n            # Publish action sequence\n            action_msg = String()\n            action_msg.data = json.dumps(action_sequence)\n            self.action_pub.publish(action_msg)\n\n            # Provide feedback\n            feedback = f\"Understood command: {command}. Executing {len(action_sequence)} actions.\"\n            self.provide_feedback(feedback)\n        else:\n            feedback = f\"Could not understand command: {command}\"\n            self.provide_feedback(feedback)\n\n    def map_command_to_actions(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"Map natural language command to sequence of actions using LLM\"\"\"\n        context_info = self.get_context_info()\n\n        prompt = f\"\"\"\n        Convert this natural language command to a sequence of robot actions:\n        Command: \"{command}\"\n\n        Context:\n        {context_info}\n\n        Provide a JSON list of actions with the following format:\n        {{\n            \"action\": \"action_type\",\n            \"parameters\": {{\"param1\": \"value1\", \"param2\": \"value2\"}},\n            \"description\": \"Brief description of what this action does\"\n        }}\n\n        Available action types:\n        - navigation: move to a location\n        - manipulation: pick/place objects\n        - perception: look for/detect objects\n        - communication: speak/communicate\n        - wait: pause execution\n\n        Consider safety, environment constraints, and robot capabilities.\n        \"\"\"\n\n        try:\n            response = self.client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1\n            )\n\n            content = response.choices[0].message.content\n\n            # Extract JSON from response\n            if '```json' in content:\n                start = content.find('```json') + 7\n                end = content.find('```', start)\n                json_content = content[start:end].strip()\n            elif '```' in content:\n                start = content.find('```') + 3\n                end = content.find('```', start)\n                json_content = content[start:end].strip()\n            else:\n                json_content = content\n\n            return json.loads(json_content)\n\n        except Exception as e:\n            self.get_logger().error(f'LLM mapping error: {e}')\n            # Fallback to rule-based mapping\n            return self.rule_based_mapping(command)\n\n    def rule_based_mapping(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"Fallback rule-based command mapping\"\"\"\n        command_lower = command.lower()\n\n        # Navigation commands\n        if any(word in command_lower for word in ['go to', 'move to', 'navigate to', 'go', 'move']):\n            return self.parse_navigation_command(command)\n\n        # Manipulation commands\n        elif any(word in command_lower for word in ['pick up', 'grasp', 'take', 'pick']):\n            return self.parse_manipulation_command(command)\n\n        # Perception commands\n        elif any(word in command_lower for word in ['find', 'look for', 'see', 'detect', 'where']):\n            return self.parse_perception_command(command)\n\n        # Simple movement commands\n        elif any(word in command_lower for word in ['forward', 'backward', 'left', 'right', 'turn']):\n            return self.parse_movement_command(command)\n\n        else:\n            return []\n\n    def parse_navigation_command(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse navigation command\"\"\"\n        # Extract location using regex\n        location_patterns = [\n            r'to the (\\w+)',  # \"go to the kitchen\"\n            r'to (\\w+)',     # \"go to kitchen\"\n            r'(\\w+)',        # \"kitchen\" (if context suggests movement)\n        ]\n\n        location = None\n        for pattern in location_patterns:\n            match = re.search(pattern, command.lower())\n            if match:\n                location = match.group(1)\n                break\n\n        if location:\n            return [{\n                'action': 'navigation',\n                'parameters': {'target_location': location},\n                'description': f'Navigate to {location}'\n            }]\n        else:\n            return [{\n                'action': 'navigation',\n                'parameters': {'target_location': 'unknown'},\n                'description': 'Navigate to specified location'\n            }]\n\n    def parse_manipulation_command(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse manipulation command\"\"\"\n        # Extract object\n        object_patterns = [\n            r'pick up the (\\w+)',\n            r'pick up (\\w+)',\n            r'grasp the (\\w+)',\n            r'take the (\\w+)',\n        ]\n\n        obj = None\n        for pattern in object_patterns:\n            match = re.search(pattern, command.lower())\n            if match:\n                obj = match.group(1)\n                break\n\n        return [{\n            'action': 'manipulation',\n            'parameters': {'action_type': 'pick', 'object': obj or 'unknown'},\n            'description': f'Pick up {obj or \"object\"}'\n        }]\n\n    def parse_perception_command(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse perception command\"\"\"\n        # Extract object to find\n        object_patterns = [\n            r'find the (\\w+)',\n            r'look for the (\\w+)',\n            r'where is the (\\w+)',\n            r'find (\\w+)',\n        ]\n\n        obj = None\n        for pattern in object_patterns:\n            match = re.search(pattern, command.lower())\n            if match:\n                obj = match.group(1)\n                break\n\n        return [{\n            'action': 'perception',\n            'parameters': {'task': 'detection', 'target_object': obj or 'unknown'},\n            'description': f'Look for {obj or \"object\"}'\n        }]\n\n    def parse_movement_command(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse simple movement command\"\"\"\n        command_lower = command.lower()\n\n        if 'forward' in command_lower:\n            return [{\n                'action': 'movement',\n                'parameters': {'direction': 'forward', 'distance': 1.0},\n                'description': 'Move forward 1 meter'\n            }]\n        elif 'backward' in command_lower:\n            return [{\n                'action': 'movement',\n                'parameters': {'direction': 'backward', 'distance': 1.0},\n                'description': 'Move backward 1 meter'\n            }]\n        elif 'left' in command_lower:\n            return [{\n                'action': 'movement',\n                'parameters': {'direction': 'left', 'angle': 90},\n                'description': 'Turn left 90 degrees'\n            }]\n        elif 'right' in command_lower:\n            return [{\n                'action': 'movement',\n                'parameters': {'direction': 'right', 'angle': 90},\n                'description': 'Turn right 90 degrees'\n            }]\n        else:\n            return []\n\n    def get_context_info(self) -> str:\n        \"\"\"Get relevant context information\"\"\"\n        context_str = f\"\"\"\n        Robot State:\n        - Current position: {self.context.get('robot_position', 'unknown')}\n        - Battery level: {self.context.get('battery_level', 'unknown')}%\n        - Available capabilities: {self.context.get('capabilities', [])}\n\n        Environment:\n        - Detected objects: {self.context.get('detected_objects', [])}\n        - Navigation zones: {self.context.get('navigation_zones', {})}\n        - Obstacles: {len(self.scan_data.ranges) if self.scan_data else 0} range readings\n        \"\"\"\n        return context_str\n\n    def provide_feedback(self, feedback: str):\n        \"\"\"Provide feedback to user\"\"\"\n        feedback_msg = String()\n        feedback_msg.data = feedback\n        self.feedback_pub.publish(feedback_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ActionLanguageMapperNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"multimodal-decision-making",children:"Multimodal Decision Making"}),"\n",(0,a.jsx)(e.h3,{id:"cross-modal-reasoning-engine",children:"Cross-Modal Reasoning Engine"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: multimodal_integration/reasoning_engine.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import PoseStamped\nimport json\nimport openai\nimport numpy as np\nfrom typing import Dict, Any, List, Optional, Tuple\nimport threading\nimport time\n\nclass MultimodalReasoningNode(Node):\n    def __init__(self):\n        super().__init__('multimodal_reasoning_node')\n\n        # Initialize OpenAI client\n        self.client = openai.OpenAI(api_key='your-api-key')\n\n        # Subscribers\n        self.vision_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.vision_callback, 10)\n        self.language_sub = self.create_subscription(\n            String, '/user/request', self.language_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10)\n        self.pose_sub = self.create_subscription(\n            PoseStamped, '/current_pose', self.pose_callback, 10)\n\n        # Publishers\n        self.decision_pub = self.create_publisher(\n            String, '/multimodal/decision', 10)\n        self.plan_pub = self.create_publisher(\n            String, '/multimodal/action_plan', 10)\n\n        # State\n        self.multimodal_state = {\n            'vision_data': None,\n            'language_request': None,\n            'scan_data': None,\n            'robot_pose': None,\n            'timestamp': time.time()\n        }\n\n        # Lock for thread-safe state updates\n        self.state_lock = threading.Lock()\n\n        # Timer for periodic reasoning\n        self.reasoning_timer = self.create_timer(2.0, self.periodic_reasoning)\n\n        self.get_logger().info('Multimodal Reasoning Node Started')\n\n    def vision_callback(self, msg):\n        \"\"\"Update vision data\"\"\"\n        with self.state_lock:\n            self.multimodal_state['vision_data'] = {\n                'timestamp': time.time(),\n                'encoding': msg.encoding,\n                'height': msg.height,\n                'width': msg.width\n            }\n            self.multimodal_state['timestamp'] = time.time()\n\n    def language_callback(self, msg):\n        \"\"\"Update language request\"\"\"\n        with self.state_lock:\n            self.multimodal_state['language_request'] = msg.data\n            self.multimodal_state['timestamp'] = time.time()\n\n    def scan_callback(self, msg):\n        \"\"\"Update scan data\"\"\"\n        with self.state_lock:\n            self.multimodal_state['scan_data'] = {\n                'ranges_count': len(msg.ranges),\n                'min_range': min(msg.ranges) if msg.ranges else float('inf'),\n                'max_range': max(msg.ranges) if msg.ranges else 0,\n                'timestamp': time.time()\n            }\n            self.multimodal_state['timestamp'] = time.time()\n\n    def pose_callback(self, msg):\n        \"\"\"Update robot pose\"\"\"\n        with self.state_lock:\n            self.multimodal_state['robot_pose'] = {\n                'x': msg.pose.position.x,\n                'y': msg.pose.position.y,\n                'z': msg.pose.position.z,\n                'timestamp': time.time()\n            }\n            self.multimodal_state['timestamp'] = time.time()\n\n    def periodic_reasoning(self):\n        \"\"\"Perform periodic multimodal reasoning\"\"\"\n        with self.state_lock:\n            current_state = self.multimodal_state.copy()\n\n        # Only reason if we have recent data\n        if time.time() - current_state['timestamp'] < 5.0:  # 5 seconds\n            decision = self.perform_multimodal_reasoning(current_state)\n\n            if decision:\n                # Publish decision\n                decision_msg = String()\n                decision_msg.data = json.dumps(decision)\n                self.decision_pub.publish(decision_msg)\n\n                # Generate action plan\n                action_plan = self.generate_action_plan(decision, current_state)\n                if action_plan:\n                    plan_msg = String()\n                    plan_msg.data = json.dumps(action_plan)\n                    self.plan_pub.publish(plan_msg)\n\n    def perform_multimodal_reasoning(self, state: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"Perform multimodal reasoning using LLM\"\"\"\n        # Check if we have a language request to respond to\n        if not state.get('language_request'):\n            return None\n\n        # Build reasoning prompt\n        prompt = f\"\"\"\n        Perform multimodal reasoning based on the following information:\n\n        Language Request: \"{state.get('language_request', 'No request')}\"\n        Robot Position: {state.get('robot_pose', 'Unknown')}\n        Vision Data: {state.get('vision_data', 'No vision data')}\n        Scan Data: {state.get('scan_data', 'No scan data')}\n        Current Time: {time.time()}\n\n        Analyze the situation by combining visual, spatial, and linguistic information to:\n        1. Understand what the user wants\n        2. Assess the current situation\n        3. Determine if the request is feasible\n        4. Identify potential challenges or constraints\n\n        Provide your reasoning and a decision in JSON format:\n        {{\n            \"reasoning\": \"step-by-step analysis combining all modalities\",\n            \"feasibility\": true/false,\n            \"confidence\": 0.0-1.0,\n            \"action_needed\": \"type of action required\",\n            \"constraints\": [\"list of constraints\"],\n            \"safety_considerations\": [\"list of safety factors\"]\n        }}\n        \"\"\"\n\n        try:\n            response = self.client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.2\n            )\n\n            content = response.choices[0].message.content\n\n            # Extract JSON\n            if '```json' in content:\n                start = content.find('```json') + 7\n                end = content.find('```', start)\n                json_content = content[start:end].strip()\n            elif '```' in content:\n                start = content.find('```') + 3\n                end = content.find('```', start)\n                json_content = content[start:end].strip()\n            else:\n                json_content = content\n\n            return json.loads(json_content)\n\n        except Exception as e:\n            self.get_logger().error(f'Multimodal reasoning error: {e}')\n            return None\n\n    def generate_action_plan(self, decision: Dict[str, Any], state: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"Generate action plan based on decision\"\"\"\n        if not decision.get('feasibility', False):\n            return {\n                'plan': [],\n                'reason': 'Request not feasible',\n                'decision': decision\n            }\n\n        # Build planning prompt\n        prompt = f\"\"\"\n        Based on this decision and current state, generate an action plan:\n\n        Decision: {json.dumps(decision, indent=2)}\n        Current State: {json.dumps(state, indent=2)}\n\n        Create a detailed action plan as JSON:\n        {{\n            \"plan\": [\n                {{\n                    \"step\": 1,\n                    \"action\": \"action_type\",\n                    \"parameters\": {{\"param1\": \"value1\"}},\n                    \"reason\": \"why this action is needed\",\n                    \"expected_outcome\": \"what should happen\",\n                    \"safety_check\": \"what to verify\"\n                }}\n            ],\n            \"estimated_time\": \"time in seconds\",\n            \"success_criteria\": \"how to verify completion\",\n            \"fallback_actions\": [\"list of alternatives if primary fails\"]\n        }}\n        \"\"\"\n\n        try:\n            response = self.client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.1\n            )\n\n            content = response.choices[0].message.content\n\n            # Extract JSON\n            if '```json' in content:\n                start = content.find('```json') + 7\n                end = content.find('```', start)\n                json_content = content[start:end].strip()\n            elif '```' in content:\n                start = content.find('```') + 3\n                end = content.find('```', start)\n                json_content = content[start:end].strip()\n            else:\n                json_content = content\n\n            plan_data = json.loads(json_content)\n            plan_data['decision'] = decision  # Include original decision\n\n            return plan_data\n\n        except Exception as e:\n            self.get_logger().error(f'Action planning error: {e}')\n            return None\n\n    def validate_multimodal_integration(self, decision: Dict[str, Any]) -> bool:\n        \"\"\"Validate that the decision properly integrates multiple modalities\"\"\"\n        required_elements = ['reasoning', 'feasibility', 'confidence']\n\n        for element in required_elements:\n            if element not in decision:\n                return False\n\n        # Check that reasoning mentions multiple modalities\n        reasoning = decision.get('reasoning', '').lower()\n        modalities_mentioned = 0\n\n        if any(modality in reasoning for modality in ['vision', 'visual', 'see', 'image', 'camera']):\n            modalities_mentioned += 1\n        if any(modality in reasoning for modality in ['language', 'text', 'command', 'understand']):\n            modalities_mentioned += 1\n        if any(modality in reasoning for modality in ['position', 'location', 'pose', 'spatial', 'navigation']):\n            modalities_mentioned += 1\n        if any(modality in reasoning for modality in ['scan', 'obstacle', 'laser', 'distance', 'safety']):\n            modalities_mentioned += 1\n\n        # Should mention at least 2 modalities\n        return modalities_mentioned >= 2\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultimodalReasoningNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"real-time-multimodal-fusion",children:"Real-Time Multimodal Fusion"}),"\n",(0,a.jsx)(e.h3,{id:"synchronized-processing-pipeline",children:"Synchronized Processing Pipeline"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: multimodal_integration/synchronization.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom typing import Dict, Any, Optional\nimport time\nfrom collections import deque\nimport threading\n\nclass MultimodalSynchronizerNode(Node):\n    def __init__(self):\n        super().__init__('multimodal_synchronizer_node')\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10)\n        self.pose_sub = self.create_subscription(\n            PoseStamped, '/current_pose', self.pose_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, '/user/command', self.command_callback, 10)\n\n        # Publishers\n        self.fused_output_pub = self.create_publisher(\n            String, '/multimodal/synchronized_data', 10)\n\n        # Buffers for synchronization\n        self.image_buffer = deque(maxlen=10)\n        self.scan_buffer = deque(maxlen=10)\n        self.pose_buffer = deque(maxlen=10)\n        self.command_buffer = deque(maxlen=5)\n\n        # Timestamp tolerance for synchronization (in seconds)\n        self.sync_tolerance = 0.1\n\n        # Lock for thread safety\n        self.buffer_lock = threading.Lock()\n\n        # Timer for periodic fusion\n        self.fusion_timer = self.create_timer(0.2, self.perform_fusion)\n\n        self.get_logger().info('Multimodal Synchronizer Node Started')\n\n    def image_callback(self, msg):\n        \"\"\"Add image to buffer\"\"\"\n        with self.buffer_lock:\n            self.image_buffer.append({\n                'data': msg,\n                'timestamp': self.get_timestamp_from_msg(msg)\n            })\n\n    def scan_callback(self, msg):\n        \"\"\"Add scan to buffer\"\"\"\n        with self.buffer_lock:\n            self.scan_buffer.append({\n                'data': msg,\n                'timestamp': self.get_timestamp_from_msg(msg)\n            })\n\n    def pose_callback(self, msg):\n        \"\"\"Add pose to buffer\"\"\"\n        with self.buffer_lock:\n            self.pose_buffer.append({\n                'data': msg,\n                'timestamp': self.get_timestamp_from_msg(msg)\n            })\n\n    def command_callback(self, msg):\n        \"\"\"Add command to buffer\"\"\"\n        with self.buffer_lock:\n            self.command_buffer.append({\n                'data': msg,\n                'timestamp': time.time()  # Commands use system time\n            })\n\n    def perform_fusion(self):\n        \"\"\"Perform multimodal fusion with temporal synchronization\"\"\"\n        with self.buffer_lock:\n            # Find temporally aligned data\n            aligned_data = self.find_aligned_data()\n\n        if aligned_data:\n            # Perform fusion\n            fused_result = self.fuse_data(aligned_data)\n\n            if fused_result:\n                # Publish fused result\n                fused_msg = String()\n                fused_msg.data = fused_result\n                self.fused_output_pub.publish(fused_msg)\n\n    def find_aligned_data(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Find data that is temporally aligned\"\"\"\n        current_time = time.time()\n\n        # Find latest data of each type within tolerance\n        aligned = {}\n\n        # Find latest image\n        for item in reversed(self.image_buffer):\n            if current_time - item['timestamp'] <= self.sync_tolerance:\n                aligned['image'] = item['data']\n                break\n\n        # Find latest scan\n        for item in reversed(self.scan_buffer):\n            if current_time - item['timestamp'] <= self.sync_tolerance:\n                aligned['scan'] = item['data']\n                break\n\n        # Find latest pose\n        for item in reversed(self.pose_buffer):\n            if current_time - item['timestamp'] <= self.sync_tolerance:\n                aligned['pose'] = item['data']\n                break\n\n        # Use the most recent command (if any)\n        if self.command_buffer:\n            aligned['command'] = self.command_buffer[-1]['data']\n\n        # Must have at least image and scan for meaningful fusion\n        if 'image' in aligned and 'scan' in aligned:\n            return aligned\n\n        return None\n\n    def fuse_data(self, aligned_data: Dict[str, Any]) -> Optional[str]:\n        \"\"\"Fuse synchronized multimodal data\"\"\"\n        try:\n            fusion_result = {\n                'timestamp': time.time(),\n                'fused_data': {\n                    'image_info': {\n                        'encoding': aligned_data['image'].encoding,\n                        'dimensions': [aligned_data['image'].width, aligned_data['image'].height],\n                        'timestamp': self.get_timestamp_from_msg(aligned_data['image'])\n                    },\n                    'scan_info': {\n                        'ranges_count': len(aligned_data['scan'].ranges),\n                        'range_min': aligned_data['scan'].range_min,\n                        'range_max': aligned_data['scan'].range_max,\n                        'timestamp': self.get_timestamp_from_msg(aligned_data['scan'])\n                    },\n                    'pose_info': {\n                        'position': [\n                            aligned_data['pose'].pose.position.x,\n                            aligned_data['pose'].pose.position.y,\n                            aligned_data['pose'].pose.position.z\n                        ],\n                        'orientation': [\n                            aligned_data['pose'].pose.orientation.x,\n                            aligned_data['pose'].pose.orientation.y,\n                            aligned_data['pose'].pose.orientation.z,\n                            aligned_data['pose'].pose.orientation.w\n                        ],\n                        'timestamp': self.get_timestamp_from_msg(aligned_data['pose'])\n                    }\n                },\n                'has_command': 'command' in aligned_data\n            }\n\n            return json.dumps(fusion_result)\n\n        except Exception as e:\n            self.get_logger().error(f'Fusion error: {e}')\n            return None\n\n    def get_timestamp_from_msg(self, msg) -> float:\n        \"\"\"Extract timestamp from ROS message\"\"\"\n        if hasattr(msg.header, 'stamp'):\n            return float(msg.header.stamp.sec) + float(msg.header.stamp.nanosec) / 1e9\n        else:\n            return time.time()  # Fallback to current time\n\nimport json\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultimodalSynchronizerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices-for-multimodal-integration",children:"Best Practices for Multimodal Integration"}),"\n",(0,a.jsx)(e.h3,{id:"1-temporal-synchronization",children:"1. Temporal Synchronization"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Align data from different modalities based on timestamps"}),"\n",(0,a.jsx)(e.li,{children:"Use appropriate buffer sizes to handle latency differences"}),"\n",(0,a.jsx)(e.li,{children:"Implement interpolation for high-frequency modalities"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-uncertainty-management",children:"2. Uncertainty Management"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Track uncertainty in each modality"}),"\n",(0,a.jsx)(e.li,{children:"Use probabilistic fusion methods"}),"\n",(0,a.jsx)(e.li,{children:"Implement confidence-based decision making"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-computational-efficiency",children:"3. Computational Efficiency"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use appropriate processing frequencies for each modality"}),"\n",(0,a.jsx)(e.li,{children:"Implement early fusion vs. late fusion strategies"}),"\n",(0,a.jsx)(e.li,{children:"Optimize for real-time performance requirements"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"4-robustness",children:"4. Robustness"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Handle missing modality data gracefully"}),"\n",(0,a.jsx)(e.li,{children:"Implement fallback strategies"}),"\n",(0,a.jsx)(e.li,{children:"Validate cross-modal consistency"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,a.jsx)(e.p,{children:"Multimodal integration enables Physical AI systems to process and combine information from multiple sensory modalities simultaneously. The VLA framework provides a unified approach to integrating vision, language, and action capabilities. Successful multimodal systems require careful attention to temporal synchronization, cross-modal grounding, uncertainty management, and real-time performance optimization. The integration of these modalities enables robots to understand complex, natural commands and respond appropriately to their environment."}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:'Implement a multimodal system that can respond to commands like "Bring me the red cup on the table."'}),"\n",(0,a.jsx)(e.li,{children:"Create a vision-language grounding system that can identify and manipulate specific objects."}),"\n",(0,a.jsx)(e.li,{children:"Build a cross-modal reasoning system that combines perception and language for decision making."}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"In the next chapter, we'll work on the CAPSTONE project that integrates all the VLA concepts learned in Module 4 into a complete Physical AI system."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const a={},o=i.createContext(a);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);