"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[8151],{8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>i});var r=t(6540);const o={},a=r.createContext(o);function s(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),r.createElement(a.Provider,{value:e},n.children)}},9246:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"appendices/python-libraries-reference","title":"Appendix C: Python Libraries Reference","description":"Table of Contents","source":"@site/docs/appendices/python-libraries-reference.md","sourceDirName":"appendices","slug":"/appendices/python-libraries-reference","permalink":"/physical-ai-book/docs/appendices/python-libraries-reference","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/appendices/python-libraries-reference.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Appendix B: ROS 2 Command Reference","permalink":"/physical-ai-book/docs/appendices/ros2-command-reference"},"next":{"title":"Appendix D: Resources and Links","permalink":"/physical-ai-book/docs/appendices/resources-links"}}');var o=t(4848),a=t(8453);const s={sidebar_position:3},i="Appendix C: Python Libraries Reference",l={},c=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"ROS 2 Python Libraries",id:"ros-2-python-libraries",level:2},{value:"Core ROS 2 Libraries",id:"core-ros-2-libraries",level:3},{value:"Advanced ROS 2 Features",id:"advanced-ros-2-features",level:3},{value:"Computer Vision Libraries",id:"computer-vision-libraries",level:2},{value:"OpenCV",id:"opencv",level:3},{value:"Pillow (PIL)",id:"pillow-pil",level:3},{value:"Machine Learning Libraries",id:"machine-learning-libraries",level:2},{value:"PyTorch",id:"pytorch",level:3},{value:"Transformers (Hugging Face)",id:"transformers-hugging-face",level:3},{value:"Scikit-Learn",id:"scikit-learn",level:3},{value:"Robotics Libraries",id:"robotics-libraries",level:2},{value:"PyRobot",id:"pyrobot",level:3},{value:"Modern Robotics Library",id:"modern-robotics-library",level:3},{value:"Audio Processing Libraries",id:"audio-processing-libraries",level:2},{value:"Speech Recognition",id:"speech-recognition",level:3},{value:"Vosk for Offline Speech Recognition",id:"vosk-for-offline-speech-recognition",level:3},{value:"Data Processing Libraries",id:"data-processing-libraries",level:2},{value:"Pandas",id:"pandas",level:3},{value:"NumPy",id:"numpy",level:3},{value:"Visualization Libraries",id:"visualization-libraries",level:2},{value:"Matplotlib",id:"matplotlib",level:3},{value:"Seaborn for Statistical Plots",id:"seaborn-for-statistical-plots",level:3},{value:"Networking and Communication",id:"networking-and-communication",level:2},{value:"ZeroMQ",id:"zeromq",level:3},{value:"WebSockets",id:"websockets",level:3},{value:"System and Hardware Libraries",id:"system-and-hardware-libraries",level:2},{value:"Serial Communication",id:"serial-communication",level:3},{value:"GPIO (for Raspberry Pi)",id:"gpio-for-raspberry-pi",level:3},{value:"Utility Libraries",id:"utility-libraries",level:2},{value:"Configuration Management",id:"configuration-management",level:3},{value:"Logging and Monitoring",id:"logging-and-monitoring",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:2},{value:"Requirements Management",id:"requirements-management",level:3},{value:"Virtual Environment Setup",id:"virtual-environment-setup",level:3},{value:"Docker Setup for Isolated Environments",id:"docker-setup-for-isolated-environments",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Error Handling and Logging",id:"1-error-handling-and-logging",level:3},{value:"2. Resource Management",id:"2-resource-management",level:3},{value:"3. Configuration Validation",id:"3-configuration-validation",level:3}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"appendix-c-python-libraries-reference",children:"Appendix C: Python Libraries Reference"})}),"\n",(0,o.jsx)(e.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#ros-2-python-libraries",children:"ROS 2 Python Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#computer-vision-libraries",children:"Computer Vision Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#machine-learning-libraries",children:"Machine Learning Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#robotics-libraries",children:"Robotics Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#audio-processing-libraries",children:"Audio Processing Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#data-processing-libraries",children:"Data Processing Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#visualization-libraries",children:"Visualization Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#networking-and-communication",children:"Networking and Communication"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#system-and-hardware-libraries",children:"System and Hardware Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#utility-libraries",children:"Utility Libraries"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#installation-and-setup",children:"Installation and Setup"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"#best-practices",children:"Best Practices"})}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"ros-2-python-libraries",children:"ROS 2 Python Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"core-ros-2-libraries",children:"Core ROS 2 Libraries"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Essential imports for ROS 2 Python nodes\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Int32, Float32, Bool\nfrom geometry_msgs.msg import Twist, Pose, Point, Vector3\nfrom sensor_msgs.msg import Image, LaserScan, PointCloud2, CameraInfo\nfrom nav_msgs.msg import Odometry, Path\nfrom geometry_msgs.msg import PoseStamped, TransformStamped\nfrom tf2_ros import TransformBroadcaster, Buffer, TransformListener\nfrom tf2_geometry_msgs import do_transform_pose\nfrom builtin_interfaces.msg import Time\nimport message_filters\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\n\n# Example: Basic ROS 2 Node\nclass MyRobotNode(Node):\n    def __init__(self):\n        super().__init__('my_robot_node')\n\n        # Create publisher\n        self.publisher = self.create_publisher(String, 'topic_name', 10)\n\n        # Create subscriber\n        self.subscription = self.create_subscription(\n            String,\n            'input_topic',\n            self.listener_callback,\n            10\n        )\n\n        # Create timer\n        self.timer = self.create_timer(0.5, self.timer_callback)\n\n        self.i = 0\n\n    def listener_callback(self, msg):\n        self.get_logger().info(f'Received: {msg.data}')\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = f'Hello World: {self.i}'\n        self.publisher.publish(msg)\n        self.get_logger().info(f'Publishing: {msg.data}')\n        self.i += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MyRobotNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"advanced-ros-2-features",children:"Advanced ROS 2 Features"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Services and Actions\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\nfrom rclpy.callback_groups import MutuallyExclusiveCallbackGroup, ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nfrom rclpy.parameter import Parameter\nfrom rclpy.time import Time\nfrom rclpy.duration import Duration\n\n# Example: Parameter declaration and usage\nclass ParameterNode(Node):\n    def __init__(self):\n        super().__init__('parameter_node')\n\n        # Declare parameters with descriptions\n        self.declare_parameter(\n            'robot_name',\n            'default_robot',\n            ParameterDescriptor(description='Name of the robot')\n        )\n\n        self.declare_parameter('max_speed', 1.0)\n        self.declare_parameter('operating_mode', 'autonomous')\n\n    def get_config(self):\n        name = self.get_parameter('robot_name').value\n        speed = self.get_parameter('max_speed').value\n        mode = self.get_parameter('operating_mode').value\n        return {'name': name, 'speed': speed, 'mode': mode}\n\n# Example: Service server\nfrom std_srvs.srv import SetBool, Trigger\n\nclass ServiceNode(Node):\n    def __init__(self):\n        super().__init__('service_node')\n\n        self.srv = self.create_service(\n            SetBool,\n            'toggle_service',\n            self.toggle_callback\n        )\n\n    def toggle_callback(self, request, response):\n        self.get_logger().info(f'Request: {request.data}')\n        response.success = True\n        response.message = f'Toggled to {request.data}'\n        return response\n"})}),"\n",(0,o.jsx)(e.h2,{id:"computer-vision-libraries",children:"Computer Vision Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"opencv",children:"OpenCV"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import cv2\nimport numpy as np\nfrom cv2 import aruco\nimport matplotlib.pyplot as plt\n\n# Basic image processing\ndef basic_cv_operations(image_path):\n    # Load image\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Resize\n    resized = cv2.resize(img, (640, 480))\n\n    # Gaussian blur\n    blurred = cv2.GaussianBlur(img, (15, 15), 0)\n\n    # Edge detection\n    edges = cv2.Canny(gray, 50, 150)\n\n    # Contours\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Draw contours\n    contour_img = img.copy()\n    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n\n    return {\n        'original': img,\n        'gray': gray,\n        'resized': resized,\n        'blurred': blurred,\n        'edges': edges,\n        'contours': contour_img\n    }\n\n# Feature detection\ndef feature_detection(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # ORB detector\n    orb = cv2.ORB_create(nfeatures=1000)\n    kp, des = orb.detectAndCompute(gray, None)\n\n    # Draw keypoints\n    kp_img = cv2.drawKeypoints(img, kp, None)\n\n    # SIFT detector (if available)\n    try:\n        sift = cv2.SIFT_create()\n        kp_sift, des_sift = sift.detectAndCompute(gray, None)\n        kp_sift_img = cv2.drawKeypoints(img, kp_sift, None)\n    except:\n        kp_sift_img = None\n\n    return {\n        'orb_keypoints': kp_img,\n        'sift_keypoints': kp_sift_img,\n        'keypoints': kp,\n        'descriptors': des\n    }\n\n# Object detection with Haar Cascades\ndef face_detection(img):\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n\n    for (x, y, w, h) in faces:\n        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n\n    return img, faces\n\n# ArUco marker detection\ndef aruco_detection(img):\n    aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)\n    parameters = aruco.DetectorParameters_create()\n\n    corners, ids, rejectedImgPoints = aruco.detectMarkers(img, aruco_dict, parameters=parameters)\n\n    if ids is not None:\n        img = aruco.drawDetectedMarkers(img, corners)\n\n    return img, corners, ids\n"})}),"\n",(0,o.jsx)(e.h3,{id:"pillow-pil",children:"Pillow (PIL)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"from PIL import Image, ImageDraw, ImageFont, ImageFilter\nimport numpy as np\n\ndef pil_image_processing(image_path):\n    # Open image\n    img = Image.open(image_path)\n\n    # Basic operations\n    width, height = img.size\n    mode = img.mode\n\n    # Resize\n    resized = img.resize((800, 600))\n\n    # Crop\n    cropped = img.crop((100, 100, 400, 400))\n\n    # Rotate\n    rotated = img.rotate(45, expand=True)\n\n    # Convert to different mode\n    grayscale = img.convert('L')\n    rgba = img.convert('RGBA')\n\n    # Apply filters\n    blurred = img.filter(ImageFilter.BLUR)\n    sharpened = img.filter(ImageFilter.SHARPEN)\n    edge_enhanced = img.filter(ImageFilter.EDGE_ENHANCE)\n\n    # Draw on image\n    draw = ImageDraw.Draw(img)\n    draw.rectangle([50, 50, 200, 200], outline='red', width=3)\n    draw.text((10, 10), \"Hello PIL!\", fill='blue')\n\n    return {\n        'resized': resized,\n        'cropped': cropped,\n        'rotated': rotated,\n        'grayscale': grayscale,\n        'filtered': {\n            'blur': blurred,\n            'sharpen': sharpened,\n            'edge': edge_enhanced\n        },\n        'annotated': img\n    }\n\n# Convert between PIL and OpenCV\ndef pil_to_cv2(pil_image):\n    \"\"\"Convert PIL image to OpenCV format\"\"\"\n    return np.array(pil_image)[:, :, ::-1]  # RGB to BGR\n\ndef cv2_to_pil(cv2_image):\n    \"\"\"Convert OpenCV image to PIL format\"\"\"\n    return Image.fromarray(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n"})}),"\n",(0,o.jsx)(e.h2,{id:"machine-learning-libraries",children:"Machine Learning Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"pytorch",children:"PyTorch"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Basic neural network\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Convolutional Neural Network\nclass CNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.fc1 = nn.Linear(64 * 8 * 8, 512)  # Adjust based on input size\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = x.view(-1, 64 * 8 * 8)  # Flatten\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Training loop example\ndef train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n\n# Custom Dataset\nclass CustomDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n# Data loading\ndef create_dataloader(X, y, batch_size=32, shuffle=True):\n    dataset = CustomDataset(X, y)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n# Model saving and loading\ndef save_model(model, filepath):\n    torch.save(model.state_dict(), filepath)\n\ndef load_model(model_class, filepath, input_size, hidden_size, output_size):\n    model = model_class(input_size, hidden_size, output_size)\n    model.load_state_dict(torch.load(filepath))\n    return model\n"})}),"\n",(0,o.jsx)(e.h3,{id:"transformers-hugging-face",children:"Transformers (Hugging Face)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\n\n# Text classification pipeline\ndef text_classifier():\n    classifier = pipeline("sentiment-analysis")\n    result = classifier("I love this product!")\n    return result\n\n# Named Entity Recognition\ndef ner_pipeline():\n    ner = pipeline("ner", grouped_entities=True)\n    result = ner("John Doe works at Google in New York.")\n    return result\n\n# Question Answering\ndef qa_pipeline():\n    qa = pipeline("question-answering")\n    context = "The Amazon rainforest is a moist broadleaf tropical rainforest."\n    question = "What type of forest is the Amazon?"\n    result = qa(question=question, context=context)\n    return result\n\n# CLIP for vision-language tasks\ndef clip_example(image_path, text_prompts):\n    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n    image = Image.open(image_path)\n    inputs = processor(text=text_prompts, images=image, return_tensors="pt", padding=True)\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs.detach().cpu().numpy()\n\n# Fine-tuning example\ndef fine_tune_model():\n    from transformers import Trainer, TrainingArguments\n\n    model_name = "bert-base-uncased"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=2  # binary classification\n    )\n\n    training_args = TrainingArguments(\n        output_dir=\'./results\',\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir=\'./logs\',\n    )\n\n    # trainer = Trainer(\n    #     model=model,\n    #     args=training_args,\n    #     train_dataset=train_dataset,\n    #     eval_dataset=eval_dataset\n    # )\n    #\n    # trainer.train()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"scikit-learn",children:"Scikit-Learn"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport numpy as np\nimport pandas as pd\n\n# Data preprocessing\ndef preprocess_data(X, y=None, scale=True, encode_labels=True):\n    scaler = StandardScaler() if scale else None\n    label_encoder = LabelEncoder() if encode_labels and y is not None else None\n\n    X_processed = X.copy()\n\n    if scale:\n        X_processed = scaler.fit_transform(X)\n\n    y_processed = y\n    if encode_labels and y is not None:\n        y_processed = label_encoder.fit_transform(y)\n\n    return X_processed, y_processed, scaler, label_encoder\n\n# Classification example\ndef classification_example(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Train models\n    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    lr_model = LogisticRegression(random_state=42)\n\n    rf_model.fit(X_train_scaled, y_train)\n    lr_model.fit(X_train_scaled, y_train)\n\n    # Predictions\n    rf_pred = rf_model.predict(X_test_scaled)\n    lr_pred = lr_model.predict(X_test_scaled)\n\n    # Evaluation\n    rf_accuracy = accuracy_score(y_test, rf_pred)\n    lr_accuracy = accuracy_score(y_test, lr_pred)\n\n    return {\n        'rf_accuracy': rf_accuracy,\n        'lr_accuracy': lr_accuracy,\n        'rf_report': classification_report(y_test, rf_pred),\n        'rf_cm': confusion_matrix(y_test, rf_pred),\n        'models': {'rf': rf_model, 'lr': lr_model},\n        'scaler': scaler\n    }\n\n# Clustering example\ndef clustering_example(X, n_clusters=3):\n    # Scale the data\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(X_scaled)\n\n    # Dimensionality reduction for visualization\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled)\n\n    return {\n        'cluster_labels': cluster_labels,\n        'kmeans_model': kmeans,\n        'pca_model': pca,\n        'pca_transformed': X_pca,\n        'inertia': kmeans.inertia_\n    }\n\n# Feature selection\ndef feature_importance_selection(X, y, n_features=10):\n    from sklearn.feature_selection import SelectKBest, f_classif\n\n    selector = SelectKBest(score_func=f_classif, k=n_features)\n    X_selected = selector.fit_transform(X, y)\n\n    # Get selected feature indices\n    selected_indices = selector.get_support(indices=True)\n\n    return {\n        'selected_features': X_selected,\n        'selected_indices': selected_indices,\n        'selector': selector\n    }\n"})}),"\n",(0,o.jsx)(e.h2,{id:"robotics-libraries",children:"Robotics Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"pyrobot",children:"PyRobot"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Note: PyRobot needs to be installed separately\n# pip install pyrobot\n\ntry:\n    from pyrobot import Robot\n    import numpy as np\n\n    def pyrobot_example():\n        # Initialize robot (example with LoCoBot)\n        robot = Robot(\'locobot\')\n\n        # Get robot state\n        pose = robot.base.get_pose()\n        joint_positions = robot.arm.get_joint_positions()\n\n        # Move the robot\n        robot.base.go_to_relative([0.5, 0, 0])  # Move forward 0.5m\n\n        # Move the arm\n        robot.arm.set_joint_positions([0, 0, 0, 0, 0])  # Home position\n\n        # Get camera images\n        rgb, depth = robot.camera.get_rgb_depth()\n\n        # Close gripper\n        robot.gripper.close()\n\n        return {\n            \'pose\': pose,\n            \'joints\': joint_positions,\n            \'rgb\': rgb,\n            \'depth\': depth\n        }\nexcept ImportError:\n    print("PyRobot not available")\n\n# Custom robot interface\nclass SimpleRobotInterface:\n    def __init__(self):\n        self.position = [0, 0, 0]\n        self.orientation = [0, 0, 0, 1]  # quaternion\n        self.joint_angles = [0] * 6  # 6 DOF arm\n\n    def move_to_position(self, x, y, z):\n        """Move robot to absolute position"""\n        self.position = [x, y, z]\n        print(f"Moved to position: {self.position}")\n\n    def move_by_offset(self, dx, dy, dz):\n        """Move robot by offset"""\n        self.position[0] += dx\n        self.position[1] += dy\n        self.position[2] += dz\n        print(f"Moved by offset to: {self.position}")\n\n    def get_position(self):\n        """Get current position"""\n        return self.position.copy()\n\n    def set_joint_angles(self, angles):\n        """Set joint angles for manipulator"""\n        if len(angles) == len(self.joint_angles):\n            self.joint_angles = angles.copy()\n            print(f"Set joint angles: {self.joint_angles}")\n        else:\n            raise ValueError(f"Expected {len(self.joint_angles)} angles, got {len(angles)}")\n\n    def get_joint_angles(self):\n        """Get current joint angles"""\n        return self.joint_angles.copy()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"modern-robotics-library",children:"Modern Robotics Library"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Modern Robotics Library functions\nimport numpy as np\n\ndef modern_robotics_examples():\n    """Examples using modern robotics concepts"""\n\n    # 1. Transformation matrices\n    def vec_to_so3(omega):\n        """Convert 3-vector to so(3) matrix"""\n        return np.array([[0, -omega[2], omega[1]],\n                         [omega[2], 0, -omega[0]],\n                         [-omega[1], omega[0], 0]])\n\n    def matrix_exp3(so3_mat):\n        """Matrix exponential for so(3)"""\n        omgtheta = so3_mat\n        if np.allclose(omgtheta, 0):\n            return np.eye(3)\n        else:\n            theta = np.linalg.norm([omgtheta[2, 1], omgtheta[0, 2], omgtheta[1, 0]])\n            omgmat = omgtheta / theta\n            return np.eye(3) + np.sin(theta) * omgmat + (1 - np.cos(theta)) * np.dot(omgmat, omgmat)\n\n    # 2. Screw motion\n    def screw_to_matrix(screw_axis, theta):\n        """Convert screw axis and angle to transformation matrix"""\n        omega = screw_axis[:3]\n        v = screw_axis[3:]\n\n        so3_omega = vec_to_so3(omega)\n        exp_so3 = matrix_exp3(so3_omega)\n\n        # Calculate translation part\n        if np.allclose(omega, 0):\n            trans = v * theta\n        else:\n            omega_norm = np.linalg.norm(omega)\n            omega_unit = omega / omega_norm\n            so3_omega_unit = vec_to_so3(omega_unit)\n            trans = np.dot(np.eye(3) * theta +\n                          (1 - np.cos(theta * omega_norm)) / (omega_norm**2) * so3_omega_unit +\n                          (theta * omega_norm - np.sin(theta * omega_norm)) / (omega_norm**3) *\n                          np.dot(so3_omega_unit, so3_omega_unit), v)\n\n        T = np.eye(4)\n        T[:3, :3] = exp_so3\n        T[:3, 3] = trans\n        return T\n\n    # 3. Forward kinematics example\n    def forward_kinematics(joint_angles, screw_axes, M):\n        """Compute forward kinematics"""\n        T = M.copy()\n        for i, (theta, screw_axis) in enumerate(zip(joint_angles, screw_axes)):\n            exp_ti = screw_to_matrix(screw_axis, theta)\n            T = np.dot(T, exp_ti)\n        return T\n\n    # Example usage\n    # Define a simple 2-link planar arm\n    joint_angles = [np.pi/4, np.pi/6]  # 45\xb0 and 30\xb0\n\n    # Screw axes for revolute joints (omega, v) - simplified example\n    screw_axes = [\n        np.array([0, 0, 1, 0, 0, 0]),  # First joint: rotation about z-axis\n        np.array([0, 0, 1, 0, 0, 0])   # Second joint: rotation about z-axis\n    ]\n\n    # Initial transformation matrix\n    M = np.eye(4)\n\n    # Compute forward kinematics\n    end_effector_pose = forward_kinematics(joint_angles, screw_axes, M)\n\n    return {\n        \'end_effector_pose\': end_effector_pose,\n        \'joint_angles\': joint_angles\n    }\n'})}),"\n",(0,o.jsx)(e.h2,{id:"audio-processing-libraries",children:"Audio Processing Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"speech-recognition",children:"Speech Recognition"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import speech_recognition as sr\nimport pyttsx3\nimport pyaudio\nimport wave\n\n# Speech recognition\ndef speech_to_text():\n    recognizer = sr.Recognizer()\n    microphone = sr.Microphone()\n\n    with microphone as source:\n        print(\"Adjusting for ambient noise...\")\n        recognizer.adjust_for_ambient_noise(source)\n        print(\"Listening...\")\n        audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)\n\n    try:\n        # Use Google's speech recognition\n        text = recognizer.recognize_google(audio)\n        print(f\"Recognized: {text}\")\n        return text\n    except sr.UnknownValueError:\n        print(\"Could not understand audio\")\n        return None\n    except sr.RequestError as e:\n        print(f\"Could not request results: {e}\")\n        return None\n\n# Text to speech\ndef text_to_speech(text, rate=200, volume=0.9):\n    engine = pyttsx3.init()\n\n    # Set properties\n    engine.setProperty('rate', rate)  # Speed of speech\n    engine.setProperty('volume', volume)  # Volume level (0.0 to 1.0)\n\n    # Get available voices\n    voices = engine.getProperty('voices')\n    engine.setProperty('voice', voices[0].id)  # Change voice if needed\n\n    # Speak the text\n    engine.say(text)\n    engine.runAndWait()\n\n# Audio recording\ndef record_audio(filename, duration=5, sample_rate=44100, chunk_size=1024):\n    audio = pyaudio.PyAudio()\n\n    # Open stream\n    stream = audio.open(\n        format=pyaudio.paInt16,\n        channels=1,\n        rate=sample_rate,\n        input=True,\n        frames_per_buffer=chunk_size\n    )\n\n    print(\"Recording...\")\n    frames = []\n\n    for i in range(0, int(sample_rate / chunk_size * duration)):\n        data = stream.read(chunk_size)\n        frames.append(data)\n\n    print(\"Finished recording\")\n\n    # Stop stream\n    stream.stop_stream()\n    stream.close()\n    audio.terminate()\n\n    # Save to file\n    with wave.open(filename, 'wb') as wf:\n        wf.setnchannels(1)\n        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n        wf.setframerate(sample_rate)\n        wf.writeframes(b''.join(frames))\n\n# Audio processing\ndef analyze_audio(filename):\n    import librosa\n    import numpy as np\n\n    # Load audio\n    y, sr = librosa.load(filename)\n\n    # Extract features\n    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n\n    return {\n        'tempo': tempo,\n        'beats': beats,\n        'mfccs': mfccs,\n        'spectral_centroids': spectral_centroids,\n        'chroma': chroma,\n        'duration': librosa.get_duration(y=y, sr=sr)\n    }\n"})}),"\n",(0,o.jsx)(e.h3,{id:"vosk-for-offline-speech-recognition",children:"Vosk for Offline Speech Recognition"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Note: Vosk needs to be installed separately\n# pip install vosk\n\ntry:\n    from vosk import Model, KaldiRecognizer\n    import json\n    import sys\n\n    def vosk_offline_recognition(audio_file_path, model_path="model"):\n        """Offline speech recognition using Vosk"""\n\n        # Load model\n        model = Model(model_path)\n\n        # Create recognizer\n        rec = KaldiRecognizer(model, 16000)  # Sample rate: 16kHz\n\n        # Open audio file\n        wf = open(audio_file_path, "rb")\n\n        results = []\n\n        while True:\n            data = wf.read(4000)\n            if len(data) == 0:\n                break\n            if rec.AcceptWaveform(data):\n                results.append(json.loads(rec.Result()))\n\n        # Final result\n        results.append(json.loads(rec.FinalResult()))\n\n        wf.close()\n\n        # Extract text from results\n        text_results = []\n        for result in results:\n            if \'text\' in result and result[\'text\']:\n                text_results.append(result[\'text\'])\n\n        return \' \'.join(text_results)\n\nexcept ImportError:\n    print("Vosk not available - install with: pip install vosk")\n\n# Audio streaming with Vosk\ndef vosk_microphone_streaming(model_path="model"):\n    """Real-time speech recognition from microphone"""\n    import pyaudio\n\n    model = Model(model_path)\n    rec = KaldiRecognizer(model, 16000)\n\n    p = pyaudio.PyAudio()\n    stream = p.open(format=pyaudio.paInt16,\n                    channels=1,\n                    rate=16000,\n                    input=True,\n                    frames_per_buffer=8000)\n\n    print("Listening... Press Ctrl+C to stop")\n\n    try:\n        while True:\n            data = stream.read(4000)\n            if len(data) == 0:\n                break\n            if rec.AcceptWaveform(data):\n                result = rec.Result()\n                print(json.loads(result)[\'text\'])\n            else:\n                # Partial result\n                partial = rec.PartialResult()\n                # Uncomment to see partial results\n                # print(json.loads(partial)[\'partial\'])\n    except KeyboardInterrupt:\n        print("\\nStopped listening")\n    finally:\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"data-processing-libraries",children:"Data Processing Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"pandas",children:"Pandas"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# DataFrame creation and manipulation\ndef create_robot_dataframe():\n    \"\"\"Create a sample robot data DataFrame\"\"\"\n\n    # Sample data\n    timestamps = pd.date_range(start='2023-01-01', periods=100, freq='1S')\n    positions_x = np.cumsum(np.random.normal(0, 0.1, 100)).cumsum()\n    positions_y = np.cumsum(np.random.normal(0, 0.1, 100)).cumsum()\n    velocities = np.random.normal(0.5, 0.2, 100)\n    battery_levels = np.linspace(100, 20, 100)  # Decreasing battery\n\n    df = pd.DataFrame({\n        'timestamp': timestamps,\n        'position_x': positions_x,\n        'position_y': positions_y,\n        'velocity': velocities,\n        'battery_level': battery_levels,\n        'status': np.random.choice(['active', 'charging', 'idle'], 100)\n    })\n\n    return df\n\ndef analyze_robot_data(df):\n    \"\"\"Analyze robot operational data\"\"\"\n\n    # Basic statistics\n    stats = df.describe()\n\n    # Time-based analysis\n    df['hour'] = df['timestamp'].dt.hour\n    df['day_of_week'] = df['timestamp'].dt.dayofweek\n\n    # Group by status\n    status_analysis = df.groupby('status').agg({\n        'velocity': ['mean', 'std', 'min', 'max'],\n        'battery_level': ['mean', 'min', 'max'],\n        'position_x': ['mean', 'std'],\n        'position_y': ['mean', 'std']\n    }).round(3)\n\n    # Velocity trends\n    df['velocity_smoothed'] = df['velocity'].rolling(window=5).mean()\n\n    # Battery consumption rate\n    df['battery_consumption_rate'] = df['battery_level'].diff().abs() / df['timestamp'].diff().dt.total_seconds()\n\n    return {\n        'basic_stats': stats,\n        'status_analysis': status_analysis,\n        'smoothed_velocity': df['velocity_smoothed'],\n        'consumption_rate': df['battery_consumption_rate'].mean()\n    }\n\n# Data cleaning and preprocessing\ndef clean_robot_data(df):\n    \"\"\"Clean and preprocess robot data\"\"\"\n\n    # Remove duplicates\n    df_clean = df.drop_duplicates(subset=['timestamp'])\n\n    # Handle missing values\n    df_clean = df_clean.fillna(method='forward')  # Forward fill\n    df_clean = df_clean.fillna(0)  # Fill remaining with 0\n\n    # Remove outliers using IQR method\n    for col in ['position_x', 'position_y', 'velocity']:\n        Q1 = df_clean[col].quantile(0.25)\n        Q3 = df_clean[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n\n    # Normalize numerical columns\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    numerical_cols = ['position_x', 'position_y', 'velocity', 'battery_level']\n    df_clean[numerical_cols] = scaler.fit_transform(df_clean[numerical_cols])\n\n    return df_clean\n\n# Export and import\ndef save_robot_data(df, filename):\n    \"\"\"Save robot data to various formats\"\"\"\n\n    # Save as CSV\n    df.to_csv(f\"{filename}.csv\", index=False)\n\n    # Save as Parquet (more efficient for large datasets)\n    df.to_parquet(f\"{filename}.parquet\", index=False)\n\n    # Save as Excel\n    df.to_excel(f\"{filename}.xlsx\", index=False)\n\n    # Save specific columns\n    df[['timestamp', 'position_x', 'position_y']].to_json(f\"{filename}_positions.json\", orient='records', date_format='iso')\n\ndef load_robot_data(filename):\n    \"\"\"Load robot data from various formats\"\"\"\n\n    if filename.endswith('.csv'):\n        return pd.read_csv(filename)\n    elif filename.endswith('.parquet'):\n        return pd.read_parquet(filename)\n    elif filename.endswith('.xlsx'):\n        return pd.read_excel(filename)\n    elif filename.endswith('.json'):\n        return pd.read_json(filename)\n    else:\n        raise ValueError(\"Unsupported file format\")\n"})}),"\n",(0,o.jsx)(e.h3,{id:"numpy",children:"NumPy"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy import ndimage, signal\nimport matplotlib.pyplot as plt\n\n# Array creation and manipulation\ndef numpy_robotics_examples():\n    """Examples of NumPy for robotics applications"""\n\n    # Create transformation matrices\n    def create_rotation_matrix(angle, axis=\'z\'):\n        """Create 2D or 3D rotation matrix"""\n        if axis == \'z\':\n            return np.array([\n                [np.cos(angle), -np.sin(angle), 0],\n                [np.sin(angle), np.cos(angle), 0],\n                [0, 0, 1]\n            ])\n        elif axis == \'x\':\n            return np.array([\n                [1, 0, 0],\n                [0, np.cos(angle), -np.sin(angle)],\n                [0, np.sin(angle), np.cos(angle)]\n            ])\n        elif axis == \'y\':\n            return np.array([\n                [np.cos(angle), 0, np.sin(angle)],\n                [0, 1, 0],\n                [-np.sin(angle), 0, np.cos(angle)]\n            ])\n\n    # Homogeneous transformation\n    def create_transform(rotation_matrix, translation):\n        """Create 4x4 homogeneous transformation matrix"""\n        transform = np.eye(4)\n        transform[:3, :3] = rotation_matrix[:3, :3]\n        transform[:3, 3] = translation\n        return transform\n\n    # Vector operations\n    def normalize_vector(v):\n        """Normalize a vector"""\n        norm = np.linalg.norm(v)\n        if norm == 0:\n            return v\n        return v / norm\n\n    def rotate_vector(vector, angle, axis=\'z\'):\n        """Rotate a vector by given angle around specified axis"""\n        rot_matrix = create_rotation_matrix(angle, axis)\n        # Only use 3x3 rotation part for vector rotation\n        return np.dot(rot_matrix[:3, :3], vector)\n\n    # Example usage\n    angle = np.pi / 4  # 45 degrees\n    rotation = create_rotation_matrix(angle, \'z\')\n    translation = np.array([1, 2, 0])\n    transform = create_transform(rotation, translation)\n\n    # Rotate a point\n    point = np.array([1, 0, 0])\n    rotated_point = rotate_vector(point, angle, \'z\')\n\n    return {\n        \'rotation_matrix\': rotation,\n        \'transform_matrix\': transform,\n        \'original_point\': point,\n        \'rotated_point\': rotated_point\n    }\n\n# Signal processing with NumPy/SciPy\ndef signal_processing_examples():\n    """Signal processing examples for sensor data"""\n\n    # Generate sample sensor data\n    time = np.linspace(0, 10, 1000)\n    # Simulate noisy sensor reading with trend\n    signal_data = np.sin(2 * np.pi * 0.5 * time) + 0.1 * np.random.randn(len(time))\n    signal_data += 0.02 * time  # Add linear trend\n\n    # Apply filters\n    # Moving average filter\n    window_size = 50\n    moving_avg = np.convolve(signal_data, np.ones(window_size)/window_size, mode=\'same\')\n\n    # Butterworth filter (using SciPy)\n    from scipy.signal import butter, filtfilt\n    nyquist = 0.5 * 100  # Assuming 100 Hz sampling\n    low_cutoff = 5.0\n    b, a = butter(3, low_cutoff / nyquist, btype=\'low\')\n    filtered_signal = filtfilt(b, a, signal_data)\n\n    # Peak detection\n    peaks = signal.find_peaks(filtered_signal, height=0.5, distance=50)[0]\n\n    # Frequency analysis\n    fft_result = np.fft.fft(filtered_signal)\n    frequencies = np.fft.fftfreq(len(filtered_signal), 1/100)  # 100 Hz sampling\n    magnitude_spectrum = np.abs(fft_result)\n\n    return {\n        \'original_signal\': signal_data,\n        \'filtered_signal\': filtered_signal,\n        \'moving_average\': moving_avg,\n        \'peaks\': peaks,\n        \'frequencies\': frequencies,\n        \'magnitude_spectrum\': magnitude_spectrum\n    }\n\n# Mathematical operations\ndef mathematical_operations():\n    """Common mathematical operations for robotics"""\n\n    # Jacobian calculation example\n    def calculate_jacobian(func, x, eps=1e-8):\n        """Numerically calculate Jacobian of a function"""\n        x = np.asarray(x)\n        fx = func(x)\n        jac = np.zeros((len(fx), len(x)))\n\n        for i in range(len(x)):\n            x_plus = x.copy()\n            x_plus[i] += eps\n            fx_plus = func(x_plus)\n            jac[:, i] = (fx_plus - fx) / eps\n\n        return jac\n\n    # Example function for Jacobian\n    def robot_forward_kinematics(joint_angles):\n        """Simple 2D planar robot forward kinematics"""\n        l1, l2 = 1.0, 0.8  # Link lengths\n        q1, q2 = joint_angles\n\n        x = l1 * np.cos(q1) + l2 * np.cos(q1 + q2)\n        y = l1 * np.sin(q1) + l2 * np.sin(q1 + q2)\n\n        return np.array([x, y])\n\n    # Calculate Jacobian at specific joint angles\n    joint_angles = np.array([np.pi/4, np.pi/6])\n    jacobian = calculate_jacobian(robot_forward_kinematics, joint_angles)\n\n    # Inverse kinematics (geometric solution for 2R robot)\n    def inverse_kinematics_2R(x, y, l1=1.0, l2=0.8):\n        """Geometric inverse kinematics for 2R planar robot"""\n        r = np.sqrt(x**2 + y**2)\n\n        # Check if position is reachable\n        if r > l1 + l2:\n            raise ValueError("Position is outside workspace")\n        if r < abs(l1 - l2):\n            raise ValueError("Position is inside workspace boundary")\n\n        # Calculate elbow-up solution\n        cos_q2 = (r**2 - l1**2 - l2**2) / (2 * l1 * l2)\n        sin_q2 = np.sqrt(1 - cos_q2**2)  # Take positive square root for elbow-up\n        q2 = np.arctan2(sin_q2, cos_q2)\n\n        k1 = l1 + l2 * cos_q2\n        k2 = l2 * sin_q2\n        q1 = np.arctan2(y, x) - np.arctan2(k2, k1)\n\n        return np.array([q1, q2])\n\n    target_pos = np.array([1.2, 0.5])\n    joint_solution = inverse_kinematics_2R(target_pos[0], target_pos[1])\n\n    return {\n        \'jacobian\': jacobian,\n        \'joint_solution\': joint_solution,\n        \'target_position\': target_pos\n    }\n'})}),"\n",(0,o.jsx)(e.h2,{id:"visualization-libraries",children:"Visualization Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"matplotlib",children:"Matplotlib"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n\n# Robot trajectory visualization\ndef plot_robot_trajectory(positions, velocities=None, title=\"Robot Trajectory\"):\n    \"\"\"Plot 2D or 3D robot trajectory\"\"\"\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    if positions.shape[1] == 2:  # 2D trajectory\n        x, y = positions[:, 0], positions[:, 1]\n        ax.plot(x, y, 'b-', linewidth=2, label='Trajectory')\n        ax.scatter(x[0], y[0], color='green', s=100, label='Start', zorder=5)\n        ax.scatter(x[-1], y[-1], color='red', s=100, label='End', zorder=5)\n\n        # Plot velocity vectors if provided\n        if velocities is not None:\n            ax.quiver(x[::10], y[::10], velocities[::10, 0], velocities[::10, 1],\n                     alpha=0.5, scale_units='xy', angles='xy', scale=1, color='orange')\n\n    elif positions.shape[1] == 3:  # 3D trajectory\n        fig = plt.figure(figsize=(12, 9))\n        ax = fig.add_subplot(111, projection='3d')\n\n        x, y, z = positions[:, 0], positions[:, 1], positions[:, 2]\n        ax.plot(x, y, z, 'b-', linewidth=2, label='Trajectory')\n        ax.scatter(x[0], y[0], z[0], color='green', s=100, label='Start', zorder=5)\n        ax.scatter(x[-1], y[-1], z[-1], color='red', s=100, label='End', zorder=5)\n\n    ax.set_xlabel('X Position (m)')\n    ax.set_ylabel('Y Position (m)')\n    if positions.shape[1] == 3:\n        ax.set_zlabel('Z Position (m)')\n\n    ax.set_title(title)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    return fig, ax\n\n# Sensor data visualization\ndef plot_sensor_data(timestamps, sensor_data_dict, title=\"Sensor Data\"):\n    \"\"\"Plot multiple sensor data streams\"\"\"\n\n    n_sensors = len(sensor_data_dict)\n    fig, axes = plt.subplots(n_sensors, 1, figsize=(12, 4*n_sensors), sharex=True)\n\n    if n_sensors == 1:\n        axes = [axes]\n\n    for i, (sensor_name, data) in enumerate(sensor_data_dict.items()):\n        axes[i].plot(timestamps, data, linewidth=1.5)\n        axes[i].set_ylabel(sensor_name)\n        axes[i].grid(True, alpha=0.3)\n        axes[i].set_title(f'{sensor_name} Data')\n\n    axes[-1].set_xlabel('Time (s)')\n    plt.suptitle(title)\n    plt.tight_layout()\n\n    return fig, axes\n\n# Real-time plotting\ndef real_time_plot():\n    \"\"\"Create real-time plot for live sensor data\"\"\"\n\n    # Set up the figure and axis\n    fig, ax = plt.subplots(figsize=(12, 6))\n    x_data, y_data = [], []\n\n    # Create empty line object\n    line, = ax.plot([], [], 'b-', linewidth=2)\n    ax.set_xlim(0, 100)\n    ax.set_ylim(-2, 2)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Real-time Data Stream')\n    ax.grid(True, alpha=0.3)\n\n    def animate(frame):\n        # Generate new data point\n        x_data.append(frame)\n        y_data.append(np.sin(frame * 0.1) + np.random.normal(0, 0.1))\n\n        # Keep only last 100 points\n        if len(x_data) > 100:\n            x_data.pop(0)\n            y_data.pop(0)\n\n        # Update plot\n        line.set_data(x_data, y_data)\n\n        # Adjust x-axis limits to show sliding window\n        if len(x_data) > 0:\n            ax.set_xlim(max(0, x_data[-1] - 100), x_data[-1])\n\n        return line,\n\n    # Create animation\n    ani = animation.FuncAnimation(fig, animate, interval=50, blit=True, cache_frame_data=False)\n\n    return fig, ani\n\n# Heatmap visualization for occupancy grids\ndef plot_occupancy_grid(grid, title=\"Occupancy Grid\"):\n    \"\"\"Plot occupancy grid as heatmap\"\"\"\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    im = ax.imshow(grid, cmap='RdYlBu_r', origin='lower', interpolation='nearest')\n    cbar = plt.colorbar(im, ax=ax)\n    cbar.set_label('Occupancy Probability')\n\n    ax.set_xlabel('X Cell Index')\n    ax.set_ylabel('Y Cell Index')\n    ax.set_title(title)\n\n    return fig, ax\n\n# 3D point cloud visualization\ndef plot_point_cloud(point_cloud, title=\"Point Cloud\"):\n    \"\"\"Plot 3D point cloud data\"\"\"\n\n    fig = plt.figure(figsize=(12, 9))\n    ax = fig.add_subplot(111, projection='3d')\n\n    if point_cloud.shape[1] >= 3:\n        x, y, z = point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2]\n        scatter = ax.scatter(x, y, z, c=z, cmap='viridis', s=1)\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title(title)\n\n    return fig, ax\n"})}),"\n",(0,o.jsx)(e.h3,{id:"seaborn-for-statistical-plots",children:"Seaborn for Statistical Plots"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef statistical_robot_plots(df):\n    \"\"\"Create statistical plots for robot data analysis\"\"\"\n\n    # Set style\n    sns.set_style(\"whitegrid\")\n    plt.rcParams['figure.figsize'] = (12, 8)\n\n    # Correlation heatmap\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    corr_matrix = df[numeric_cols].corr()\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n    # 1. Correlation heatmap\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n                square=True, ax=axes[0,0])\n    axes[0,0].set_title('Feature Correlation Matrix')\n\n    # 2. Pair plot for key variables\n    key_vars = [col for col in numeric_cols if col in ['position_x', 'position_y', 'velocity', 'battery_level']]\n    if len(key_vars) >= 2:\n        sns.scatterplot(data=df, x=key_vars[0], y=key_vars[1], hue='status', ax=axes[0,1])\n        axes[0,1].set_title(f'{key_vars[0]} vs {key_vars[1]}')\n\n    # 3. Distribution plots\n    for i, col in enumerate(key_vars[:2]):\n        if i < 2:\n            sns.histplot(data=df, x=col, hue='status', kde=True, ax=axes[1,i])\n            axes[1,i].set_title(f'Distribution of {col}')\n\n    # 4. Box plot for status comparison\n    if 'status' in df.columns and len(key_vars) > 0:\n        sns.boxplot(data=df, x='status', y=key_vars[0], ax=axes[1,1])\n        axes[1,1].set_title(f'{key_vars[0]} by Status')\n        axes[1,1].tick_params(axis='x', rotation=45)\n\n    plt.tight_layout()\n    return fig, axes\n\n# Time series analysis plots\ndef time_series_plots(df):\n    \"\"\"Create time series plots for temporal analysis\"\"\"\n\n    if 'timestamp' not in df.columns:\n        print(\"No timestamp column found\")\n        return None, None\n\n    # Ensure timestamp is datetime\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # 1. Time series plot\n    axes[0,0].plot(df['timestamp'], df['position_x'], label='X Position', alpha=0.7)\n    axes[0,0].plot(df['timestamp'], df['position_y'], label='Y Position', alpha=0.7)\n    axes[0,0].set_xlabel('Time')\n    axes[0,0].set_ylabel('Position (m)')\n    axes[0,0].set_title('Position Over Time')\n    axes[0,0].legend()\n    axes[0,0].tick_params(axis='x', rotation=45)\n\n    # 2. Battery level over time\n    axes[0,1].plot(df['timestamp'], df['battery_level'], 'g-', linewidth=2)\n    axes[0,1].set_xlabel('Time')\n    axes[0,1].set_ylabel('Battery Level (%)')\n    axes[0,1].set_title('Battery Level Over Time')\n    axes[0,1].grid(True, alpha=0.3)\n    axes[0,1].tick_params(axis='x', rotation=45)\n\n    # 3. Velocity histogram\n    axes[1,0].hist(df['velocity'], bins=30, alpha=0.7, color='orange')\n    axes[1,0].set_xlabel('Velocity (m/s)')\n    axes[1,0].set_ylabel('Frequency')\n    axes[1,0].set_title('Velocity Distribution')\n    axes[1,0].grid(True, alpha=0.3)\n\n    # 4. 2D position scatter\n    scatter = axes[1,1].scatter(df['position_x'], df['position_y'],\n                               c=pd.to_numeric(pd.factorize(df['status'])[0]),\n                               cmap='viridis', alpha=0.6)\n    axes[1,1].set_xlabel('X Position (m)')\n    axes[1,1].set_ylabel('Y Position (m)')\n    axes[1,1].set_title('Position Scatter (Colored by Status)')\n    axes[1,1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    return fig, axes\n"})}),"\n",(0,o.jsx)(e.h2,{id:"networking-and-communication",children:"Networking and Communication"}),"\n",(0,o.jsx)(e.h3,{id:"zeromq",children:"ZeroMQ"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import zmq\nimport json\nimport threading\nimport time\n\nclass ZMQRobotInterface:\n    """ZeroMQ interface for robot communication"""\n\n    def __init__(self, port=5555):\n        self.context = zmq.Context()\n        self.port = port\n\n    def start_server(self):\n        """Start ZMQ server for robot commands"""\n        socket = self.context.socket(zmq.REP)  # Reply socket\n        socket.bind(f"tcp://*:{self.port}")\n\n        print(f"Server listening on port {self.port}")\n\n        try:\n            while True:\n                # Receive request\n                message = socket.recv_string()\n                print(f"Received request: {message}")\n\n                # Parse command\n                try:\n                    cmd_data = json.loads(message)\n                    response = self.process_command(cmd_data)\n                except json.JSONDecodeError:\n                    response = {"error": "Invalid JSON"}\n                except Exception as e:\n                    response = {"error": str(e)}\n\n                # Send response\n                socket.send_string(json.dumps(response))\n\n        except KeyboardInterrupt:\n            print("Server shutting down...")\n        finally:\n            socket.close()\n\n    def start_client(self):\n        """Start ZMQ client to send commands"""\n        socket = self.context.socket(zmq.REQ)  # Request socket\n        socket.connect(f"tcp://localhost:{self.port}")\n\n        return socket\n\n    def send_command(self, command_data):\n        """Send command to robot server"""\n        socket = self.start_client()\n\n        # Send command\n        socket.send_string(json.dumps(command_data))\n\n        # Receive response\n        response = socket.recv_string()\n        socket.close()\n\n        return json.loads(response)\n\n    def process_command(self, cmd_data):\n        """Process robot command"""\n        command = cmd_data.get(\'command\')\n\n        if command == \'move_to\':\n            x = cmd_data.get(\'x\', 0)\n            y = cmd_data.get(\'y\', 0)\n            # Simulate robot movement\n            time.sleep(0.1)  # Simulate movement time\n            return {"success": True, "message": f"Moved to ({x}, {y})"}\n\n        elif command == \'get_position\':\n            # Simulate getting current position\n            import random\n            return {\n                "success": True,\n                "position": {\n                    "x": random.uniform(-10, 10),\n                    "y": random.uniform(-10, 10),\n                    "theta": random.uniform(-3.14, 3.14)\n                }\n            }\n\n        else:\n            return {"success": False, "error": f"Unknown command: {command}"}\n\n# Example usage\ndef zmq_example():\n    # In one thread/process, start server\n    def run_server():\n        interface = ZMQRobotInterface(5555)\n        interface.start_server()\n\n    # In another thread, send commands\n    def send_commands():\n        interface = ZMQRobotInterface(5555)\n        time.sleep(1)  # Wait for server to start\n\n        # Send move command\n        response = interface.send_command({\n            "command": "move_to",\n            "x": 1.0,\n            "y": 2.0\n        })\n        print(f"Move response: {response}")\n\n        # Get position\n        response = interface.send_command({"command": "get_position"})\n        print(f"Position response: {response}")\n\n    # Run server in background thread\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n\n    # Send commands from main thread\n    send_commands()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"websockets",children:"WebSockets"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Note: Requires installation of websockets library\n# pip install websockets\n\ntry:\n    import asyncio\n    import websockets\n    import json\n\n    class WebSocketRobotServer:\n        def __init__(self):\n            self.clients = set()\n            self.robot_state = {\n                'position': {'x': 0, 'y': 0, 'theta': 0},\n                'battery': 100,\n                'status': 'idle'\n            }\n\n        async def register_client(self, websocket):\n            self.clients.add(websocket)\n            print(f\"Client connected. Total clients: {len(self.clients)}\")\n\n        async def unregister_client(self, websocket):\n            self.clients.remove(websocket)\n            print(f\"Client disconnected. Total clients: {len(self.clients)}\")\n\n        async def broadcast_state(self):\n            \"\"\"Broadcast robot state to all connected clients\"\"\"\n            if self.clients:\n                message = json.dumps({\n                    'type': 'state_update',\n                    'data': self.robot_state\n                })\n                await asyncio.gather(\n                    *[client.send(message) for client in self.clients],\n                    return_exceptions=True\n                )\n\n        async def handle_command(self, websocket, command_data):\n            \"\"\"Handle incoming commands\"\"\"\n            command = command_data.get('command')\n\n            if command == 'move_to':\n                x = command_data.get('x', self.robot_state['position']['x'])\n                y = command_data.get('y', self.robot_state['position']['y'])\n\n                # Update position (simulate movement)\n                self.robot_state['position']['x'] = x\n                self.robot_state['position']['y'] = y\n\n                response = {\n                    'type': 'command_response',\n                    'success': True,\n                    'message': f'Moved to ({x}, {y})'\n                }\n\n            elif command == 'get_state':\n                response = {\n                    'type': 'state_response',\n                    'data': self.robot_state\n                }\n\n            else:\n                response = {\n                    'type': 'command_response',\n                    'success': False,\n                    'error': f'Unknown command: {command}'\n                }\n\n            await websocket.send(json.dumps(response))\n            await self.broadcast_state()\n\n        async def handler(self, websocket, path):\n            await self.register_client(websocket)\n            try:\n                async for message in websocket:\n                    try:\n                        data = json.loads(message)\n                        await self.handle_command(websocket, data)\n                    except json.JSONDecodeError:\n                        await websocket.send(json.dumps({\n                            'type': 'error',\n                            'message': 'Invalid JSON'\n                        }))\n            finally:\n                await self.unregister_client(websocket)\n\n        async def start_server(self, host='localhost', port=8765):\n            server = await websockets.serve(self.handler, host, port)\n            print(f\"WebSocket server started on ws://{host}:{port}\")\n            await server.wait_closed()\n\n    # Example client function\n    async def websocket_client_example():\n        uri = \"ws://localhost:8765\"\n        async with websockets.connect(uri) as websocket:\n            # Send move command\n            command = {\n                \"command\": \"move_to\",\n                \"x\": 1.0,\n                \"y\": 2.0\n            }\n            await websocket.send(json.dumps(command))\n\n            response = await websocket.recv()\n            print(f\"Response: {response}\")\n\n            # Listen for state updates\n            try:\n                while True:\n                    message = await websocket.recv()\n                    data = json.loads(message)\n                    if data['type'] == 'state_update':\n                        print(f\"State updated: {data['data']}\")\n            except websockets.exceptions.ConnectionClosed:\n                print(\"Connection closed\")\n\nexcept ImportError:\n    print(\"websockets library not available\")\n"})}),"\n",(0,o.jsx)(e.h2,{id:"system-and-hardware-libraries",children:"System and Hardware Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"serial-communication",children:"Serial Communication"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import serial\nimport serial.tools.list_ports\nimport time\n\nclass SerialRobotInterface:\n    """Serial interface for robot communication"""\n\n    def __init__(self, port=None, baudrate=115200, timeout=1):\n        self.port = port\n        self.baudrate = baudrate\n        self.timeout = timeout\n        self.serial_conn = None\n\n    def list_serial_ports(self):\n        """List available serial ports"""\n        ports = serial.tools.list_ports.comports()\n        available_ports = []\n\n        for port in ports:\n            available_ports.append({\n                \'device\': port.device,\n                \'description\': port.description,\n                \'hwid\': port.hwid\n            })\n\n        return available_ports\n\n    def connect(self, port=None):\n        """Connect to serial port"""\n        if port:\n            self.port = port\n\n        if not self.port:\n            available_ports = self.list_serial_ports()\n            if available_ports:\n                self.port = available_ports[0][\'device\']  # Use first available\n                print(f"Using port: {self.port}")\n            else:\n                raise Exception("No serial ports available")\n\n        try:\n            self.serial_conn = serial.Serial(\n                port=self.port,\n                baudrate=self.baudrate,\n                timeout=self.timeout\n            )\n            print(f"Connected to {self.port} at {self.baudrate} baud")\n            return True\n        except serial.SerialException as e:\n            print(f"Failed to connect: {e}")\n            return False\n\n    def disconnect(self):\n        """Disconnect from serial port"""\n        if self.serial_conn and self.serial_conn.is_open:\n            self.serial_conn.close()\n            print("Serial connection closed")\n\n    def send_command(self, command, terminator=\'\\n\'):\n        """Send command to robot"""\n        if not self.serial_conn or not self.serial_conn.is_open:\n            raise Exception("Not connected to serial port")\n\n        full_command = command + terminator\n        self.serial_conn.write(full_command.encode())\n        print(f"Sent: {full_command.strip()}")\n\n    def read_response(self, timeout=2):\n        """Read response from robot"""\n        if not self.serial_conn or not self.serial_conn.is_open:\n            raise Exception("Not connected to serial port")\n\n        start_time = time.time()\n        response_lines = []\n\n        while time.time() - start_time < timeout:\n            if self.serial_conn.in_waiting > 0:\n                line = self.serial_conn.readline().decode().strip()\n                if line:\n                    response_lines.append(line)\n                    # Break if we get an acknowledgment or end marker\n                    if line.startswith(\'OK\') or line.startswith(\'DONE\'):\n                        break\n            time.sleep(0.01)\n\n        return \'\\n\'.join(response_lines)\n\n    def send_and_receive(self, command, timeout=2):\n        """Send command and wait for response"""\n        self.send_command(command)\n        return self.read_response(timeout)\n\n    def ping(self):\n        """Ping robot to check connection"""\n        response = self.send_and_receive(\'PING\', timeout=1)\n        return \'PONG\' in response or \'OK\' in response\n\n# Example usage\ndef serial_example():\n    robot = SerialRobotInterface()\n\n    # List available ports\n    ports = robot.list_serial_ports()\n    print("Available ports:")\n    for port in ports:\n        print(f"  {port[\'device\']}: {port[\'description\']}")\n\n    # Connect to robot\n    if robot.connect():\n        # Test connection\n        if robot.ping():\n            print("Robot responded to ping!")\n\n            # Send some commands\n            response = robot.send_and_receive(\'GET_POS\')\n            print(f"Position: {response}")\n\n            # Move robot\n            robot.send_command(\'MOVE_TO 100 200\')\n            time.sleep(2)  # Wait for movement\n\n        robot.disconnect()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"gpio-for-raspberry-pi",children:"GPIO (for Raspberry Pi)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Note: GPIO libraries are hardware-specific\n# For Raspberry Pi, you might use RPi.GPIO or gpiozero\n\ntry:\n    import RPi.GPIO as GPIO\n    import time\n\n    class GPIORobotControl:\n        def __init__(self, pins_config):\n            \"\"\"\n            pins_config example:\n            {\n                'motor_left': {'dir': 18, 'pwm': 19},\n                'motor_right': {'dir': 23, 'pwm': 24},\n                'servo': 12,\n                'sensors': {'ultrasonic': {'trig': 20, 'echo': 21}}\n            }\n            \"\"\"\n            self.pins = pins_config\n            GPIO.setmode(GPIO.BCM)\n            GPIO.setwarnings(False)\n\n            # Setup motor pins\n            if 'motor_left' in self.pins:\n                left_pins = self.pins['motor_left']\n                GPIO.setup(left_pins['dir'], GPIO.OUT)\n                GPIO.setup(left_pins['pwm'], GPIO.OUT)\n                self.left_pwm = GPIO.PWM(left_pins['pwm'], 1000)  # 1kHz PWM\n                self.left_pwm.start(0)\n\n            if 'motor_right' in self.pins:\n                right_pins = self.pins['motor_right']\n                GPIO.setup(right_pins['dir'], GPIO.OUT)\n                GPIO.setup(right_pins['pwm'], GPIO.OUT)\n                self.right_pwm = GPIO.PWM(right_pins['pwm'], 1000)\n                self.right_pwm.start(0)\n\n        def set_motor_speed(self, motor, speed, direction=1):\n            \"\"\"\n            Set motor speed and direction\n            motor: 'left' or 'right'\n            speed: 0-100 percentage\n            direction: 1 for forward, -1 for backward\n            \"\"\"\n            if motor == 'left':\n                pwm = self.left_pwm\n                dir_pin = self.pins['motor_left']['dir']\n            elif motor == 'right':\n                pwm = self.right_pwm\n                dir_pin = self.pins['motor_right']['dir']\n            else:\n                raise ValueError(\"Motor must be 'left' or 'right'\")\n\n            # Set direction\n            GPIO.output(dir_pin, GPIO.HIGH if direction >= 0 else GPIO.LOW)\n\n            # Set speed\n            pwm.ChangeDutyCycle(abs(speed))\n\n        def stop_motors(self):\n            \"\"\"Stop both motors\"\"\"\n            if hasattr(self, 'left_pwm'):\n                self.left_pwm.ChangeDutyCycle(0)\n            if hasattr(self, 'right_pwm'):\n                self.right_pwm.ChangeDutyCycle(0)\n\n        def cleanup(self):\n            \"\"\"Clean up GPIO\"\"\"\n            self.stop_motors()\n            GPIO.cleanup()\n\n    # Alternative using gpiozero (often easier)\n    from gpiozero import Motor, Servo, DistanceSensor\n    import time\n\n    class GPIOZeroRobot:\n        def __init__(self, left_motor_pins=(18, 19), right_motor_pins=(23, 24), servo_pin=12):\n            self.left_motor = Motor(forward=left_motor_pins[0], backward=left_motor_pins[1])\n            self.right_motor = Motor(forward=right_motor_pins[0], backward=right_motor_pins[1])\n            self.servo = Servo(servo_pin)\n\n            # Ultrasonic sensor\n            self.distance_sensor = DistanceSensor(echo=21, trigger=20)\n\n        def move_forward(self, speed=0.5):\n            self.left_motor.forward(speed)\n            self.right_motor.forward(speed)\n\n        def move_backward(self, speed=0.5):\n            self.left_motor.backward(speed)\n            self.right_motor.backward(speed)\n\n        def turn_left(self, speed=0.5):\n            self.left_motor.backward(speed)\n            self.right_motor.forward(speed)\n\n        def turn_right(self, speed=0.5):\n            self.left_motor.forward(speed)\n            self.right_motor.backward(speed)\n\n        def stop(self):\n            self.left_motor.stop()\n            self.right_motor.stop()\n\n        def get_distance(self):\n            return self.distance_sensor.distance\n\n        def set_servo_position(self, position):\n            \"\"\"Position between -1 and 1\"\"\"\n            self.servo.value = position\n\n    def gpio_example():\n        # Initialize robot\n        robot = GPIOZeroRobot()\n\n        # Move forward\n        robot.move_forward(0.5)\n        time.sleep(2)\n\n        # Stop\n        robot.stop()\n        time.sleep(1)\n\n        # Check distance\n        distance = robot.get_distance()\n        print(f\"Distance: {distance:.2f} m\")\n\n        # Clean up\n        robot.stop()\n\nexcept ImportError:\n    print(\"GPIO libraries not available (requires Raspberry Pi hardware)\")\n"})}),"\n",(0,o.jsx)(e.h2,{id:"utility-libraries",children:"Utility Libraries"}),"\n",(0,o.jsx)(e.h3,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import json\nimport yaml\nimport configparser\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, Any, Optional\nimport os\n\n@dataclass\nclass RobotConfig:\n    """Data class for robot configuration"""\n    name: str = "default_robot"\n    max_speed: float = 1.0\n    acceleration: float = 0.5\n    operating_mode: str = "autonomous"\n    sensors: Dict[str, Any] = None\n\n    def __post_init__(self):\n        if self.sensors is None:\n            self.sensors = {\n                "camera": {"enabled": True, "resolution": [640, 480]},\n                "lidar": {"enabled": True, "range": 10.0},\n                "imu": {"enabled": True}\n            }\n\nclass ConfigManager:\n    """Configuration manager for robot systems"""\n\n    def __init__(self, config_path: str = "config"):\n        self.config_path = config_path\n        self.config = RobotConfig()\n\n    def load_json(self, filepath: str) -> RobotConfig:\n        """Load configuration from JSON file"""\n        with open(filepath, \'r\') as f:\n            data = json.load(f)\n\n        # Create config object from data\n        config = RobotConfig(**data)\n        self.config = config\n        return config\n\n    def save_json(self, filepath: str, config: RobotConfig = None):\n        """Save configuration to JSON file"""\n        if config is None:\n            config = self.config\n\n        with open(filepath, \'w\') as f:\n            json.dump(asdict(config), f, indent=2)\n\n    def load_yaml(self, filepath: str) -> RobotConfig:\n        """Load configuration from YAML file"""\n        with open(filepath, \'r\') as f:\n            data = yaml.safe_load(f)\n\n        config = RobotConfig(**data)\n        self.config = config\n        return config\n\n    def save_yaml(self, filepath: str, config: RobotConfig = None):\n        """Save configuration to YAML file"""\n        if config is None:\n            config = self.config\n\n        with open(filepath, \'w\') as f:\n            yaml.dump(asdict(config), f, default_flow_style=False, indent=2)\n\n    def load_ini(self, filepath: str) -> RobotConfig:\n        """Load configuration from INI file"""\n        parser = configparser.ConfigParser()\n        parser.read(filepath)\n\n        # Convert INI structure to config dict\n        config_dict = {}\n\n        # Robot section\n        if \'ROBOT\' in parser:\n            robot_section = parser[\'ROBOT\']\n            config_dict[\'name\'] = robot_section.get(\'name\', \'default_robot\')\n            config_dict[\'max_speed\'] = robot_section.getfloat(\'max_speed\', 1.0)\n            config_dict[\'acceleration\'] = robot_section.getfloat(\'acceleration\', 0.5)\n            config_dict[\'operating_mode\'] = robot_section.get(\'operating_mode\', \'autonomous\')\n\n        # Sensors section\n        if \'SENSORS\' in parser:\n            sensors_section = parser[\'SENSORS\']\n            sensors = {}\n            for key, value in sensors_section.items():\n                # Parse sensor configurations\n                sensors[key] = json.loads(value) if value.startswith(\'{\') else value\n            config_dict[\'sensors\'] = sensors\n\n        config = RobotConfig(**config_dict)\n        self.config = config\n        return config\n\n    def save_ini(self, filepath: str, config: RobotConfig = None):\n        """Save configuration to INI file"""\n        if config is None:\n            config = self.config\n\n        parser = configparser.ConfigParser()\n\n        # Robot section\n        parser[\'ROBOT\'] = {\n            \'name\': config.name,\n            \'max_speed\': str(config.max_speed),\n            \'acceleration\': str(config.acceleration),\n            \'operating_mode\': config.operating_mode\n        }\n\n        # Sensors section\n        parser[\'SENSORS\'] = {}\n        for sensor_name, sensor_config in config.sensors.items():\n            parser[\'SENSORS\'][sensor_name] = json.dumps(sensor_config)\n\n        with open(filepath, \'w\') as f:\n            parser.write(f)\n\n    def get_config(self) -> RobotConfig:\n        """Get current configuration"""\n        return self.config\n\n    def update_config(self, **kwargs):\n        """Update configuration with new values"""\n        config_dict = asdict(self.config)\n        config_dict.update(kwargs)\n        self.config = RobotConfig(**config_dict)\n\n# Environment variable configuration\ndef load_config_from_env():\n    """Load configuration from environment variables"""\n    config = RobotConfig()\n\n    # Update from environment variables\n    config.name = os.getenv(\'ROBOT_NAME\', config.name)\n    config.max_speed = float(os.getenv(\'MAX_SPEED\', config.max_speed))\n    config.acceleration = float(os.getenv(\'ACCELERATION\', config.acceleration))\n    config.operating_mode = os.getenv(\'OPERATING_MODE\', config.operating_mode)\n\n    return config\n\n# Example configuration files\n\n# config.json\njson_config = """\n{\n  "name": "delivery_robot_01",\n  "max_speed": 0.8,\n  "acceleration": 0.3,\n  "operating_mode": "delivery",\n  "sensors": {\n    "camera": {\n      "enabled": true,\n      "resolution": [1280, 720],\n      "fov": 60\n    },\n    "lidar": {\n      "enabled": true,\n      "range": 20.0,\n      "resolution": 0.5\n    },\n    "imu": {\n      "enabled": true,\n      "rate": 100\n    }\n  }\n}\n"""\n\n# config.yaml\nyaml_config = """\nname: "inspection_robot_01"\nmax_speed: 0.5\nacceleration: 0.2\noperating_mode: "inspection"\nsensors:\n  camera:\n    enabled: true\n    resolution: [640, 480]\n    fov: 90\n  lidar:\n    enabled: true\n    range: 15.0\n    resolution: 1.0\n  imu:\n    enabled: true\n    rate: 50\n"""\n'})}),"\n",(0,o.jsx)(e.h3,{id:"logging-and-monitoring",children:"Logging and Monitoring"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import logging\nimport logging.handlers\nimport sys\nfrom datetime import datetime\nimport os\n\nclass RobotLogger:\n    """Advanced logging for robot systems"""\n\n    def __init__(self, name="RobotSystem", log_dir="logs"):\n        self.logger = logging.getLogger(name)\n        self.logger.setLevel(logging.DEBUG)\n\n        # Create logs directory\n        os.makedirs(log_dir, exist_ok=True)\n\n        # Create formatter\n        formatter = logging.Formatter(\n            \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\n        )\n\n        # Console handler\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.INFO)\n        console_handler.setFormatter(formatter)\n\n        # File handler - daily rotating\n        log_file = os.path.join(log_dir, f"{name.lower()}_{datetime.now().strftime(\'%Y%m%d\')}.log")\n        file_handler = logging.handlers.TimedRotatingFileHandler(\n            log_file, when=\'midnight\', interval=1, backupCount=7\n        )\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n\n        # Error file handler - only errors\n        error_file = os.path.join(log_dir, f"{name.lower()}_errors.log")\n        error_handler = logging.FileHandler(error_file)\n        error_handler.setLevel(logging.ERROR)\n        error_handler.setFormatter(formatter)\n\n        # Add handlers\n        self.logger.addHandler(console_handler)\n        self.logger.addHandler(file_handler)\n        self.logger.addHandler(error_handler)\n\n        # Prevent duplicate logs if parent logger exists\n        self.logger.propagate = False\n\n    def debug(self, message):\n        self.logger.debug(message)\n\n    def info(self, message):\n        self.logger.info(message)\n\n    def warning(self, message):\n        self.logger.warning(message)\n\n    def error(self, message):\n        self.logger.error(message)\n\n    def critical(self, message):\n        self.logger.critical(message)\n\n    def log_robot_state(self, state_data):\n        """Log robot state information"""\n        state_str = ", ".join([f"{k}={v}" for k, v in state_data.items()])\n        self.info(f"Robot state: {state_str}")\n\n    def log_sensor_data(self, sensor_name, value):\n        """Log sensor data with timestamp"""\n        self.debug(f"Sensor[{sensor_name}]: {value}")\n\n    def log_navigation_event(self, event_type, details):\n        """Log navigation-related events"""\n        self.info(f"Navigation {event_type}: {details}")\n\n# Performance monitoring\nclass PerformanceMonitor:\n    """Monitor performance metrics"""\n\n    def __init__(self):\n        self.metrics = {}\n        self.start_times = {}\n\n    def start_timer(self, name):\n        """Start a timer"""\n        import time\n        self.start_times[name] = time.time()\n\n    def stop_timer(self, name):\n        """Stop a timer and record duration"""\n        import time\n        if name in self.start_times:\n            duration = time.time() - self.start_times[name]\n            if name not in self.metrics:\n                self.metrics[name] = []\n            self.metrics[name].append(duration)\n            return duration\n        return None\n\n    def get_average_time(self, name):\n        """Get average execution time for a named operation"""\n        if name in self.metrics and self.metrics[name]:\n            return sum(self.metrics[name]) / len(self.metrics[name])\n        return 0\n\n    def get_metric_summary(self):\n        """Get summary of all metrics"""\n        summary = {}\n        for name, times in self.metrics.items():\n            if times:\n                summary[name] = {\n                    \'count\': len(times),\n                    \'average\': sum(times) / len(times),\n                    \'min\': min(times),\n                    \'max\': max(times),\n                    \'total\': sum(times)\n                }\n        return summary\n\n# Example usage\ndef logging_example():\n    # Create logger\n    robot_logger = RobotLogger("DeliveryRobot")\n\n    # Log some events\n    robot_logger.info("Robot initialized")\n    robot_logger.log_robot_state({\n        \'position\': (1.0, 2.0, 0.5),\n        \'battery\': 85.3,\n        \'status\': \'ready\'\n    })\n\n    # Performance monitoring\n    perf_monitor = PerformanceMonitor()\n\n    # Monitor a function\n    perf_monitor.start_timer(\'navigation_calculation\')\n    # ... some computation ...\n    import time\n    time.sleep(0.1)  # Simulate work\n    duration = perf_monitor.stop_timer(\'navigation_calculation\')\n\n    robot_logger.info(f"Navigation calculation took {duration:.3f}s")\n\n    # Print performance summary\n    summary = perf_monitor.get_metric_summary()\n    for name, stats in summary.items():\n        robot_logger.info(f"{name}: avg={stats[\'average\']:.3f}s, count={stats[\'count\']}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,o.jsx)(e.h3,{id:"requirements-management",children:"Requirements Management"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# requirements.txt for Physical AI projects\nnumpy>=1.21.0\nscipy>=1.7.0\nmatplotlib>=3.4.0\nopencv-python>=4.5.0\nPillow>=8.3.0\ntorch>=1.10.0\ntorchvision>=0.11.0\ntransformers>=4.12.0\nscikit-learn>=1.0.0\npandas>=1.3.0\npyserial>=3.5\nspeechrecognition>=3.8.1\npyttsx3>=2.90\nrclpy>=3.1.0\nros2\nwebsockets>=10.0\npyyaml>=6.0\nrequests>=2.26.0\nseaborn>=0.11.0\nlibrosa>=0.9.0\nvosk>=0.3.45\n"})}),"\n",(0,o.jsx)(e.h3,{id:"virtual-environment-setup",children:"Virtual Environment Setup"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Create virtual environment\npython3 -m venv physical_ai_env\n\n# Activate environment\nsource physical_ai_env/bin/activate  # Linux/Mac\n# physical_ai_env\\Scripts\\activate  # Windows\n\n# Upgrade pip\npip install --upgrade pip\n\n# Install requirements\npip install -r requirements.txt\n\n# Install ROS 2 Python packages\npip install -r requirements_ros.txt\n"})}),"\n",(0,o.jsx)(e.h3,{id:"docker-setup-for-isolated-environments",children:"Docker Setup for Isolated Environments"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-dockerfile",children:'# Dockerfile for Physical AI development\nFROM ubuntu:22.04\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    python3 \\\n    python3-pip \\\n    python3-dev \\\n    build-essential \\\n    cmake \\\n    git \\\n    curl \\\n    wget \\\n    vim \\\n    htop \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install ROS 2 Humble\nRUN apt-get update && apt-get install -y \\\n    locales \\\n    software-properties-common \\\n    && locale-gen en_US en_US.UTF-8 \\\n    && apt-get install -y \\\n    curl \\\n    gnupg2 \\\n    lsb-release \\\n    && curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg \\\n    && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main" | tee /etc/apt/sources.list.d/ros2.list > /dev/null \\\n    && apt-get update \\\n    && apt-get install -y ros-humble-desktop \\\n    && apt-get install -y python3-colcon-common-extensions \\\n    && apt-get install -y python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential \\\n    && rosdep init \\\n    && rosdep update \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set up ROS 2 environment\nENV ROS_DISTRO=humble\nENV ROS_ROOT=/opt/ros/humble\nENV PATH=$ROS_ROOT/bin:$PATH\nENV PYTHONPATH=$ROS_ROOT/lib/python3.10/site-packages:$PYTHONPATH\nENV LD_LIBRARY_PATH=$ROS_ROOT/lib:$LD_LIBRARY_PATH\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Install Python packages\nRUN pip3 install --upgrade pip && \\\n    pip3 install -r requirements.txt\n\n# Set up workspace\nRUN mkdir -p /workspace/src\nWORKDIR /workspace\n\n# Source ROS 2 setup\nRUN echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc\n\nCMD ["/bin/bash"]\n'})}),"\n",(0,o.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsx)(e.h3,{id:"1-error-handling-and-logging",children:"1. Error Handling and Logging"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import logging\nfrom functools import wraps\n\ndef robust_function(func):\n    """Decorator for robust function execution with error handling"""\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            logging.error(f"Error in {func.__name__}: {str(e)}")\n            # Return safe default or re-raise based on context\n            return None\n    return wrapper\n\n@robust_function\ndef sensor_reading_function():\n    # Robust sensor reading code\n    pass\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-resource-management",children:"2. Resource Management"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class ResourceManager:\n    """Manage system resources for robot operations"""\n\n    def __enter__(self):\n        # Acquire resources\n        self.acquire_resources()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Release resources\n        self.release_resources()\n\n    def acquire_resources(self):\n        # Open files, connect to devices, etc.\n        pass\n\n    def release_resources(self):\n        # Close files, disconnect devices, etc.\n        pass\n\n# Usage\nwith ResourceManager() as rm:\n    # Use resources\n    pass\n'})}),"\n",(0,o.jsx)(e.h3,{id:"3-configuration-validation",children:"3. Configuration Validation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"from typing import Dict, Any\nimport json\n\ndef validate_robot_config(config: Dict[str, Any]) -> bool:\n    \"\"\"Validate robot configuration\"\"\"\n    required_fields = ['name', 'max_speed', 'sensors']\n\n    for field in required_fields:\n        if field not in config:\n            raise ValueError(f\"Missing required field: {field}\")\n\n    # Validate sensor configurations\n    for sensor_name, sensor_config in config['sensors'].items():\n        if not isinstance(sensor_config, dict):\n            raise ValueError(f\"Sensor {sensor_name} configuration must be a dict\")\n\n    # Validate numeric ranges\n    if config['max_speed'] <= 0 or config['max_speed'] > 5.0:\n        raise ValueError(\"max_speed must be between 0 and 5.0\")\n\n    return True\n"})}),"\n",(0,o.jsx)(e.p,{children:"This comprehensive reference covers all the essential Python libraries needed for Physical AI and robotics development. Each section includes practical examples and best practices for real-world applications."})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}}}]);