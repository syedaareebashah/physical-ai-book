"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[1647],{8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>r});var t=a(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},9870:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module3-isaac/navigation-stack","title":"Navigation Stack (Nav2) with Isaac","description":"Chapter Objectives","source":"@site/docs/module3-isaac/04-navigation-stack.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/navigation-stack","permalink":"/physical-ai-book/docs/module3-isaac/navigation-stack","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module3-isaac/04-navigation-stack.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Visual SLAM with Isaac ROS","permalink":"/physical-ai-book/docs/module3-isaac/visual-slam"},"next":{"title":"Perception Pipeline with Isaac","permalink":"/physical-ai-book/docs/module3-isaac/perception-pipeline"}}');var i=a(4848),s=a(8453);const o={sidebar_position:4},r="Navigation Stack (Nav2) with Isaac",l={},c=[{value:"Chapter Objectives",id:"chapter-objectives",level:2},{value:"Navigation2 Overview",id:"navigation2-overview",level:2},{value:"Nav2 Architecture",id:"nav2-architecture",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Isaac Integration with Nav2",id:"isaac-integration-with-nav2",level:2},{value:"GPU-Accelerated Components",id:"gpu-accelerated-components",level:3},{value:"Isaac Nav2 Package Structure",id:"isaac-nav2-package-structure",level:3},{value:"Setting Up Isaac-Enhanced Navigation",id:"setting-up-isaac-enhanced-navigation",level:2},{value:"Nav2 Configuration with Isaac",id:"nav2-configuration-with-isaac",level:3},{value:"Isaac-Specific Navigation Launch File",id:"isaac-specific-navigation-launch-file",level:3},{value:"Perception-Aware Navigation",id:"perception-aware-navigation",level:2},{value:"Multi-Sensor Fusion for Navigation",id:"multi-sensor-fusion-for-navigation",level:3},{value:"Behavior Trees for Complex Navigation",id:"behavior-trees-for-complex-navigation",level:2},{value:"Custom Behavior Tree Nodes",id:"custom-behavior-tree-nodes",level:3},{value:"Isaac-Specific Navigation Features",id:"isaac-specific-navigation-features",level:2},{value:"GPU-Accelerated Path Planning",id:"gpu-accelerated-path-planning",level:3},{value:"Multi-Sensor Costmap Integration",id:"multi-sensor-costmap-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Adaptive Navigation Parameters",id:"adaptive-navigation-parameters",level:3},{value:"Integration with Isaac Sim",id:"integration-with-isaac-sim",level:2},{value:"Simulation-Specific Navigation",id:"simulation-specific-navigation",level:3},{value:"Best Practices for Isaac-Enhanced Navigation",id:"best-practices-for-isaac-enhanced-navigation",level:2},{value:"Configuration Best Practices",id:"configuration-best-practices",level:3},{value:"Performance Best Practices",id:"performance-best-practices",level:3},{value:"Integration Best Practices",id:"integration-best-practices",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function _(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"navigation-stack-nav2-with-isaac",children:"Navigation Stack (Nav2) with Isaac"})}),"\n",(0,i.jsx)(n.h2,{id:"chapter-objectives",children:"Chapter Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate Isaac ROS with the Navigation2 stack"}),"\n",(0,i.jsx)(n.li,{children:"Configure GPU-accelerated navigation components"}),"\n",(0,i.jsx)(n.li,{children:"Implement perception-aware navigation for Physical AI"}),"\n",(0,i.jsx)(n.li,{children:"Create behavior trees for complex navigation tasks"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate and optimize navigation performance in Isaac Sim"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"navigation2-overview",children:"Navigation2 Overview"}),"\n",(0,i.jsx)(n.p,{children:"Navigation2 (Nav2) is the next-generation navigation stack for ROS 2, designed for autonomous mobile robots. When combined with Isaac's GPU-accelerated capabilities, it provides powerful navigation solutions for Physical AI applications."}),"\n",(0,i.jsx)(n.h3,{id:"nav2-architecture",children:"Nav2 Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Behavior      \u2502    \u2502   Planning      \u2502    \u2502   Control       \u2502\n\u2502   Trees         \u2502\u2500\u2500\u2500\u25b6\u2502   & Costmaps    \u2502\u2500\u2500\u2500\u25b6\u2502   & Smoothing   \u2502\n\u2502   (Task Logic)  \u2502    \u2502   (Path Finding)\u2502    \u2502   (Motion)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2                       \u25b2                       \u25b2\n         \u2502                       \u2502                       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Perception    \u2502    \u2502   Localization  \u2502    \u2502   Safety &      \u2502\n\u2502   & Mapping     \u2502    \u2502   (AMCL, SLAM)  \u2502    \u2502   Recovery      \u2502\n\u2502   (Isaac ROS)   \u2502    \u2502                 \u2502    \u2502   (Actions)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Behavior Trees"}),": Task orchestration and decision making"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Planner Server"}),": Global and local path planning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Controller Server"}),": Local trajectory generation and control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recovery Server"}),": Behavior for escaping failure conditions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lifecycle Manager"}),": Component state management"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"isaac-integration-with-nav2",children:"Isaac Integration with Nav2"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-accelerated-components",children:"GPU-Accelerated Components"}),"\n",(0,i.jsx)(n.p,{children:"Isaac provides GPU-accelerated alternatives for key navigation components:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": GPU-accelerated image processing for perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": GPU-accelerated simultaneous localization and mapping"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Detection"}),": GPU-accelerated object detection and tracking"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Point Cloud Utils"}),": GPU-accelerated point cloud processing"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"isaac-nav2-package-structure",children:"Isaac Nav2 Package Structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Isaac-specific Nav2 configuration\nisaac_nav2:\n  ros__parameters:\n    # Use Isaac-optimized components where available\n    use_isaac_image_pipeline: true\n    use_isaac_visual_slam: true\n    use_isaac_detection: true\n\n    # Isaac-specific parameters\n    gpu_compute_mode: "performance"  # Options: performance, power_saving\n    memory_pool_size: 1024  # MB\n    cuda_device_id: 0\n'})}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-isaac-enhanced-navigation",children:"Setting Up Isaac-Enhanced Navigation"}),"\n",(0,i.jsx)(n.h3,{id:"nav2-configuration-with-isaac",children:"Nav2 Configuration with Isaac"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# File: config/isaac_nav2_params.yaml\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_footprint"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n    scan_topic: scan\n\namcl_map_client:\n  ros__parameters:\n    use_sim_time: True\n\namcl_rclcpp_node:\n  ros__parameters:\n    use_sim_time: True\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_footprint\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n    - nav2_clear_costmap_service_bt_node\n    - nav2_is_stuck_condition_bt_node\n    - nav2_goal_reached_condition_bt_node\n    - nav2_goal_updated_condition_bt_node\n    - nav2_initial_pose_received_condition_bt_node\n    - nav2_reinitialize_global_localization_service_bt_node\n    - nav2_rate_controller_bt_node\n    - nav2_distance_controller_bt_node\n    - nav2_speed_controller_bt_node\n    - nav2_truncate_path_action_bt_node\n    - nav2_goal_updater_node_bt_node\n    - nav2_recovery_node_bt_node\n    - nav2_pipeline_sequence_bt_node\n    - nav2_round_robin_node_bt_node\n    - nav2_transform_available_condition_bt_node\n    - nav2_time_expired_condition_bt_node\n    - nav2_path_expiring_timer_condition\n    - nav2_distance_traveled_condition_bt_node\n    - nav2_single_trigger_bt_node\n    - nav2_is_battery_low_condition_bt_node\n    - nav2_navigate_through_poses_action_bt_node\n    - nav2_navigate_to_pose_action_bt_node\n    - nav2_remove_passed_goals_action_bt_node\n    - nav2_planner_selector_bt_node\n    - nav2_controller_selector_bt_node\n    - nav2_goal_checker_selector_bt_node\n\nbt_navigator_rclcpp_node:\n  ros__parameters:\n    use_sim_time: True\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["FollowPath"]\n\n    # Progress checker parameters\n    progress_checker:\n      plugin: "nav2_controller::SimpleProgressChecker"\n      required_movement_radius: 0.5\n      movement_time_allowance: 10.0\n\n    # Goal checker parameters\n    goal_checker:\n      plugin: "nav2_controller::SimpleGoalChecker"\n      xy_goal_tolerance: 0.25\n      yaw_goal_tolerance: 0.25\n      stateful: True\n\n    # Controller parameters\n    FollowPath:\n      plugin: "nav2_rotation_shim_controller::RotationShimController"\n      progress_checker_plugin: "progress_checker"\n      goal_checker_plugin: "goal_checker"\n      required_movement_radius: 0.5\n      movement_time_allowance: 10.0\n      velocity_deadband: 0.05\n      simulate_ahead_time: 1.0\n      max_rotational_vel: 1.0\n      min_rotational_vel: 0.4\n      rotational_acc_lim: 3.2\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      global_frame: odom\n      robot_base_frame: base_footprint\n      use_sim_time: True\n      rolling_window: true\n      width: 3\n      height: 3\n      resolution: 0.05\n      robot_radius: 0.3\n      plugins: ["voxel_layer", "inflation_layer"]\n\n      # Isaac-optimized voxel layer\n      voxel_layer:\n        plugin: "isaac_ros_costmap_2d::IsaacVoxelLayer"  # Hypothetical Isaac-optimized plugin\n        enabled: True\n        publish_voxel_map: True\n        origin_z: 0.0\n        z_resolution: 0.05\n        z_voxels: 16\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: scan camera_points\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n        camera_points:\n          topic: /camera/depth/points\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "PointCloud2"\n          min_obstacle_height: 0.0\n          obstacle_range: 2.5\n\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n      always_send_full_costmap: True\n  local_costmap_client:\n    ros__parameters:\n      use_sim_time: True\n  local_costmap_rclcpp_node:\n    ros__parameters:\n      use_sim_time: True\n\nglobal_costmap:\n  global_costmap:\n    ros__parameters:\n      update_frequency: 1.0\n      publish_frequency: 1.0\n      global_frame: map\n      robot_base_frame: base_footprint\n      use_sim_time: True\n      robot_radius: 0.3\n      resolution: 0.05\n      track_unknown_space: true\n      plugins: ["static_layer", "obstacle_layer", "inflation_layer"]\n\n      # Isaac-optimized obstacle layer\n      obstacle_layer:\n        plugin: "isaac_ros_costmap_2d::IsaacObstacleLayer"  # Hypothetical Isaac-optimized plugin\n        enabled: True\n        observation_sources: scan camera_points\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n        camera_points:\n          topic: /camera/depth/points\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "PointCloud2"\n          min_obstacle_height: 0.0\n          obstacle_range: 2.5\n\n      static_layer:\n        plugin: "nav2_costmap_2d::StaticLayer"\n        map_subscribe_transient_local: True\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n      always_send_full_costmap: True\n  global_costmap_client:\n    ros__parameters:\n      use_sim_time: True\n  global_costmap_rclcpp_node:\n    ros__parameters:\n      use_sim_time: True\n\nmap_server:\n  ros__parameters:\n    use_sim_time: True\n    yaml_filename: "turtlebot3_world.yaml"\n\nmap_saver:\n  ros__parameters:\n    use_sim_time: True\n    save_map_timeout: 5.0\n    free_thresh_default: 0.25\n    occupied_thresh_default: 0.65\n\nplanner_server:\n  ros__parameters:\n    expected_planner_frequency: 20.0\n    use_sim_time: True\n    planner_plugins: ["GridBased"]\n    GridBased:\n      plugin: "nav2_navfn_planner/NavfnPlanner"\n      tolerance: 0.5\n      use_astar: false\n      allow_unknown: true\n\nplanner_server_rclcpp_node:\n  ros__parameters:\n    use_sim_time: True\n\nrecoveries_server:\n  ros__parameters:\n    costmap_topic: local_costmap/costmap_raw\n    footprint_topic: local_costmap/published_footprint\n    cycle_frequency: 10.0\n    recovery_plugins: ["spin", "backup", "wait"]\n    spin:\n      plugin: "nav2_recoveries/Spin"\n      sim_frequency: 10\n      angle_instep_thresh: 0.05\n      angle_tolerance: 0.1\n      max_rotational_vel: 1.0\n      min_rotational_vel: 0.4\n    backup:\n      plugin: "nav2_recoveries/BackUp"\n      sim_frequency: 10\n      distance: 0.15\n      forward_sampling_distance: 0.05\n      move_time_allowance: 10.0\n      max_translation_vel: 0.25\n    wait:\n      plugin: "nav2_recoveries/Wait"\n      sim_frequency: 10\n      backup_distance: 0.15\n      time_allowance: 5.0\n\nrobot_state_publisher:\n  ros__parameters:\n    use_sim_time: True\n'})}),"\n",(0,i.jsx)(n.h3,{id:"isaac-specific-navigation-launch-file",children:"Isaac-Specific Navigation Launch File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: launch/isaac_nav2.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, RegisterEventHandler\nfrom launch.event_handlers import OnProcessExit\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nfrom nav2_common.launch import ReplaceString\nimport os\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    params_file = LaunchConfiguration('params_file', default='isaac_nav2_params.yaml')\n    bt_xml_file = LaunchConfiguration('bt_xml_file', default='navigate_w_replanning_and_recovery.xml')\n\n    # Get package paths\n    pkg_nav2_bringup = FindPackageShare('nav2_bringup')\n    pkg_isaac_nav2 = FindPackageShare('isaac_nav2_demos')\n\n    # Launch navigation\n    navigation = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                pkg_nav2_bringup,\n                'launch',\n                'navigation_launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time,\n            'params_file': PathJoinSubstitution([\n                pkg_isaac_nav2,\n                'config',\n                params_file\n            ])\n        }.items()\n    )\n\n    # Isaac perception pipeline\n    perception = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('isaac_ros_perceptor'),\n                'launch',\n                'perceptor.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time\n        }.items()\n    )\n\n    # Isaac visual SLAM\n    visual_slam = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('isaac_ros_visual_slam'),\n                'launch',\n                'visual_slam.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time\n        }.items()\n    )\n\n    # Isaac image pipeline\n    image_pipeline = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('isaac_ros_image_pipeline'),\n                'launch',\n                'image_pipeline.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time\n        }.items()\n    )\n\n    # RViz\n    rviz_config_file = PathJoinSubstitution([\n        pkg_nav2_bringup,\n        'rviz',\n        'nav2_default_view.rviz'\n    ])\n\n    rviz = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', rviz_config_file],\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Add event handlers to ensure proper startup order\n    startup_order = []\n\n    # Start perception first\n    startup_order.append(perception)\n\n    # Then visual SLAM (depends on perception)\n    startup_order.append(RegisterEventHandler(\n        OnProcessExit(\n            target_action=perception,\n            on_exit=[visual_slam]\n        )\n    ))\n\n    # Then navigation (depends on SLAM)\n    startup_order.append(RegisterEventHandler(\n        OnProcessExit(\n            target_action=visual_slam,\n            on_exit=[navigation]\n        )\n    ))\n\n    # Finally RViz (depends on navigation)\n    startup_order.append(RegisterEventHandler(\n        OnProcessExit(\n            target_action=navigation,\n            on_exit=[rviz]\n        )\n    ))\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation time'\n        ),\n        DeclareLaunchArgument(\n            'params_file',\n            default_value='isaac_nav2_params.yaml',\n            description='Navigation parameters file'\n        ),\n        DeclareLaunchArgument(\n            'bt_xml_file',\n            default_value='navigate_w_replanning_and_recovery.xml',\n            description='Behavior tree XML file'\n        )\n    ] + startup_order)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"perception-aware-navigation",children:"Perception-Aware Navigation"}),"\n",(0,i.jsx)(n.h3,{id:"multi-sensor-fusion-for-navigation",children:"Multi-Sensor Fusion for Navigation"}),"\n",(0,i.jsx)(n.p,{children:"Isaac enables navigation systems to use multiple sensor modalities:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# File: navigation/perception_aware_navigator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, PointCloud2, CameraInfo\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Float32\nfrom tf2_ros import Buffer, TransformListener\nimport numpy as np\nfrom scipy.spatial import KDTree\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass PerceptionAwareNavigator(Node):\n    def __init__(self):\n        super().__init__(\'perception_aware_navigator\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # TF buffer for transformations\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10)\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.depth_sub = self.create_subscription(\n            PointCloud2, \'/camera/depth/points\', self.depth_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10)\n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'/goal_pose\', self.goal_callback, 10)\n\n        # Publishers\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.occupancy_pub = self.create_publisher(\n            MarkerArray, \'/perception_map\', 10)\n        self.safety_pub = self.create_publisher(Float32, \'/safety_score\', 10)\n\n        # Navigation state\n        self.current_pose = None\n        self.current_goal = None\n        self.laser_data = None\n        self.camera_matrix = None\n        self.perception_objects = []\n\n        # Navigation parameters\n        self.safety_distance = 0.5\n        self.perception_threshold = 0.7\n\n        self.get_logger().info(\'Perception-Aware Navigator Started\')\n\n    def scan_callback(self, msg):\n        """Process laser scan data for navigation"""\n        self.laser_data = msg\n\n        # Update local costmap based on laser data\n        self.update_laser_costmap()\n\n    def image_callback(self, msg):\n        """Process camera image for object detection"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Run object detection (simplified)\n            objects = self.detect_objects(cv_image)\n\n            # Update perception map\n            self.perception_objects = objects\n\n            # Check if detected objects affect navigation\n            self.update_navigation_plan()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def depth_callback(self, msg):\n        """Process depth data for 3D obstacle detection"""\n        # Convert point cloud to usable format\n        # Update 3D costmap\n        pass\n\n    def odom_callback(self, msg):\n        """Update robot pose"""\n        self.current_pose = msg.pose.pose\n\n    def goal_callback(self, msg):\n        """Update navigation goal"""\n        self.current_goal = msg.pose\n        self.plan_path_to_goal()\n\n    def detect_objects(self, image):\n        """Detect objects in image using GPU-accelerated methods"""\n        # This would use Isaac ROS detection packages\n        # For demonstration, using a simple approach\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect contours (simplified)\n        _, thresh = cv2.threshold(gray, 127, 255, 0)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        objects = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                objects.append({\n                    \'bbox\': (x, y, w, h),\n                    \'center\': (x + w/2, y + h/2),\n                    \'area\': w * h\n                })\n\n        return objects\n\n    def update_laser_costmap(self):\n        """Update costmap based on laser data"""\n        if self.laser_data is None:\n            return\n\n        # Process laser ranges to identify obstacles\n        obstacle_ranges = [\n            r for r in self.laser_data.ranges\n            if self.laser_data.range_min < r < self.laser_data.range_max\n        ]\n\n        # Update local planner with obstacle information\n        min_distance = min(obstacle_ranges) if obstacle_ranges else float(\'inf\')\n\n        if min_distance < self.safety_distance:\n            # Emergency stop or replan\n            self.emergency_stop()\n        else:\n            # Continue normal navigation\n            self.execute_navigation()\n\n    def update_navigation_plan(self):\n        """Update navigation plan based on perception data"""\n        if not self.perception_objects or not self.current_goal:\n            return\n\n        # Check if detected objects are on the planned path\n        for obj in self.perception_objects:\n            obj_pos = self.camera_to_world(obj[\'center\'])\n            if self.is_on_path(obj_pos):\n                # Object is on path, replan\n                self.replan_around_object(obj)\n\n    def camera_to_world(self, pixel_coords):\n        """Convert camera pixel coordinates to world coordinates"""\n        # This would use camera matrix and robot pose\n        # Simplified for demonstration\n        return np.array([0.0, 0.0, 0.0])\n\n    def is_on_path(self, position):\n        """Check if a position is on the current navigation path"""\n        # Implementation would check against current path\n        return False\n\n    def replan_around_object(self, obj):\n        """Replan navigation to go around detected object"""\n        # This would trigger a replanning action\n        self.get_logger().info(f\'Replanning around object at {obj}\')\n\n    def plan_path_to_goal(self):\n        """Plan path to current goal"""\n        # This would call the global planner\n        pass\n\n    def execute_navigation(self):\n        """Execute navigation with current plan"""\n        # This would call the local planner and controller\n        cmd = Twist()\n        # Set appropriate velocities\n        self.cmd_pub.publish(cmd)\n\n    def emergency_stop(self):\n        """Stop robot in emergency situation"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    navigator = PerceptionAwareNavigator()\n\n    try:\n        rclpy.spin(navigator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        navigator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"behavior-trees-for-complex-navigation",children:"Behavior Trees for Complex Navigation"}),"\n",(0,i.jsx)(n.h3,{id:"custom-behavior-tree-nodes",children:"Custom Behavior Tree Nodes"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# File: navigation/behavior_tree_nodes.py\nimport py_trees\nfrom py_trees.behaviours import SuccessEveryN\nfrom py_trees.decorators import Inverter, RetryUntilSuccessful\nfrom py_trees.composites import Sequence, Selector, Parallel\nfrom py_trees import common\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Bool\n\nclass IsaacNavigateToPose(Node):\n    """Custom behavior tree node for Isaac-enhanced navigation"""\n\n    def __init__(self, name, goal_pose):\n        super().__init__(name)\n        self.goal_pose = goal_pose\n        self.result = None\n\n    def setup(self, **kwargs):\n        # Initialize ROS clients\n        self.action_client = ActionClient(self, NavigateToPose, \'/navigate_to_pose\')\n\n    def update(self):\n        # Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = self.goal_pose\n\n        self.action_client.wait_for_server()\n        future = self.action_client.send_goal_async(goal_msg)\n\n        # This is simplified - in practice, you\'d handle the async result\n        return common.Status.RUNNING\n\nclass PerceptionCheck(py_trees.behaviour.Behaviour):\n    """Check if environment is safe based on perception data"""\n\n    def __init__(self, name):\n        super().__init__(name)\n        self.blackboard = py_trees.blackboard.Blackboard()\n\n    def setup(self, **kwargs):\n        # Setup ROS subscriptions for perception data\n        pass\n\n    def update(self):\n        # Check perception data for obstacles or hazards\n        safety_score = self.blackboard.get(\'safety_score\', 1.0)\n\n        if safety_score > 0.8:\n            return common.Status.SUCCESS\n        else:\n            return common.Status.FAILURE\n\nclass DynamicReplanning(py_trees.behaviour.Behaviour):\n    """Dynamically replan when obstacles are detected"""\n\n    def __init__(self, name):\n        super().__init__(name)\n        self.blackboard = py_trees.blackboard.Blackboard()\n\n    def update(self):\n        # Check for dynamic obstacles\n        dynamic_obstacles = self.blackboard.get(\'dynamic_obstacles\', 0)\n\n        if dynamic_obstacles > 0:\n            # Trigger replanning\n            return common.Status.SUCCESS\n        else:\n            return common.Status.FAILURE\n\ndef create_navigation_behavior_tree():\n    """Create a behavior tree for complex navigation tasks"""\n\n    # Main sequence\n    root = Sequence(name="NavigateWithPerception")\n\n    # Check if safe to navigate\n    safety_check = PerceptionCheck(name="SafetyCheck")\n\n    # Navigate to goal\n    navigate = IsaacNavigateToPose(\n        name="NavigateToGoal",\n        goal_pose=PoseStamped()  # Would be set dynamically\n    )\n\n    # Handle dynamic obstacles\n    obstacle_handler = Selector(name="HandleObstacles")\n    dynamic_check = DynamicReplanning(name="CheckDynamicObstacles")\n    replan_action = SuccessEveryN(name="Replan", n=1)  # Simplified replanning\n\n    # Build tree structure\n    obstacle_handler.add_children([dynamic_check, replan_action])\n\n    root.add_children([safety_check, navigate, obstacle_handler])\n\n    return root\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-specific-navigation-features",children:"Isaac-Specific Navigation Features"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-accelerated-path-planning",children:"GPU-Accelerated Path Planning"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: GPU-accelerated path planning using Isaac\nclass IsaacPathPlanner:\n    def __init__(self):\n        self.gpu_planner = None  # Would be Isaac-optimized planner\n        self.map_resolution = 0.05\n        self.planning_timeout = 5.0\n\n    def plan_path_gpu(self, start, goal, costmap):\n        """Plan path using GPU acceleration"""\n        # Convert costmap to GPU-compatible format\n        gpu_costmap = self.upload_to_gpu(costmap)\n\n        # Execute GPU path planning\n        path = self.gpu_planner.plan(start, goal, gpu_costmap)\n\n        return path\n\n    def upload_to_gpu(self, costmap):\n        """Upload costmap data to GPU memory"""\n        # Implementation would use CUDA or similar\n        return costmap  # Placeholder\n\n    def smooth_path_gpu(self, path):\n        """Smooth path using GPU acceleration"""\n        # Apply GPU-accelerated path smoothing\n        return path  # Placeholder\n'})}),"\n",(0,i.jsx)(n.h3,{id:"multi-sensor-costmap-integration",children:"Multi-Sensor Costmap Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Isaac-enhanced costmap with multiple sensors\nclass IsaacMultiSensorCostmap:\n    def __init__(self):\n        self.laser_layer = None\n        self.vision_layer = None\n        self.depth_layer = None\n        self.fusion_weights = {\n            \'laser\': 0.5,\n            \'vision\': 0.3,\n            \'depth\': 0.2\n        }\n\n    def update_from_sensors(self, laser_data, image_data, depth_data):\n        """Update costmap from multiple sensor sources"""\n\n        # Update individual layers\n        laser_costmap = self.process_laser_data(laser_data)\n        vision_costmap = self.process_vision_data(image_data)\n        depth_costmap = self.process_depth_data(depth_data)\n\n        # Fuse costmaps using weighted combination\n        combined_costmap = (\n            self.fusion_weights[\'laser\'] * laser_costmap +\n            self.fusion_weights[\'vision\'] * vision_costmap +\n            self.fusion_weights[\'depth\'] * depth_costmap\n        )\n\n        return combined_costmap\n\n    def process_laser_data(self, laser_data):\n        """Process laser scan data into cost values"""\n        # Convert laser ranges to occupancy grid\n        return np.zeros((100, 100))  # Placeholder\n\n    def process_vision_data(self, image_data):\n        """Process visual data for obstacle detection"""\n        # Use Isaac ROS detection to identify obstacles\n        return np.zeros((100, 100))  # Placeholder\n\n    def process_depth_data(self, depth_data):\n        """Process depth data for 3D obstacle detection"""\n        # Convert point cloud to elevation map\n        return np.zeros((100, 100))  # Placeholder\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"adaptive-navigation-parameters",children:"Adaptive Navigation Parameters"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Example: Adaptive navigation based on environment\nclass AdaptiveNavigator:\n    def __init__(self):\n        self.current_env_type = 'unknown'\n        self.navigation_params = {\n            'open_space': {\n                'max_vel': 1.0,\n                'min_turn_radius': 0.1,\n                'local_planner_horizon': 3.0\n            },\n            'cluttered': {\n                'max_vel': 0.3,\n                'min_turn_radius': 0.05,\n                'local_planner_horizon': 1.5\n            },\n            'narrow': {\n                'max_vel': 0.2,\n                'min_turn_radius': 0.03,\n                'local_planner_horizon': 1.0\n            }\n        }\n\n    def assess_environment(self, costmap):\n        \"\"\"Assess environment type based on costmap\"\"\"\n        free_space_ratio = self.calculate_free_space_ratio(costmap)\n        obstacle_density = self.calculate_obstacle_density(costmap)\n\n        if free_space_ratio > 0.7:\n            return 'open_space'\n        elif obstacle_density > 0.3:\n            return 'cluttered'\n        else:\n            return 'narrow'\n\n    def calculate_free_space_ratio(self, costmap):\n        \"\"\"Calculate ratio of free space in costmap\"\"\"\n        total_cells = costmap.size\n        free_cells = np.sum(costmap < 50)  # Assuming <50 is free space\n        return free_cells / total_cells if total_cells > 0 else 0\n\n    def calculate_obstacle_density(self, costmap):\n        \"\"\"Calculate obstacle density in costmap\"\"\"\n        obstacle_cells = np.sum(costmap > 80)  # Assuming >80 is obstacle\n        total_cells = costmap.size\n        return obstacle_cells / total_cells if total_cells > 0 else 0\n\n    def update_navigation_params(self, env_type):\n        \"\"\"Update navigation parameters based on environment\"\"\"\n        params = self.navigation_params[env_type]\n\n        # Update Nav2 parameters dynamically\n        # This would involve service calls to update parameters\n        self.get_logger().info(f'Updated params for {env_type}: {params}')\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-isaac-sim",children:"Integration with Isaac Sim"}),"\n",(0,i.jsx)(n.h3,{id:"simulation-specific-navigation",children:"Simulation-Specific Navigation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Navigation that works well in Isaac Sim\nclass IsaacSimNavigator:\n    def __init__(self):\n        self.simulation_mode = True\n        self.ground_truth_available = False\n\n    def enable_ground_truth_navigation(self):\n        """Use ground truth poses for more accurate navigation in simulation"""\n        if self.simulation_mode:\n            # Subscribe to ground truth topics in Isaac Sim\n            self.ground_truth_sub = self.create_subscription(\n                Odometry, \'/ground_truth/odometry\', self.ground_truth_callback, 10)\n            self.ground_truth_available = True\n\n    def ground_truth_callback(self, msg):\n        """Update navigation with ground truth pose"""\n        if self.ground_truth_available:\n            # Use ground truth for more accurate localization\n            self.current_pose = msg.pose.pose\n\n    def validate_navigation_performance(self):\n        """Validate navigation using simulation ground truth"""\n        if self.ground_truth_available:\n            # Compare planned path with actual path\n            # Calculate metrics like path efficiency, success rate, etc.\n            pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-isaac-enhanced-navigation",children:"Best Practices for Isaac-Enhanced Navigation"}),"\n",(0,i.jsx)(n.h3,{id:"configuration-best-practices",children:"Configuration Best Practices"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use appropriate sensor fusion for your environment"}),"\n",(0,i.jsx)(n.li,{children:"Configure costmap layers for your specific robot and sensors"}),"\n",(0,i.jsx)(n.li,{children:"Tune planner and controller parameters for your robot dynamics"}),"\n",(0,i.jsx)(n.li,{children:"Implement proper safety checks and emergency stops"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-best-practices",children:"Performance Best Practices"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use GPU-accelerated components where available"}),"\n",(0,i.jsx)(n.li,{children:"Optimize sensor data processing pipelines"}),"\n",(0,i.jsx)(n.li,{children:"Implement adaptive parameters based on environment"}),"\n",(0,i.jsx)(n.li,{children:"Monitor and log performance metrics continuously"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"integration-best-practices",children:"Integration Best Practices"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Start with simple navigation tasks and gradually add complexity"}),"\n",(0,i.jsx)(n.li,{children:"Validate in simulation before testing on real hardware"}),"\n",(0,i.jsx)(n.li,{children:"Implement comprehensive error handling and recovery"}),"\n",(0,i.jsx)(n.li,{children:"Plan for graceful degradation when sensors fail"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,i.jsx)(n.p,{children:"Isaac integration with Navigation2 provides powerful GPU-accelerated navigation capabilities for Physical AI systems. By combining Isaac's perception and SLAM capabilities with Nav2's navigation framework, robots can achieve robust autonomous navigation in complex environments. The integration includes multi-sensor fusion, behavior trees for complex tasks, and adaptive parameters for different environments."}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up Isaac-enhanced navigation in Isaac Sim with multiple sensors."}),"\n",(0,i.jsx)(n.li,{children:"Implement a behavior tree for navigation with obstacle avoidance."}),"\n",(0,i.jsx)(n.li,{children:"Evaluate navigation performance with different sensor configurations."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, we'll explore the perception pipeline with Isaac, learning how to build intelligent perception systems that enable Physical AI applications to understand and interact with their environment."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(_,{...e})}):_(e)}}}]);