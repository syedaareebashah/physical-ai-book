"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9603],{1451:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"appendices/glossary","title":"Appendix E: Glossary","description":"A","source":"@site/docs/appendices/glossary.md","sourceDirName":"appendices","slug":"/appendices/glossary","permalink":"/physical-ai-book/docs/appendices/glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/appendices/glossary.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Appendix D: Resources and Links","permalink":"/physical-ai-book/docs/appendices/resources-links"}}');var o=i(4848),r=i(8453);const t={sidebar_position:5},a="Appendix E: Glossary",l={},c=[{value:"A",id:"a",level:2},{value:"B",id:"b",level:2},{value:"C",id:"c",level:2},{value:"D",id:"d",level:2},{value:"E",id:"e",level:2},{value:"F",id:"f",level:2},{value:"G",id:"g",level:2},{value:"H",id:"h",level:2},{value:"I",id:"i",level:2},{value:"J",id:"j",level:2},{value:"K",id:"k",level:2},{value:"L",id:"l",level:2},{value:"M",id:"m",level:2},{value:"N",id:"n",level:2},{value:"O",id:"o",level:2},{value:"P",id:"p",level:2},{value:"Q",id:"q",level:2},{value:"R",id:"r",level:2},{value:"S",id:"s",level:2},{value:"T",id:"t",level:2},{value:"U",id:"u",level:2},{value:"V",id:"v",level:2},{value:"W",id:"w",level:2},{value:"X",id:"x",level:2},{value:"Y",id:"y",level:2},{value:"Z",id:"z",level:2},{value:"Acronyms and Abbreviations",id:"acronyms-and-abbreviations",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",hr:"hr",p:"p",strong:"strong",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"appendix-e-glossary",children:"Appendix E: Glossary"})}),"\n",(0,o.jsx)(e.h2,{id:"a",children:"A"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Action Space"}),": The set of all possible actions that an agent can take in an environment. In robotics, this includes all possible movements, manipulations, or other behaviors the robot can perform."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Affordance"}),": A property of an object or environment that indicates what actions are possible for an agent. For example, a handle affords grasping, a button affords pressing."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"AI Foundation Model"}),": A large-scale AI model trained on diverse data that can be adapted to various downstream tasks. Examples include GPT for language and DALL-E for vision."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Articulated Robot"}),": A robot composed of multiple rigid bodies connected by joints, allowing for complex movements. Examples include robotic arms and humanoid robots."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Autonomous System"}),": A system capable of performing tasks without direct human intervention, often incorporating AI for decision-making and adaptation."]}),"\n",(0,o.jsx)(e.h2,{id:"b",children:"B"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Behavior Tree"}),": A hierarchical structure used in robotics and AI to represent and execute complex behaviors. It provides a way to organize and control the execution of tasks."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Biomechanics"}),": The study of the structure, function, and motion of the mechanical aspects of biological systems, particularly relevant for humanoid robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Blob Detection"}),": A computer vision technique for detecting connected regions of pixels that share similar properties, often used for object detection in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Bounded Rationality"}),": The idea that decision-making is limited by available information, cognitive limitations, and time constraints, which is crucial for physical AI systems operating in real-time."]}),"\n",(0,o.jsx)(e.h2,{id:"c",children:"C"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Categorical Perception"}),": The phenomenon where continuous physical stimuli are perceived as discrete categories, important for how robots categorize objects and environments."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Cognitive Architecture"}),": A blueprint for intelligent agents, specifying the structure of memory, modules, and operations that enable intelligent behavior."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Commonsense Reasoning"}),": The ability to make sound judgments and inferences about everyday situations without formal training, essential for robots operating in human environments."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Computer Vision"}),": A field of artificial intelligence that trains computers to interpret and understand the visual world, crucial for physical AI perception."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Continual Learning"}),": The ability of a model to learn new tasks without forgetting previous ones, important for robots that must adapt to new situations over time."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Convolutional Neural Network (CNN)"}),": A class of deep neural networks commonly used for analyzing visual imagery, widely used in robotics for perception tasks."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Cross-Modal Learning"}),": Learning that involves multiple sensory modalities (e.g., vision and language), important for physical AI systems that must integrate different types of information."]}),"\n",(0,o.jsx)(e.h2,{id:"d",children:"D"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Deep Learning"}),": A subset of machine learning based on artificial neural networks with representation learning, fundamental to modern physical AI systems."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Deep Reinforcement Learning"}),": A combination of deep learning and reinforcement learning where neural networks approximate value functions or policies, used for robot control and navigation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Dexterity"}),": The skill and grace of physical movement, especially of the hands and fingers, important for robotic manipulation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Differentiable Physics"}),": Physics simulation that allows gradients to flow through the simulation for learning, enabling end-to-end training of physical AI systems."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Digital Twin"}),": A virtual replica of a physical system that serves as a real-time digital counterpart, used for simulation and testing of physical AI systems."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Domain Randomization"}),": A technique for training neural networks by randomizing simulation environments to improve transfer to real-world scenarios."]}),"\n",(0,o.jsx)(e.h2,{id:"e",children:"E"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Embodied AI"}),": Artificial intelligence that is grounded in physical interaction with the world, where the body plays a crucial role in intelligence."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Embodied Cognition"}),": The theory that many features of cognition are shaped by aspects of the body beyond the brain, relevant to physical AI design."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Episodic Memory"}),": A form of long-term memory that records personal experiences and events, which can be implemented in physical AI systems for learning from experience."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Euclidean Distance"}),": The straight-line distance between two points in Euclidean space, commonly used in robotics for path planning and navigation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Explainable AI (XAI)"}),": Artificial intelligence that can explain its decisions and actions to human users, important for trust in physical AI systems."]}),"\n",(0,o.jsx)(e.h2,{id:"f",children:"F"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Feature Detection"}),": The process of computing representations of image features of interest, fundamental to computer vision in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Field of View (FOV)"}),": The extent of the observable world that is seen at any given moment, important for sensor configuration in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Fine-Tuning"}),": The process of further training a pre-trained model on a specific task, commonly used to adapt foundation models for robotics applications."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Forward Kinematics"}),": The process of determining the position and orientation of the end-effector given the joint angles, fundamental to robot control."]}),"\n",(0,o.jsx)(e.h2,{id:"g",children:"G"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Gaussian Process"}),": A stochastic process used for regression and classification, applicable to uncertainty quantification in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Generalization"}),": The ability of a model to perform well on unseen data, crucial for physical AI systems operating in diverse environments."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Generative Adversarial Network (GAN)"}),": A class of machine learning frameworks designed for generating new data, used in robotics for simulation and planning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Grounding"}),": The process of connecting abstract symbols or concepts to perceptual experience, essential for physical AI systems that must connect language to physical actions."]}),"\n",(0,o.jsx)(e.h2,{id:"h",children:"H"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Haptic Feedback"}),": The use of touch sensations to provide information to a user, important for human-robot interaction."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Human-Robot Interaction (HRI)"}),": The study of interactions between humans and robots, focusing on design and implementation of robots for human environments."]}),"\n",(0,o.jsx)(e.h2,{id:"i",children:"I"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Imitation Learning"}),": Learning by observing and mimicking the behavior of others, used in robotics for teaching complex behaviors."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Information Gain"}),": The expected reduction in entropy caused by partitioning the examples according to a given attribute, used in active learning for robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Inverse Kinematics"}),": The process of determining joint angles needed to achieve a desired end-effector position, fundamental to robot manipulation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Isaac Sim"}),": NVIDIA's simulation environment for robotics, AI, and autonomous systems, designed for training and testing physical AI systems."]}),"\n",(0,o.jsx)(e.h2,{id:"j",children:"J"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Joint Space"}),": The space defined by the joint angles of a robot manipulator, used for robot control and planning."]}),"\n",(0,o.jsx)(e.h2,{id:"k",children:"K"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Kinematics"}),": The branch of mechanics concerned with the motion of objects without reference to the forces that cause the motion, fundamental to robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Knowledge Representation"}),": An area in artificial intelligence dedicated to representing information about the world in a form that a computer can utilize, important for physical AI reasoning."]}),"\n",(0,o.jsx)(e.h2,{id:"l",children:"L"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Language Model"}),": A probability distribution over sequences of words, increasingly important for commanding physical AI systems through natural language."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Learning Rate"}),": A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated, important for training physical AI systems."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"LiDAR"}),": Light Detection and Ranging, a remote sensing method that uses light in the form of a pulsed laser to measure distances, widely used in robotics for navigation and mapping."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Long Short-Term Memory (LSTM)"}),": A type of recurrent neural network that can learn long-term dependencies, used in robotics for sequence modeling and control."]}),"\n",(0,o.jsx)(e.h2,{id:"m",children:"M"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Manipulandum"}),": An object that is manipulated by a person or robot, important in the study of robotic manipulation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Manifold Learning"}),": A class of unsupervised learning algorithms for dimensionality reduction, applicable to high-dimensional sensor data in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Mechanical Advantage"}),": The factor by which a mechanism multiplies the force put into it, relevant to robot design and actuator selection."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Meta-Learning"}),": Learning to learn, where a model learns how to adapt quickly to new tasks, important for robots that must adapt to new environments."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Multimodal Integration"}),": The process of combining information from multiple sensory modalities, crucial for physical AI systems that must understand their environment comprehensively."]}),"\n",(0,o.jsx)(e.h2,{id:"n",children:"N"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Navigation Mesh"}),": A data structure that represents the walkable areas of a game world or environment, used in robotics for path planning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Neural Radiance Fields (NeRF)"}),": A method for synthesizing novel views of complex 3D scenes from a sparse set of images, increasingly used in robotics for scene understanding."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Non-Holonomic Constraint"}),": A constraint that cannot be integrated to give a constraint on positions alone, relevant to wheeled robot motion planning."]}),"\n",(0,o.jsx)(e.h2,{id:"o",children:"O"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Occupancy Grid"}),": A probabilistic representation of space that discretizes the environment into a grid of occupied/free cells, fundamental to robot navigation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Operational Space"}),": The space in which the end-effector of a robot operates, used for motion planning and control."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Overfitting"}),": When a model learns the training data too well, including noise and details that don't generalize, a problem in training physical AI systems."]}),"\n",(0,o.jsx)(e.h2,{id:"p",children:"P"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Part Affordance"}),": The relationship between an object part and the actions it affords, important for robotic manipulation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Path Planning"}),": The process of determining a route from a start point to a goal point, fundamental to robot navigation."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Perceptual Binding"}),": The process of combining different features of an object into a coherent percept, relevant to robot perception."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Phase Space"}),": A space in which all possible states of a system are represented, used in robotics for motion planning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Phenomenology"}),": The study of structures of consciousness as experienced from the first-person point of view, relevant to understanding robot perception."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Point Cloud"}),": A collection of data points in space, typically representing the external surface of an object, used in robotics for 3D scene understanding."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Probabilistic Robotics"}),": A branch of robotics that deals with uncertainty in sensing and actuation, fundamental to real-world physical AI systems."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Proxemics"}),": The study of human use of space and the effects that population density has on behavior, relevant to human-robot interaction."]}),"\n",(0,o.jsx)(e.h2,{id:"q",children:"Q"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Quaternion"}),": A number system that extends the complex numbers, commonly used for representing rotations in robotics and 3D graphics."]}),"\n",(0,o.jsx)(e.h2,{id:"r",children:"R"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Reachability"}),": The ability of a robot to reach a given position in space, fundamental to manipulation planning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Recurrent Neural Network (RNN)"}),": A class of artificial neural networks where connections between nodes form a directed graph with connections pointing backward, used in robotics for sequence modeling."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Reinforcement Learning"}),": A type of machine learning where an agent learns to behave in an environment by performing actions and seeing the results, increasingly used in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Representation Learning"}),": Learning representations of data that make it easier to extract useful information for downstream tasks, fundamental to modern physical AI."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Robot Operating System (ROS)"}),": Flexible framework for writing robot software, providing services designed for heterogeneous computer clusters."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Robotics Middleware"}),": Software that provides services for communication between different components of a robot system, such as ROS."]}),"\n",(0,o.jsx)(e.h2,{id:"s",children:"S"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Scene Graph"}),": A collection of nodes in a graph structure that represents the spatial relationship between objects, used in robotics simulation and visualization."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Semi-Supervised Learning"}),": A class of machine learning that combines a small amount of labeled data with a large amount of unlabeled data, applicable to robotics where labeled data is expensive."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Sensor Fusion"}),": The combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty, crucial for physical AI perception."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Sequential Decision Making"}),": The process of making a series of decisions over time to achieve a long-term goal, fundamental to robot autonomy."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Sim-to-Real Transfer"}),": The process of transferring knowledge learned in simulation to real-world applications, a key challenge in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),": The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location, fundamental to robot autonomy."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Social Robot"}),": A robot that interacts and communicates with humans or other robots using social signals, relevant to human-robot interaction."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Spatial Reasoning"}),": The cognitive process of acquiring and using knowledge about objects and actions in space, fundamental to physical AI."]}),"\n",(0,o.jsx)(e.h2,{id:"t",children:"T"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Task and Motion Planning (TAMP)"}),": An integrated approach to planning at both the task and motion levels, important for complex robotic behaviors."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Temporal Difference Learning"}),": A prediction-based machine learning method, used in robotics for learning value functions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Thompson Sampling"}),": A heuristic for choosing actions that addresses the exploration-exploitation dilemma, applicable to robot learning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Transform Coding"}),": A technique for data compression involving mathematical transforms, used in robotics for efficient sensor data processing."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Turing Test"}),": A test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human, relevant to physical AI evaluation."]}),"\n",(0,o.jsx)(e.h2,{id:"u",children:"U"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Unsupervised Learning"}),": A type of algorithm that learns patterns from unlabeled data, applicable to robotics for discovering structure in sensor data."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Universal Approximator"}),": A mathematical function that can approximate any continuous function to arbitrary precision, relevant to neural network design in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"URDF (Unified Robot Description Format)"}),": An XML format for representing a robot model, used in ROS for robot description."]}),"\n",(0,o.jsx)(e.h2,{id:"v",children:"V"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Vanishing Gradient Problem"}),": The issue where gradients become increasingly small as they propagate backward through a neural network, affecting training of deep networks in robotics."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Variable Impedance Control"}),": Control that allows the robot's impedance to be adjusted based on the task requirements, important for safe human-robot interaction."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Variational Autoencoder (VAE)"}),": A generative model that learns to encode data into a latent space and decode it back, used in robotics for learning representations."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Visual Servoing"}),": The control of a robot using visual feedback, fundamental to vision-based robot control."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Vision-Language-Action (VLA)"}),": A framework that integrates visual perception, language understanding, and physical action, representing the current state-of-the-art in physical AI."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Vision-Language Models (VLM)"}),": Models that jointly understand visual and textual information, increasingly important for commanding robots through natural language."]}),"\n",(0,o.jsx)(e.h2,{id:"w",children:"W"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Workspace"}),": The space within which a robot can operate, important for motion planning and task execution."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"World Coordinate System"}),": A fixed coordinate system used as a reference for all other coordinate systems in a robotic application."]}),"\n",(0,o.jsx)(e.h2,{id:"x",children:"X"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"XR (Extended Reality)"}),": An umbrella term encompassing virtual reality (VR), augmented reality (AR), and mixed reality (MR), increasingly used for robot teleoperation and programming."]}),"\n",(0,o.jsx)(e.h2,{id:"y",children:"Y"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Yaw"}),": The rotation of an object about its vertical axis, one of the three rotational degrees of freedom in robotics."]}),"\n",(0,o.jsx)(e.h2,{id:"z",children:"Z"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Zero-Shot Learning"}),": The ability to recognize objects or perform tasks without having seen examples during training, important for robots operating in novel environments."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"ZMP (Zero Moment Point)"}),": A concept used in robotics and biomechanics to aid in the analysis and control of legged locomotion."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"acronyms-and-abbreviations",children:"Acronyms and Abbreviations"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"AI"}),": Artificial Intelligence"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"API"}),": Application Programming Interface"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"CNN"}),": Convolutional Neural Network"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"CPU"}),": Central Processing Unit"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"CUDA"}),": Compute Unified Device Architecture (NVIDIA parallel computing platform)"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"CV"}),": Computer Vision"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"DNN"}),": Deep Neural Network"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"GAN"}),": Generative Adversarial Network"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"GPU"}),": Graphics Processing Unit"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"GUI"}),": Graphical User Interface"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"HRI"}),": Human-Robot Interaction"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"IDE"}),": Integrated Development Environment"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"IoT"}),": Internet of Things"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"JSON"}),": JavaScript Object Notation"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"LIDAR"}),": Light Detection and Ranging"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"LLM"}),": Large Language Model"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"ML"}),": Machine Learning"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"NLP"}),": Natural Language Processing"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"NVMe"}),": Non-Volatile Memory Express"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"OOP"}),": Object-Oriented Programming"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"PID"}),": Proportional-Integral-Derivative (controller)"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"RAM"}),": Random Access Memory"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"RNN"}),": Recurrent Neural Network"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"ROS"}),": Robot Operating System"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"SLAM"}),": Simultaneous Localization and Mapping"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"SOTA"}),": State-of-the-Art"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"SSL"}),": Semi-Supervised Learning"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"TCP"}),": Transmission Control Protocol"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"UDP"}),": User Datagram Protocol"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"UI"}),": User Interface"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"USB"}),": Universal Serial Bus"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"UX"}),": User Experience"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"VLA"}),": Vision-Language-Action"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"VRAM"}),": Video Random Access Memory"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"XML"}),": eXtensible Markup Language"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"YAML"}),": YAML Ain't Markup Language"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"ZMP"}),": Zero Moment Point"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.p,{children:"This glossary provides definitions for key terms used throughout the Physical AI and robotics literature. Understanding these terms is essential for navigating the complex interdisciplinary field that combines robotics, AI, computer vision, and cognitive science."})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>a});var s=i(6540);const o={},r=s.createContext(o);function t(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:t(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);