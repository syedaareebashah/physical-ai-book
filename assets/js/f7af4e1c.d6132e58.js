"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[3050],{690:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module1-ros2/project","title":"Project: Voice-Controlled Robot Arm","description":"Project Objectives","source":"@site/docs/module1-ros2/06-project.md","sourceDirName":"module1-ros2","slug":"/module1-ros2/project","permalink":"/physical-ai-book/docs/module1-ros2/project","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module1-ros2/06-project.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"URDF for Humanoid Robots","permalink":"/physical-ai-book/docs/module1-ros2/urdf"},"next":{"title":"Module 1 Assessment","permalink":"/physical-ai-book/docs/module1-ros2/assessment"}}');var s=o(4848),t=o(8453);const r={sidebar_position:6},a="Project: Voice-Controlled Robot Arm",l={},c=[{value:"Project Objectives",id:"project-objectives",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Create the Project Package",id:"step-1-create-the-project-package",level:3},{value:"Step 2: Install Dependencies",id:"step-2-install-dependencies",level:3},{value:"Step 3: Voice Recognition Node",id:"step-3-voice-recognition-node",level:3},{value:"Step 4: Command Parser Node",id:"step-4-command-parser-node",level:3},{value:"Step 5: Motion Planning Node",id:"step-5-motion-planning-node",level:3},{value:"Step 6: Safety Monitor Node",id:"step-6-safety-monitor-node",level:3},{value:"Step 7: Package Configuration",id:"step-7-package-configuration",level:3},{value:"Step 8: Launch File",id:"step-8-launch-file",level:3},{value:"Running the System",id:"running-the-system",level:2},{value:"Build and Run",id:"build-and-run",level:3},{value:"Test Commands",id:"test-commands",level:3},{value:"Physical AI Concepts Demonstrated",id:"physical-ai-concepts-demonstrated",level:2},{value:"Multi-Modal Interaction",id:"multi-modal-interaction",level:3},{value:"Safety-First Design",id:"safety-first-design",level:3},{value:"Modular Architecture",id:"modular-architecture",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"project-voice-controlled-robot-arm",children:"Project: Voice-Controlled Robot Arm"})}),"\n",(0,s.jsx)(n.h2,{id:"project-objectives",children:"Project Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By completing this project, you will:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate multiple ROS 2 concepts into a cohesive Physical AI application"}),"\n",(0,s.jsx)(n.li,{children:"Create a voice recognition system that controls robot movements"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor feedback for safe operation"}),"\n",(0,s.jsx)(n.li,{children:"Design a modular system architecture for Physical AI applications"}),"\n",(0,s.jsx)(n.li,{children:"Practice debugging and testing techniques for robotic systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,s.jsx)(n.p,{children:"In this project, we'll build a voice-controlled robot arm that responds to spoken commands. The system will:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Listen for voice commands using speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Parse commands to determine desired arm movements"}),"\n",(0,s.jsx)(n.li,{children:"Execute movements using ROS 2 control interfaces"}),"\n",(0,s.jsx)(n.li,{children:"Provide visual and audio feedback"}),"\n",(0,s.jsx)(n.li,{children:"Include safety mechanisms to prevent dangerous movements"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Voice Input \u2192 Speech Recognition \u2192 Command Parser \u2192 Motion Planning \u2192 Robot Control \u2192 Feedback\n     \u2191                                                                                     \u2193\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Safety Monitor \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-create-the-project-package",children:"Step 1: Create the Project Package"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create the project workspace\nmkdir -p ~/voice_robot_ws/src\ncd ~/voice_robot_ws/src\n\n# Create the package\nros2 pkg create --build-type ament_python voice_robot_control\ncd voice_robot_control\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-install-dependencies",children:"Step 2: Install Dependencies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip3 install speechrecognition pyaudio numpy transforms3d\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-voice-recognition-node",children:"Step 3: Voice Recognition Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: voice_robot_control/voice_robot_control/voice_recognition.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport speech_recognition as sr\nimport threading\nimport queue\n\nclass VoiceRecognitionNode(Node):\n    def __init__(self):\n        super().__init__('voice_recognition_node')\n        self.publisher = self.create_publisher(String, 'voice_command', 10)\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Set up for continuous listening\n        self.listening = True\n        self.command_queue = queue.Queue()\n\n        # Start voice recognition thread\n        self.voice_thread = threading.Thread(target=self.listen_continuously)\n        self.voice_thread.daemon = True\n        self.voice_thread.start()\n\n        # Timer to process recognized commands\n        self.timer = self.create_timer(0.1, self.process_commands)\n\n        self.get_logger().info('Voice Recognition Node Started')\n\n    def listen_continuously(self):\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        while self.listening:\n            try:\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=3)\n\n                # Use Google's speech recognition\n                command = self.recognizer.recognize_google(audio).lower()\n                self.command_queue.put(command)\n\n            except sr.WaitTimeoutError:\n                pass  # No speech detected, continue listening\n            except sr.UnknownValueError:\n                self.get_logger().info('Could not understand audio')\n            except sr.RequestError as e:\n                self.get_logger().error(f'Speech recognition error: {e}')\n\n    def process_commands(self):\n        while not self.command_queue.empty():\n            command = self.command_queue.get()\n            self.get_logger().info(f'Recognized command: {command}')\n\n            # Publish the command\n            msg = String()\n            msg.data = command\n            self.publisher.publish(msg)\n\n    def destroy_node(self):\n        self.listening = False\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceRecognitionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-command-parser-node",children:"Step 4: Command Parser Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: voice_robot_control/voice_robot_control/command_parser.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nfrom sensor_msgs.msg import JointState\nimport re\n\nclass CommandParserNode(Node):\n    def __init__(self):\n        super().__init__('command_parser_node')\n\n        # Subscribers\n        self.voice_sub = self.create_subscription(\n            String, 'voice_command', self.voice_callback, 10)\n\n        # Publishers\n        self.goal_pub = self.create_publisher(Point, 'arm_goal', 10)\n        self.joint_pub = self.create_publisher(JointState, 'joint_commands', 10)\n        self.feedback_pub = self.create_publisher(String, 'system_feedback', 10)\n\n        self.get_logger().info('Command Parser Node Started')\n\n    def voice_callback(self, msg):\n        command = msg.data.lower()\n        self.get_logger().info(f'Processing command: {command}')\n\n        # Parse different types of commands\n        if 'move arm to' in command:\n            self.parse_position_command(command)\n        elif 'pick up' in command:\n            self.parse_pickup_command(command)\n        elif 'put down' in command:\n            self.parse_putdown_command(command)\n        elif 'home position' in command:\n            self.parse_home_command(command)\n        elif 'wave' in command:\n            self.parse_wave_command(command)\n        else:\n            self.send_feedback(f'Unknown command: {command}')\n\n    def parse_position_command(self, command):\n        # Extract coordinates from command like \"move arm to x 10 y 20 z 30\"\n        x_match = re.search(r'x\\s+([+-]?\\d*\\.?\\d+)', command)\n        y_match = re.search(r'y\\s+([+-]?\\d*\\.?\\d+)', command)\n        z_match = re.search(r'z\\s+([+-]?\\d*\\.?\\d+)', command)\n\n        if x_match and y_match and z_match:\n            goal = Point()\n            goal.x = float(x_match.group(1))\n            goal.y = float(y_match.group(1))\n            goal.z = float(z_match.group(1))\n\n            self.goal_pub.publish(goal)\n            self.send_feedback(f'Moving arm to position: ({goal.x}, {goal.y}, {goal.z})')\n        else:\n            self.send_feedback('Could not parse coordinates from command')\n\n    def parse_pickup_command(self, command):\n        # Parse pickup command\n        object_name = command.replace('pick up', '').strip()\n        if object_name:\n            self.send_feedback(f'Attempting to pick up {object_name}')\n            # Publish command to execute pickup sequence\n        else:\n            self.send_feedback('Pickup command needs object name')\n\n    def parse_putdown_command(self, command):\n        # Parse putdown command\n        self.send_feedback('Executing put down sequence')\n        # Publish command to execute put down sequence\n\n    def parse_home_command(self, command):\n        # Move to home position\n        home_pos = Point()\n        home_pos.x = 0.0\n        home_pos.y = 0.0\n        home_pos.z = 0.0\n\n        self.goal_pub.publish(home_pos)\n        self.send_feedback('Moving to home position')\n\n    def parse_wave_command(self, command):\n        # Execute waving motion\n        self.send_feedback('Executing wave motion')\n        # Publish joint commands for waving motion\n\n    def send_feedback(self, message):\n        feedback_msg = String()\n        feedback_msg.data = message\n        self.feedback_pub.publish(feedback_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandParserNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-motion-planning-node",children:"Step 5: Motion Planning Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: voice_robot_control/voice_robot_control/motion_planner.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Point\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import String\nimport numpy as np\nfrom transforms3d.euler import euler2mat\n\nclass MotionPlannerNode(Node):\n    def __init__(self):\n        super().__init__('motion_planner_node')\n\n        # Subscribers\n        self.goal_sub = self.create_subscription(\n            Point, 'arm_goal', self.goal_callback, 10)\n\n        # Publishers\n        self.joint_pub = self.create_publisher(JointState, 'joint_commands', 10)\n        self.feedback_pub = self.create_publisher(String, 'system_feedback', 10)\n\n        # Robot parameters (simplified 3-DOF arm)\n        self.link_lengths = [0.5, 0.4, 0.3]  # Link lengths in meters\n\n        self.get_logger().info('Motion Planner Node Started')\n\n    def goal_callback(self, msg):\n        self.get_logger().info(f'Planning motion to goal: ({msg.x}, {msg.y}, {msg.z})')\n\n        # Check if goal is reachable\n        goal_distance = np.sqrt(msg.x**2 + msg.y**2 + msg.z**2)\n        max_reach = sum(self.link_lengths)\n\n        if goal_distance > max_reach:\n            self.send_feedback('Goal position is out of reach')\n            return\n\n        # Calculate inverse kinematics (simplified)\n        joint_angles = self.calculate_inverse_kinematics(msg.x, msg.y, msg.z)\n\n        if joint_angles is not None:\n            self.publish_joint_commands(joint_angles)\n            self.send_feedback(f'Motion planned: Joint angles {joint_angles}')\n        else:\n            self.send_feedback('Could not calculate motion to goal position')\n\n    def calculate_inverse_kinematics(self, x, y, z):\n        \"\"\"\n        Simplified inverse kinematics for a 3-DOF arm\n        This is a basic implementation - real systems would use more sophisticated algorithms\n        \"\"\"\n        try:\n            # Calculate distance from base to target in XY plane\n            r = np.sqrt(x**2 + y**2)\n\n            # Height\n            h = z\n\n            # Calculate joint angles using geometric approach\n            l1, l2, l3 = self.link_lengths\n\n            # For a 3-DOF arm: base rotation, shoulder, elbow\n            theta1 = np.arctan2(y, x)  # Base rotation\n\n            # Calculate remaining distances\n            d = np.sqrt(r**2 + h**2)  # Distance from base to target\n\n            # Check if target is reachable\n            if d > l2 + l3:\n                return None  # Target too far\n\n            if d < abs(l2 - l3):\n                return None  # Target too close\n\n            # Calculate shoulder and elbow angles\n            cos_theta3 = (l2**2 + l3**2 - d**2) / (2 * l2 * l3)\n            if abs(cos_theta3) > 1:\n                return None\n\n            theta3 = np.arccos(cos_theta3)  # Elbow angle\n            alpha = np.arctan2(h, r)\n            beta = np.arccos((l2**2 + d**2 - l3**2) / (2 * l2 * d))\n\n            theta2 = alpha - beta  # Shoulder angle\n\n            # Convert to joint angles\n            joint_angles = [theta1, theta2, theta3]\n\n            # Check joint limits (example limits)\n            joint_limits = [(-np.pi, np.pi), (-np.pi/2, np.pi/2), (-np.pi, np.pi)]\n\n            for i, (angle, (min_limit, max_limit)) in enumerate(zip(joint_angles, joint_limits)):\n                if angle < min_limit or angle > max_limit:\n                    self.get_logger().warn(f'Joint {i} angle {angle} exceeds limits')\n                    # Adjust or return None based on safety policy\n                    return None\n\n            return joint_angles\n\n        except Exception as e:\n            self.get_logger().error(f'Error in inverse kinematics: {e}')\n            return None\n\n    def publish_joint_commands(self, joint_angles):\n        joint_msg = JointState()\n        joint_msg.name = ['joint1', 'joint2', 'joint3']\n        joint_msg.position = joint_angles\n        joint_msg.velocity = [0.0, 0.0, 0.0]  # Start with zero velocity\n        joint_msg.effort = [0.0, 0.0, 0.0]\n\n        self.joint_pub.publish(joint_msg)\n\n    def send_feedback(self, message):\n        feedback_msg = String()\n        feedback_msg.data = message\n        self.feedback_pub.publish(feedback_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MotionPlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-6-safety-monitor-node",children:"Step 6: Safety Monitor Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: voice_robot_control/voice_robot_control/safety_monitor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nimport numpy as np\n\nclass SafetyMonitorNode(Node):\n    def __init__(self):\n        super().__init__('safety_monitor_node')\n\n        # Subscribers\n        self.joint_sub = self.create_subscription(\n            JointState, 'joint_commands', self.joint_command_callback, 10)\n        self.goal_sub = self.create_subscription(\n            Point, 'arm_goal', self.goal_command_callback, 10)\n\n        # Publishers\n        self.safety_pub = self.create_publisher(String, 'safety_alert', 10)\n        self.emergency_pub = self.create_publisher(String, 'emergency_stop', 10)\n\n        # Safety parameters\n        self.joint_limits = {\n            'joint1': (-2.0, 2.0),   # radians\n            'joint2': (-1.5, 1.5),\n            'joint3': (-2.5, 2.5)\n        }\n\n        self.velocity_limits = [1.0, 1.0, 1.0]  # rad/s\n\n        self.get_logger().info('Safety Monitor Node Started')\n\n    def joint_command_callback(self, msg):\n        # Check joint limits\n        for i, (name, position) in enumerate(zip(msg.name, msg.position)):\n            if name in self.joint_limits:\n                min_limit, max_limit = self.joint_limits[name]\n                if position < min_limit or position > max_limit:\n                    self.send_safety_alert(f'Joint {name} position {position} exceeds limits [{min_limit}, {max_limit}]')\n\n        # Check velocity limits if provided\n        if msg.velocity:\n            for i, velocity in enumerate(msg.velocity):\n                if abs(velocity) > self.velocity_limits[i]:\n                    self.send_safety_alert(f'Joint {i} velocity {velocity} exceeds limit {self.velocity_limits[i]}')\n\n    def goal_command_callback(self, msg):\n        # Check if goal is in safe workspace\n        distance = np.sqrt(msg.x**2 + msg.y**2 + msg.z**2)\n        if distance > 2.0:  # Max safe reach\n            self.send_safety_alert(f'Goal position ({msg.x}, {msg.y}, {msg.z}) too far: distance {distance:.2f}m')\n\n    def send_safety_alert(self, message):\n        self.get_logger().warn(f'SAFETY ALERT: {message}')\n\n        alert_msg = String()\n        alert_msg.data = message\n        self.safety_pub.publish(alert_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SafetyMonitorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-7-package-configuration",children:"Step 7: Package Configuration"}),"\n",(0,s.jsxs)(n.p,{children:["Update ",(0,s.jsx)(n.code,{children:"package.xml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>voice_robot_control</name>\n  <version>0.0.0</version>\n  <description>Package for voice-controlled robot arm</description>\n  <maintainer email="user@example.com">user</maintainer>\n  <license>Apache-2.0</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</end>\n  <depend>geometry_msgs</depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Update ",(0,s.jsx)(n.code,{children:"setup.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\nimport os\nfrom glob import glob\n\npackage_name = 'voice_robot_control'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='user',\n    maintainer_email='user@example.com',\n    description='Package for voice-controlled robot arm',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'voice_recognition = voice_robot_control.voice_recognition:main',\n            'command_parser = voice_robot_control.command_parser:main',\n            'motion_planner = voice_robot_control.motion_planner:main',\n            'safety_monitor = voice_robot_control.safety_monitor:main',\n        ],\n    },\n)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-8-launch-file",children:"Step 8: Launch File"}),"\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:"launch/voice_robot.launch.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='voice_robot_control',\n            executable='voice_recognition',\n            name='voice_recognition_node',\n            output='screen',\n        ),\n        Node(\n            package='voice_robot_control',\n            executable='command_parser',\n            name='command_parser_node',\n            output='screen',\n        ),\n        Node(\n            package='voice_robot_control',\n            executable='motion_planner',\n            name='motion_planner_node',\n            output='screen',\n        ),\n        Node(\n            package='voice_robot_control',\n            executable='safety_monitor',\n            name='safety_monitor_node',\n            output='screen',\n        ),\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"running-the-system",children:"Running the System"}),"\n",(0,s.jsx)(n.h3,{id:"build-and-run",children:"Build and Run"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Build the package\ncd ~/voice_robot_ws\ncolcon build --packages-select voice_robot_control\nsource install/setup.bash\n\n# Run the system\nros2 launch voice_robot_control voice_robot.launch.py\n"})}),"\n",(0,s.jsx)(n.h3,{id:"test-commands",children:"Test Commands"}),"\n",(0,s.jsx)(n.p,{children:"Once running, try these voice commands:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Move arm to x 0.5 y 0.3 z 0.2"'}),"\n",(0,s.jsx)(n.li,{children:'"Home position"'}),"\n",(0,s.jsx)(n.li,{children:'"Wave"'}),"\n",(0,s.jsx)(n.li,{children:'"Pick up object"'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"physical-ai-concepts-demonstrated",children:"Physical AI Concepts Demonstrated"}),"\n",(0,s.jsx)(n.h3,{id:"multi-modal-interaction",children:"Multi-Modal Interaction"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Voice input processing"}),"\n",(0,s.jsx)(n.li,{children:"Sensor feedback integration"}),"\n",(0,s.jsx)(n.li,{children:"Real-time command parsing"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"safety-first-design",children:"Safety-First Design"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Joint limit monitoring"}),"\n",(0,s.jsx)(n.li,{children:"Workspace boundary checking"}),"\n",(0,s.jsx)(n.li,{children:"Emergency stop capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"modular-architecture",children:"Modular Architecture"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Separate nodes for different functions"}),"\n",(0,s.jsx)(n.li,{children:"Clear communication interfaces"}),"\n",(0,s.jsx)(n.li,{children:"Independent development and testing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Microphone Access"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure proper permissions for microphone access"}),"\n",(0,s.jsx)(n.li,{children:"Check that PyAudio is properly installed"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Speech Recognition Accuracy"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use a quiet environment"}),"\n",(0,s.jsx)(n.li,{children:"Speak clearly and at consistent volume"}),"\n",(0,s.jsx)(n.li,{children:"Consider using alternative speech recognition APIs"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Joint Limit Violations"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Verify robot kinematic parameters"}),"\n",(0,s.jsx)(n.li,{children:"Check inverse kinematics calculations"}),"\n",(0,s.jsx)(n.li,{children:"Adjust joint limits as needed"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Communication Issues"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Verify topic names and message types"}),"\n",(0,s.jsx)(n.li,{children:"Check that all nodes are properly connected"}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"ros2 topic list"})," and ",(0,s.jsx)(n.code,{children:"ros2 node list"})," for debugging"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,s.jsx)(n.p,{children:"This project integrates multiple ROS 2 concepts into a comprehensive Physical AI application. It demonstrates voice interaction, motion planning, safety monitoring, and modular system design. The voice-controlled robot arm showcases how ROS 2 enables complex Physical AI systems that interact naturally with humans."}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Add gesture recognition using a camera to complement voice commands."}),"\n",(0,s.jsx)(n.li,{children:"Implement a learning component that improves command recognition over time."}),"\n",(0,s.jsx)(n.li,{children:"Add haptic feedback to enhance the human-robot interaction experience."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"In the next chapter, we'll assess your understanding of ROS 2 concepts through practical challenges and exercises."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var i=o(6540);const s={},t=i.createContext(s);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);