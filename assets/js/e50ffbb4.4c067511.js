"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[217],{2634:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module3-isaac/visual-slam","title":"Visual SLAM with Isaac ROS","description":"Chapter Objectives","source":"@site/docs/module3-isaac/03-visual-slam.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/visual-slam","permalink":"/physical-ai-book/docs/module3-isaac/visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module3-isaac/03-visual-slam.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Deep Dive","permalink":"/physical-ai-book/docs/module3-isaac/isaac-sim"},"next":{"title":"Navigation Stack (Nav2) with Isaac","permalink":"/physical-ai-book/docs/module3-isaac/navigation-stack"}}');var s=a(4848),r=a(8453);const t={sidebar_position:3},l="Visual SLAM with Isaac ROS",o={},c=[{value:"Chapter Objectives",id:"chapter-objectives",level:2},{value:"Visual SLAM Fundamentals",id:"visual-slam-fundamentals",level:2},{value:"What is Visual SLAM?",id:"what-is-visual-slam",level:3},{value:"Visual SLAM Components",id:"visual-slam-components",level:3},{value:"Why GPU-Accelerated Visual SLAM?",id:"why-gpu-accelerated-visual-slam",level:3},{value:"Isaac ROS Visual SLAM Architecture",id:"isaac-ros-visual-slam-architecture",level:2},{value:"Isaac ROS Visual SLAM Package",id:"isaac-ros-visual-slam-package",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Setting Up Visual SLAM with Isaac ROS",id:"setting-up-visual-slam-with-isaac-ros",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Camera Calibration",id:"camera-calibration",level:3},{value:"Launching Isaac ROS Visual SLAM",id:"launching-isaac-ros-visual-slam",level:3},{value:"Visual SLAM Implementation",id:"visual-slam-implementation",level:2},{value:"Basic Visual SLAM Node",id:"basic-visual-slam-node",level:3},{value:"Isaac ROS Visual SLAM Configuration",id:"isaac-ros-visual-slam-configuration",level:2},{value:"Parameter Tuning",id:"parameter-tuning",level:3},{value:"Sensor Configuration",id:"sensor-configuration",level:3},{value:"Sensor Fusion with IMU",id:"sensor-fusion-with-imu",level:2},{value:"IMU Integration Benefits",id:"imu-integration-benefits",level:3},{value:"IMU Configuration for SLAM",id:"imu-configuration-for-slam",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"SLAM Quality Metrics",id:"slam-quality-metrics",level:3},{value:"Real-World Applications",id:"real-world-applications",level:2},{value:"Indoor Navigation",id:"indoor-navigation",level:3},{value:"Object Tracking and Mapping",id:"object-tracking-and-mapping",level:3},{value:"Troubleshooting and Optimization",id:"troubleshooting-and-optimization",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Best Practices for Isaac ROS Visual SLAM",id:"best-practices-for-isaac-ros-visual-slam",level:2},{value:"Sensor Configuration",id:"sensor-configuration-1",level:3},{value:"Environmental Considerations",id:"environmental-considerations",level:3},{value:"Integration Strategies",id:"integration-strategies",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"visual-slam-with-isaac-ros",children:"Visual SLAM with Isaac ROS"})}),"\n",(0,s.jsx)(n.h2,{id:"chapter-objectives",children:"Chapter Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand Visual SLAM concepts and their importance in Physical AI"}),"\n",(0,s.jsx)(n.li,{children:"Implement GPU-accelerated Visual SLAM using Isaac ROS"}),"\n",(0,s.jsx)(n.li,{children:"Configure camera and IMU sensors for SLAM applications"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate SLAM performance and accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Integrate SLAM with navigation systems for Physical AI applications"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"visual-slam-fundamentals",children:"Visual SLAM Fundamentals"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-visual-slam",children:"What is Visual SLAM?"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) is a technique that allows a robot to build a map of an unknown environment while simultaneously tracking its position within that map, using only visual sensors like cameras. This is crucial for Physical AI systems that need to navigate and interact with the physical world without prior knowledge of their surroundings."}),"\n",(0,s.jsx)(n.h3,{id:"visual-slam-components",children:"Visual SLAM Components"}),"\n",(0,s.jsx)(n.p,{children:"A typical Visual SLAM system consists of:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Detection"}),": Identifying distinctive points in images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Tracking"}),": Following features across image sequences"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose Estimation"}),": Calculating camera motion between frames"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mapping"}),": Building a 3D representation of the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loop Closure"}),": Recognizing previously visited locations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimization"}),": Refining map and trajectory estimates"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"why-gpu-accelerated-visual-slam",children:"Why GPU-Accelerated Visual SLAM?"}),"\n",(0,s.jsx)(n.p,{children:"Traditional CPU-based SLAM systems often struggle with real-time performance, especially in complex environments. GPU acceleration provides:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallel Processing"}),": GPUs can process thousands of pixels simultaneously"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Extraction"}),": Fast computation of descriptors and matches"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimization"}),": Efficient bundle adjustment and graph optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Performance"}),": Maintaining high frame rates for navigation"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-visual-slam-architecture",children:"Isaac ROS Visual SLAM Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-visual-slam-package",children:"Isaac ROS Visual SLAM Package"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"isaac_ros_visual_slam"})," package provides GPU-accelerated Visual SLAM capabilities:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Camera        \u2502    \u2502 Isaac ROS       \u2502    \u2502   SLAM Map      \u2502\n\u2502   Images        \u2502\u2500\u2500\u2500\u25b6\u2502   Visual SLAM   \u2502\u2500\u2500\u2500\u25b6\u2502   (ROS TF,     \u2502\n\u2502                 \u2502    \u2502   (GPU)         \u2502    \u2502   Occupancy     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   Grid, etc.)   \u2502\n                                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Preprocessing"}),": GPU-accelerated image rectification and undistortion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Detection"}),": CUDA-accelerated FAST/ORB feature detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Matching"}),": GPU-based descriptor matching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose Estimation"}),": GPU-accelerated PnP and motion estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Map Optimization"}),": Bundle adjustment and loop closure on GPU"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS Interface"}),": Standard ROS 2 message types and TF publishing"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"setting-up-visual-slam-with-isaac-ros",children:"Setting Up Visual SLAM with Isaac ROS"}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before implementing Visual SLAM, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"NVIDIA GPU with CUDA support"}),"\n",(0,s.jsx)(n.li,{children:"Isaac ROS Visual SLAM package installed"}),"\n",(0,s.jsx)(n.li,{children:"Camera calibrated with intrinsic parameters"}),"\n",(0,s.jsx)(n.li,{children:"Optional: IMU for sensor fusion (recommended for better accuracy)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM requires accurate camera calibration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Using Isaac ROS camera calibration tools\nros2 run isaac_ros_apriltag_calibrator calibrator_node \\\n  --ros-args -p image_width:=640 -p image_height:=480 \\\n  -p calibration_board_size:=10 \\\n  -p target_frame_name:=camera_link\n"})}),"\n",(0,s.jsx)(n.p,{children:"Or use standard ROS camera calibration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Standard ROS camera calibration\nros2 run camera_calibration cameracalibrator \\\n  --size 8x6 --square 0.025 \\\n  image:=/camera/image_raw \\\n  camera:=/camera\n"})}),"\n",(0,s.jsx)(n.h3,{id:"launching-isaac-ros-visual-slam",children:"Launching Isaac ROS Visual SLAM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# File: config/visual_slam_params.yaml\nisaac_ros_visual_slam:\n  ros__parameters:\n    # Input topics\n    camera_qos: 10\n    imu_qos: 10\n\n    # Feature detection parameters\n    enable_debug_mode: false\n    enable_imu_fusion: true\n    imu_queue_size: 10\n\n    # Map parameters\n    map_frame: "map"\n    tracking_frame: "camera_link"\n    publish_odom_tf: true\n\n    # Loop closure parameters\n    enable_localization: false\n    enable_mapping: true\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: launch/visual_slam.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config_file = os.path.join(\n        get_package_share_directory('your_package'),\n        'config',\n        'visual_slam_params.yaml'\n    )\n\n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='visual_slam_node',\n        parameters=[config_file],\n        remappings=[\n            ('/visual_slam/image_raw', '/camera/image_rect_color'),\n            ('/visual_slam/camera_info', '/camera/camera_info'),\n            ('/visual_slam/imu', '/imu/data')\n        ]\n    )\n\n    return LaunchDescription([\n        visual_slam_node\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"visual-slam-implementation",children:"Visual SLAM Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"basic-visual-slam-node",children:"Basic Visual SLAM Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: visual_slam/visual_slam_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, Imu\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import TransformStamped\nfrom tf2_ros import TransformBroadcaster\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacVisualSLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_visual_slam_node\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # TF broadcaster for SLAM results\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_rect_color\',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Publishers\n        self.odom_pub = self.create_publisher(\n            Odometry,\n            \'/visual_slam/odometry\',\n            10\n        )\n\n        # SLAM state\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.previous_features = None\n        self.current_pose = np.eye(4)  # 4x4 transformation matrix\n\n        self.get_logger().info(\'Isaac Visual SLAM Node Started\')\n\n    def camera_info_callback(self, msg):\n        """Process camera calibration information"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n        self.get_logger().info(\'Camera calibration received\')\n\n    def imu_callback(self, msg):\n        """Process IMU data for sensor fusion"""\n        # In real implementation, this would be used by Isaac ROS Visual SLAM\n        # This is just a placeholder for understanding the data flow\n        pass\n\n    def image_callback(self, msg):\n        """Process incoming images for SLAM"""\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # In a real implementation, we would use Isaac ROS Visual SLAM\n            # which handles feature detection, tracking, and pose estimation\n            # on the GPU\n\n            # For demonstration, we\'ll show how to extract features\n            processed_features = self.extract_features(cv_image)\n\n            # Publish SLAM results (in real implementation, this comes from Isaac ROS)\n            self.publish_slam_results()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def extract_features(self, image):\n        """Extract features using GPU-accelerated methods"""\n        # This would typically be handled by Isaac ROS Visual SLAM\n        # Here\'s a simplified example using OpenCV\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Use ORB detector (CPU version - Isaac ROS uses GPU)\n        orb = cv2.ORB_create(nfeatures=1000)\n        keypoints, descriptors = orb.detectAndCompute(gray, None)\n\n        return keypoints, descriptors\n\n    def publish_slam_results(self):\n        """Publish SLAM results (odometry and transforms)"""\n        # Create odometry message\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \'map\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Set position (this would come from actual SLAM)\n        odom_msg.pose.pose.position.x = self.current_pose[0, 3]\n        odom_msg.pose.pose.position.y = self.current_pose[1, 3]\n        odom_msg.pose.pose.position.z = self.current_pose[2, 3]\n\n        # Set orientation (convert rotation matrix to quaternion)\n        # In real implementation, Isaac ROS handles this\n        odom_msg.pose.pose.orientation.w = 1.0  # Placeholder\n\n        # Publish odometry\n        self.odom_pub.publish(odom_msg)\n\n        # Broadcast transform\n        t = TransformStamped()\n        t.header.stamp = odom_msg.header.stamp\n        t.header.frame_id = \'map\'\n        t.child_frame_id = \'base_link\'\n\n        t.transform.translation.x = self.current_pose[0, 3]\n        t.transform.translation.y = self.current_pose[1, 3]\n        t.transform.translation.z = self.current_pose[2, 3]\n        t.transform.rotation.w = 1.0  # Placeholder\n\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacVisualSLAMNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-visual-slam-configuration",children:"Isaac ROS Visual SLAM Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"parameter-tuning",children:"Parameter Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# Advanced Visual SLAM parameters\nisaac_ros_visual_slam:\n  ros__parameters:\n    # Performance parameters\n    enable_debug_mode: false\n    enable_profiler: false\n\n    # Feature parameters\n    feature_detector_type: "ORB"  # Options: ORB, FAST\n    descriptor_type: "BRIEF"      # Options: BRIEF, ORB\n    max_features: 1000\n    min_features: 100\n\n    # Tracking parameters\n    tracking_rate_hz: 30.0\n    min_translation_m: 0.05       # Minimum translation to trigger new keyframe\n    min_rotation_deg: 5.0         # Minimum rotation to trigger new keyframe\n\n    # Mapping parameters\n    map_frame: "map"\n    tracking_frame: "camera_link"\n    publish_odom_tf: true\n    publish_map_tf: true\n\n    # Loop closure parameters\n    enable_localization: false\n    enable_mapping: true\n    loop_closure_detection: true\n    loop_closure_min_inliers: 10\n    loop_closure_reproj_threshold: 3.0\n\n    # IMU fusion parameters\n    enable_imu_fusion: true\n    imu_queue_size: 10\n    imu_rate_hz: 100.0\n'})}),"\n",(0,s.jsx)(n.h3,{id:"sensor-configuration",children:"Sensor Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# Camera configuration for Visual SLAM\ncamera_info:\n  width: 640\n  height: 480\n  camera_matrix:\n    rows: 3\n    cols: 3\n    data: [615.166, 0.0, 320.5, 0.0, 615.166, 240.5, 0.0, 0.0, 1.0]\n  distortion_model: "plumb_bob"\n  distortion_coefficients:\n    rows: 1\n    cols: 5\n    data: [0.0, 0.0, 0.0, 0.0, 0.0]\n  rectification_matrix:\n    rows: 3\n    cols: 3\n    data: [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n  projection_matrix:\n    rows: 3\n    cols: 4\n    data: [615.166, 0.0, 320.5, 0.0, 0.0, 615.166, 240.5, 0.0, 0.0, 0.0, 1.0, 0.0]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion-with-imu",children:"Sensor Fusion with IMU"}),"\n",(0,s.jsx)(n.h3,{id:"imu-integration-benefits",children:"IMU Integration Benefits"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM can be enhanced with IMU data for better accuracy and robustness:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Motion Prediction"}),": IMU provides motion estimates between frames"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale Recovery"}),": IMU helps with scale estimation in monocular SLAM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Drift Reduction"}),": Accelerometer and gyroscope reduce drift"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": IMU provides backup when visual features are lacking"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"imu-configuration-for-slam",children:"IMU Configuration for SLAM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: IMU preprocessing for SLAM\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass IMUPreprocessor:\n    def __init__(self):\n        self.gravity = np.array([0, 0, 9.81])\n        self.orientation = R.from_quat([0, 0, 0, 1])\n        self.linear_velocity = np.zeros(3)\n        self.position = np.zeros(3)\n\n    def process_imu(self, imu_msg):\n        \"\"\"Process IMU data for SLAM integration\"\"\"\n        # Extract measurements\n        angular_velocity = np.array([\n            imu_msg.angular_velocity.x,\n            imu_msg.angular_velocity.y,\n            imu_msg.angular_velocity.z\n        ])\n\n        linear_acceleration = np.array([\n            imu_msg.linear_acceleration.x,\n            imu_msg.linear_acceleration.y,\n            imu_msg.linear_acceleration.z\n        ])\n\n        # Remove gravity from linear acceleration\n        gravity_aligned = self.orientation.apply(self.gravity)\n        linear_acceleration_body = linear_acceleration - gravity_aligned\n\n        # Integrate to get velocity and position\n        dt = 1.0 / 100.0  # Assuming 100Hz IMU\n        self.linear_velocity += linear_acceleration_body * dt\n        self.position += self.linear_velocity * dt\n\n        return {\n            'angular_velocity': angular_velocity,\n            'linear_acceleration': linear_acceleration_body,\n            'position': self.position,\n            'velocity': self.linear_velocity\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,s.jsx)(n.h3,{id:"slam-quality-metrics",children:"SLAM Quality Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: visual_slam/slam_evaluator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, TransformStamped\nfrom std_msgs.msg import Float32\nfrom tf2_ros import Buffer, TransformListener\nimport numpy as np\nfrom collections import deque\n\nclass SLAMEvaluator(Node):\n    def __init__(self):\n        super().__init__('slam_evaluator')\n\n        # TF buffer for pose comparison\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Publishers for metrics\n        self.accuracy_pub = self.create_publisher(Float32, '/slam/accuracy', 10)\n        self.stability_pub = self.create_publisher(Float32, '/slam/stability', 10)\n        self.fps_pub = self.create_publisher(Float32, '/slam/fps', 10)\n\n        # Performance tracking\n        self.position_history = deque(maxlen=100)\n        self.fps_history = deque(maxlen=30)\n        self.last_time = self.get_clock().now()\n\n        # Timer for evaluation\n        self.eval_timer = self.create_timer(1.0, self.evaluate_performance)\n\n        self.get_logger().info('SLAM Evaluator Node Started')\n\n    def evaluate_performance(self):\n        \"\"\"Evaluate SLAM performance metrics\"\"\"\n\n        # Calculate FPS\n        current_time = self.get_clock().now()\n        dt = (current_time - self.last_time).nanoseconds / 1e9\n        fps = 1.0 / max(dt, 0.001)  # Avoid division by zero\n        self.fps_history.append(fps)\n        self.last_time = current_time\n\n        # Calculate stability (variance in position over time)\n        if len(self.position_history) > 10:\n            positions = np.array(self.position_history)\n            stability = 1.0 / (1.0 + np.std(positions, axis=0).mean())\n        else:\n            stability = 1.0\n\n        # Calculate accuracy (placeholder - would need ground truth)\n        accuracy = self.calculate_accuracy()\n\n        # Publish metrics\n        accuracy_msg = Float32()\n        accuracy_msg.data = accuracy\n        self.accuracy_pub.publish(accuracy_msg)\n\n        stability_msg = Float32()\n        stability_msg.data = stability\n        self.stability_pub.publish(stability_msg)\n\n        fps_msg = Float32()\n        fps_msg.data = np.mean(self.fps_history) if self.fps_history else 0.0\n        self.fps_pub.publish(fps_msg)\n\n        self.get_logger().info(\n            f'SLAM Metrics - FPS: {fps:.1f}, Accuracy: {accuracy:.3f}, '\n            f'Stability: {stability:.3f}'\n        )\n\n    def calculate_accuracy(self):\n        \"\"\"Calculate accuracy metric (would use ground truth in real scenario)\"\"\"\n        # In a real evaluation, this would compare against ground truth\n        # For simulation, we could use ground truth from Isaac Sim\n        return 0.8  # Placeholder value\n\ndef main(args=None):\n    rclpy.init(args=args)\n    evaluator = SLAMEvaluator()\n\n    try:\n        rclpy.spin(evaluator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        evaluator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,s.jsx)(n.h3,{id:"indoor-navigation",children:"Indoor Navigation"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM enables robots to navigate indoor environments without prior maps:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example: SLAM-based navigation\nclass SLAMNavigation:\n    def __init__(self):\n        self.map = None\n        self.current_pose = None\n        self.path_planner = None\n\n    def navigate_with_slam(self, goal_position):\n        """Navigate to goal using SLAM-generated map"""\n        # Wait for SLAM to build sufficient map\n        if self.map is None or len(self.map.keyframes) < 10:\n            return False, "Insufficient map coverage"\n\n        # Plan path using current map\n        path = self.path_planner.plan_path(\n            start=self.current_pose,\n            goal=goal_position,\n            map=self.map\n        )\n\n        if path is None:\n            return False, "No path found"\n\n        # Execute path following\n        return self.follow_path(path)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"object-tracking-and-mapping",children:"Object Tracking and Mapping"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM can simultaneously track objects and build maps:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example: Object-aware SLAM\nclass ObjectSLAM:\n    def __init__(self):\n        self.object_detector = None  # Object detection model\n        self.tracked_objects = {}\n\n    def process_frame_with_objects(self, image, camera_info):\n        \"\"\"Process frame for both SLAM and object tracking\"\"\"\n        # Extract SLAM features\n        slam_features = self.extract_slam_features(image)\n\n        # Detect objects\n        detections = self.object_detector.detect(image)\n\n        # Associate objects with map points\n        for detection in detections:\n            object_id = detection.id\n            if object_id not in self.tracked_objects:\n                self.tracked_objects[object_id] = {\n                    'trajectory': [],\n                    'class': detection.class_name\n                }\n\n            # Add current position to object trajectory\n            current_position = self.estimate_object_position(\n                detection.bbox, camera_info\n            )\n            self.tracked_objects[object_id]['trajectory'].append(\n                current_position\n            )\n\n        # Continue with normal SLAM processing\n        return self.process_slam_features(slam_features)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-and-optimization",children:"Troubleshooting and Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Poor Environments"}),": Use more robust feature detectors or add artificial markers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Drift Accumulation"}),": Enable loop closure and optimize parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Performance"}),": Reduce feature count or optimize GPU usage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale Ambiguity"}),": Use stereo cameras or IMU fusion"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example: Adaptive SLAM parameters\nclass AdaptiveSLAM:\n    def __init__(self):\n        self.feature_count = 1000\n        self.min_features = 100\n        self.max_features = 2000\n\n    def adjust_parameters(self, tracking_quality):\n        """Adjust SLAM parameters based on tracking quality"""\n        if tracking_quality < 0.3:  # Poor tracking\n            # Increase features\n            self.feature_count = min(\n                self.feature_count + 200,\n                self.max_features\n            )\n        elif tracking_quality > 0.8:  # Good tracking\n            # Reduce features to save computation\n            self.feature_count = max(\n                self.feature_count - 100,\n                self.min_features\n            )\n\n        # Apply new parameters\n        self.update_feature_detector(self.feature_count)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-for-isaac-ros-visual-slam",children:"Best Practices for Isaac ROS Visual SLAM"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-configuration-1",children:"Sensor Configuration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use calibrated cameras with accurate intrinsic parameters"}),"\n",(0,s.jsx)(n.li,{children:"Ensure proper lighting conditions for feature detection"}),"\n",(0,s.jsx)(n.li,{children:"Consider stereo cameras for better depth estimation"}),"\n",(0,s.jsx)(n.li,{children:"Use high frame rate cameras (30+ FPS) for smooth tracking"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"environmental-considerations",children:"Environmental Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure textured environments for feature-rich scenes"}),"\n",(0,s.jsx)(n.li,{children:"Avoid repetitive patterns that cause confusion"}),"\n",(0,s.jsx)(n.li,{children:"Consider dynamic objects that may affect tracking"}),"\n",(0,s.jsx)(n.li,{children:"Plan for varying lighting conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"integration-strategies",children:"Integration Strategies"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Start with simple environments and gradually increase complexity"}),"\n",(0,s.jsx)(n.li,{children:"Validate SLAM results against ground truth when possible"}),"\n",(0,s.jsx)(n.li,{children:"Monitor performance metrics continuously"}),"\n",(0,s.jsx)(n.li,{children:"Implement fallback strategies for SLAM failure"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM is a fundamental capability for Physical AI systems, enabling robots to navigate and map unknown environments using only visual sensors. Isaac ROS provides GPU-accelerated Visual SLAM that delivers real-time performance for demanding Physical AI applications. Proper configuration of cameras, IMU sensors, and SLAM parameters is essential for achieving robust and accurate localization and mapping."}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure and run Isaac ROS Visual SLAM with a camera in Isaac Sim."}),"\n",(0,s.jsx)(n.li,{children:"Evaluate SLAM performance metrics in different simulated environments."}),"\n",(0,s.jsx)(n.li,{children:"Integrate SLAM with a basic navigation system for path planning."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"In the next chapter, we'll explore the Navigation Stack (Nav2) integration with Isaac, learning how to implement autonomous navigation for Physical AI systems using GPU-accelerated algorithms."})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>t,x:()=>l});var i=a(6540);const s={},r=i.createContext(s);function t(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);