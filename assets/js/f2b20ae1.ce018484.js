"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5128],{4810:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module4-vla/capstone","title":"CAPSTONE: Autonomous Humanoid Assistant","description":"Project Objectives","source":"@site/docs/module4-vla/05-capstone.md","sourceDirName":"module4-vla","slug":"/module4-vla/capstone","permalink":"/physical-ai-book/docs/module4-vla/capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module4-vla/05-capstone.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Multimodal Integration","permalink":"/physical-ai-book/docs/module4-vla/multimodal-integration"},"next":{"title":"Advanced Topics in VLA Systems","permalink":"/physical-ai-book/docs/module4-vla/advanced-topics"}}');var i=t(4848),o=t(8453);const a={sidebar_position:5},r="CAPSTONE: Autonomous Humanoid Assistant",c={},l=[{value:"Project Objectives",id:"project-objectives",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Create the Project Package",id:"step-1-create-the-project-package",level:3},{value:"Step 2: Install Dependencies",id:"step-2-install-dependencies",level:3},{value:"Step 3: Main System Node",id:"step-3-main-system-node",level:3},{value:"Step 4: Voice Processing Component",id:"step-4-voice-processing-component",level:3},{value:"Step 5: Vision Processing Component",id:"step-5-vision-processing-component",level:3},{value:"Step 6: Main Launch File",id:"step-6-main-launch-file",level:3},{value:"Step 7: Configuration Files",id:"step-7-configuration-files",level:3},{value:"Step 8: Package Configuration",id:"step-8-package-configuration",level:3},{value:"Step 9: Running the System",id:"step-9-running-the-system",level:3},{value:"Physical AI Concepts Demonstrated",id:"physical-ai-concepts-demonstrated",level:2},{value:"1. Multimodal Integration",id:"1-multimodal-integration",level:3},{value:"2. Cognitive Reasoning",id:"2-cognitive-reasoning",level:3},{value:"3. Real-time Processing",id:"3-real-time-processing",level:3},{value:"4. Human-Robot Interaction",id:"4-human-robot-interaction",level:3},{value:"Advanced Features Implementation",id:"advanced-features-implementation",level:2},{value:"Context Management",id:"context-management",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Efficiency",id:"caching-and-efficiency",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-autonomous-humanoid-assistant",children:"CAPSTONE: Autonomous Humanoid Assistant"})}),"\n",(0,i.jsx)(n.h2,{id:"project-objectives",children:"Project Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By completing this CAPSTONE project, you will:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate all VLA (Vision-Language-Action) concepts into a complete Physical AI system"}),"\n",(0,i.jsx)(n.li,{children:"Build an autonomous humanoid assistant capable of natural interaction"}),"\n",(0,i.jsx)(n.li,{children:"Implement multimodal perception, reasoning, and action execution"}),"\n",(0,i.jsx)(n.li,{children:"Demonstrate advanced Physical AI capabilities in simulation and/or reality"}),"\n",(0,i.jsx)(n.li,{children:"Validate the complete pipeline from voice commands to physical actions"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.p,{children:"In this CAPSTONE project, we'll build an autonomous humanoid assistant that can:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Understand natural language voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Perceive and understand its environment through vision"}),"\n",(0,i.jsx)(n.li,{children:"Reason about complex tasks and their execution"}),"\n",(0,i.jsx)(n.li,{children:"Navigate and manipulate objects in physical space"}),"\n",(0,i.jsx)(n.li,{children:"Provide natural feedback and maintain conversation context"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Voice Input   \u2502    \u2502  Vision Input   \u2502    \u2502   LLM Reasoning \u2502\n\u2502   (Speech Rec)  \u2502    \u2502  (Cameras,      \u2502    \u2502   (GPT-4,       \u2502\n\u2502                 \u2502    \u2502  Perception)     \u2502    \u2502   Cognitive     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   Planning)     \u2502\n         \u2502                       \u2502             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Multimodal     \u2502\n                    \u2502  Fusion &       \u2502\n                    \u2502  Decision Making \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Action Planner \u2502\n                    \u2502  & Executor     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Robot Control  \u2502\n                    \u2502  (Navigation,   \u2502\n                    \u2502   Manipulation) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-create-the-project-package",children:"Step 1: Create the Project Package"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create project workspace\nmkdir -p ~/humanoid_assistant_ws/src\ncd ~/humanoid_assistant_ws/src\n\n# Create humanoid assistant package\nros2 pkg create --build-type ament_python humanoid_assistant\ncd humanoid_assistant\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-install-dependencies",children:"Step 2: Install Dependencies"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install required packages\npip3 install openai transformers torch torchvision torchaudio\npip3 install speechrecognition pyaudio vosk\npip3 install opencv-python numpy scipy\npip3 install webrtcvad\n\n# Install ROS2 packages\nsudo apt update\nsudo apt install ros-humble-navigation2 ros-humble-nav2-bringup\nsudo apt install ros-humble-isaac-ros-*  # If using Isaac\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-main-system-node",children:"Step 3: Main System Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: humanoid_assistant/humanoid_assistant/main_system.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom vision_msgs.msg import Detection2DArray\nimport json\nimport threading\nimport time\nfrom typing import Dict, Any, Optional\n\nclass HumanoidAssistantNode(Node):\n    def __init__(self):\n        super().__init__('humanoid_assistant_node')\n\n        # Initialize components\n        self.voice_processor = VoiceProcessor(self)\n        self.vision_processor = VisionProcessor(self)\n        self.reasoning_engine = ReasoningEngine(self)\n        self.action_executor = ActionExecutor(self)\n\n        # State management\n        self.system_state = {\n            'current_task': None,\n            'task_status': 'idle',\n            'conversation_context': [],\n            'environment_map': {},\n            'last_interaction': time.time()\n        }\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, '/assistant/status', 10)\n        self.feedback_pub = self.create_publisher(String, '/assistant/feedback', 10)\n\n        # Subscribers\n        self.voice_cmd_sub = self.create_subscription(\n            String, '/voice/command', self.voice_command_callback, 10)\n        self.vision_data_sub = self.create_subscription(\n            Detection2DArray, '/object/detections', self.vision_callback, 10)\n        self.nav_status_sub = self.create_subscription(\n            String, '/navigation/status', self.navigation_callback, 10)\n\n        # Timer for system monitoring\n        self.monitor_timer = self.create_timer(1.0, self.system_monitor)\n\n        self.get_logger().info('Humanoid Assistant System Started')\n\n    def voice_command_callback(self, msg):\n        \"\"\"Process voice command through the complete pipeline\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received voice command: {command}')\n\n        # Add to conversation context\n        self.system_state['conversation_context'].append({\n            'type': 'user_input',\n            'content': command,\n            'timestamp': time.time()\n        })\n\n        # Process through VLA pipeline\n        self.process_voice_command(command)\n\n    def vision_callback(self, msg):\n        \"\"\"Process vision data\"\"\"\n        # Update environment map with detected objects\n        objects = []\n        for detection in msg.detections:\n            if detection.results:\n                obj_info = {\n                    'class': detection.results[0].hypothesis.class_id,\n                    'confidence': detection.results[0].hypothesis.score,\n                    'bbox': {\n                        'x': detection.bbox.center.x,\n                        'y': detection.bbox.center.y,\n                        'width': detection.bbox.size_x,\n                        'height': detection.bbox.size_y\n                    }\n                }\n                objects.append(obj_info)\n\n        self.system_state['environment_map']['objects'] = objects\n\n    def navigation_callback(self, msg):\n        \"\"\"Process navigation status\"\"\"\n        try:\n            nav_status = json.loads(msg.data)\n            self.system_state['navigation_status'] = nav_status\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in navigation status')\n\n    def process_voice_command(self, command: str):\n        \"\"\"Process voice command through complete VLA pipeline\"\"\"\n        self.system_state['task_status'] = 'processing'\n\n        # 1. Natural Language Understanding\n        self.get_logger().info('Understanding command...')\n        parsed_command = self.reasoning_engine.parse_command(command)\n\n        # 2. Context Integration\n        self.get_logger().info('Integrating context...')\n        contextual_command = self.reasoning_engine.integrate_context(\n            parsed_command, self.system_state\n        )\n\n        # 3. Task Planning\n        self.get_logger().info('Planning task...')\n        action_plan = self.reasoning_engine.plan_task(contextual_command)\n\n        # 4. Action Execution\n        self.get_logger().info('Executing plan...')\n        self.system_state['current_task'] = contextual_command\n        self.system_state['task_status'] = 'executing'\n\n        execution_result = self.action_executor.execute_plan(action_plan)\n\n        # 5. Feedback Generation\n        self.get_logger().info('Generating feedback...')\n        feedback = self.reasoning_engine.generate_feedback(\n            contextual_command, execution_result\n        )\n\n        # Publish feedback\n        feedback_msg = String()\n        feedback_msg.data = feedback\n        self.feedback_pub.publish(feedback_msg)\n\n        # Update system state\n        self.system_state['task_status'] = 'completed'\n        self.system_state['last_interaction'] = time.time()\n\n        self.get_logger().info(f'Task completed. Feedback: {feedback}')\n\n    def system_monitor(self):\n        \"\"\"Monitor system status and publish updates\"\"\"\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'state': self.system_state['task_status'],\n            'current_task': self.system_state['current_task'],\n            'last_interaction': self.system_state['last_interaction'],\n            'active_components': {\n                'voice': self.voice_processor.is_active(),\n                'vision': self.vision_processor.is_active(),\n                'reasoning': self.reasoning_engine.is_active(),\n                'action': self.action_executor.is_active()\n            }\n        })\n        self.status_pub.publish(status_msg)\n\nclass VoiceProcessor:\n    def __init__(self, node):\n        self.node = node\n        self.active = True\n\n    def is_active(self):\n        return self.active\n\nclass VisionProcessor:\n    def __init__(self, node):\n        self.node = node\n        self.active = True\n\n    def is_active(self):\n        return self.active\n\nclass ReasoningEngine:\n    def __init__(self, node):\n        self.node = node\n        self.active = True\n\n    def is_active(self):\n        return self.active\n\n    def parse_command(self, command: str) -> Dict[str, Any]:\n        \"\"\"Parse natural language command\"\"\"\n        # In real implementation, this would use NLP/LLM processing\n        return {\n            'original': command,\n            'intent': self.classify_intent(command),\n            'entities': self.extract_entities(command),\n            'action_type': self.determine_action_type(command)\n        }\n\n    def classify_intent(self, command: str) -> str:\n        \"\"\"Classify the intent of the command\"\"\"\n        command_lower = command.lower()\n\n        if any(word in command_lower for word in ['go to', 'navigate', 'move to', 'bring me']):\n            return 'navigation'\n        elif any(word in command_lower for word in ['pick', 'grasp', 'take', 'get']):\n            return 'manipulation'\n        elif any(word in command_lower for word in ['find', 'look', 'see', 'where']):\n            return 'perception'\n        else:\n            return 'communication'\n\n    def extract_entities(self, command: str) -> Dict[str, str]:\n        \"\"\"Extract entities from command\"\"\"\n        # Simple entity extraction\n        entities = {}\n        words = command.lower().split()\n\n        # Look for objects\n        object_keywords = ['cup', 'bottle', 'book', 'phone', 'laptop', 'box']\n        for word in words:\n            if word in object_keywords:\n                entities['object'] = word\n                break\n\n        # Look for locations\n        location_keywords = ['kitchen', 'living room', 'bedroom', 'office', 'table', 'chair']\n        for word in words:\n            if word in location_keywords:\n                entities['location'] = word\n                break\n\n        return entities\n\n    def determine_action_type(self, command: str) -> str:\n        \"\"\"Determine the primary action type\"\"\"\n        intent = self.classify_intent(command)\n\n        if intent == 'navigation':\n            if 'bring me' in command.lower():\n                return 'fetch_object'\n            else:\n                return 'navigate'\n        elif intent == 'manipulation':\n            return 'manipulate_object'\n        elif intent == 'perception':\n            return 'perceive_environment'\n        else:\n            return 'communicate'\n\n    def integrate_context(self, parsed_command: Dict[str, Any], system_state: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Integrate command with system context\"\"\"\n        contextual_command = parsed_command.copy()\n        contextual_command['context'] = {\n            'environment': system_state.get('environment_map', {}),\n            'previous_interactions': system_state['conversation_context'][-3:],  # Last 3 interactions\n            'robot_capabilities': ['navigation', 'basic_manipulation', 'voice_feedback'],\n            'current_pose': system_state.get('current_pose')\n        }\n        return contextual_command\n\n    def plan_task(self, contextual_command: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Plan the task based on contextual command\"\"\"\n        intent = contextual_command['intent']\n\n        if intent == 'navigation':\n            if contextual_command['action_type'] == 'fetch_object':\n                # Plan: navigate to object -> pick up -> navigate to user -> deliver\n                plan = {\n                    'task_type': 'fetch_and_deliver',\n                    'steps': [\n                        {\n                            'action': 'locate_object',\n                            'parameters': contextual_command['entities'],\n                            'description': 'Locate the requested object'\n                        },\n                        {\n                            'action': 'navigate_to_object',\n                            'parameters': contextual_command['entities'],\n                            'description': 'Navigate to the object location'\n                        },\n                        {\n                            'action': 'grasp_object',\n                            'parameters': contextual_command['entities'],\n                            'description': 'Grasp the object'\n                        },\n                        {\n                            'action': 'navigate_to_user',\n                            'parameters': {},\n                            'description': 'Navigate back to the user'\n                        },\n                        {\n                            'action': 'deliver_object',\n                            'parameters': {},\n                            'description': 'Deliver the object to the user'\n                        }\n                    ]\n                }\n            else:\n                # Simple navigation task\n                plan = {\n                    'task_type': 'navigation',\n                    'steps': [\n                        {\n                            'action': 'navigate',\n                            'parameters': contextual_command['entities'],\n                            'description': f'Navigate to {contextual_command[\"entities\"].get(\"location\", \"destination\")}'\n                        }\n                    ]\n                }\n        elif intent == 'manipulation':\n            plan = {\n                'task_type': 'manipulation',\n                'steps': [\n                    {\n                        'action': 'locate_object',\n                        'parameters': contextual_command['entities'],\n                        'description': f'Locate the {contextual_command[\"entities\"].get(\"object\", \"object\")}'\n                    },\n                    {\n                        'action': 'manipulate_object',\n                        'parameters': contextual_command['entities'],\n                        'description': f'Manipulate the {contextual_command[\"entities\"].get(\"object\", \"object\")}'\n                    }\n                ]\n            }\n        else:\n            # Default communication task\n            plan = {\n                'task_type': 'communication',\n                'steps': [\n                    {\n                        'action': 'respond',\n                        'parameters': {'message': contextual_command['original']},\n                        'description': 'Provide appropriate response'\n                    }\n                ]\n            }\n\n        return plan\n\n    def generate_feedback(self, contextual_command: Dict[str, Any], execution_result: Dict[str, Any]) -> str:\n        \"\"\"Generate natural language feedback\"\"\"\n        if execution_result.get('success', False):\n            if contextual_command['action_type'] == 'fetch_object':\n                return \"I've successfully brought you the item you requested.\"\n            elif contextual_command['action_type'] == 'navigate':\n                return \"I've reached the destination successfully.\"\n            else:\n                return \"Task completed successfully.\"\n        else:\n            error_msg = execution_result.get('error', 'Unknown error occurred')\n            return f\"I'm sorry, but I encountered an issue: {error_msg}. Could you please rephrase your request?\"\n\nclass ActionExecutor:\n    def __init__(self, node):\n        self.node = node\n        self.active = True\n\n        # Create publishers for different action types\n        self.nav_goal_pub = node.create_publisher(PoseStamped, '/goal_pose', 10)\n        self.cmd_vel_pub = node.create_publisher(Twist, '/cmd_vel', 10)\n        self.manipulation_pub = node.create_publisher(String, '/manipulation/command', 10)\n\n    def is_active(self):\n        return self.active\n\n    def execute_plan(self, plan: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute the planned task\"\"\"\n        success = True\n        error_msg = None\n\n        try:\n            for step in plan['steps']:\n                step_success = self.execute_step(step)\n                if not step_success:\n                    success = False\n                    error_msg = f\"Failed at step: {step['description']}\"\n                    break\n\n        except Exception as e:\n            success = False\n            error_msg = str(e)\n\n        return {\n            'success': success,\n            'error': error_msg,\n            'completed_steps': len([s for s in plan['steps'] if self.execute_step(s, dry_run=True)]),\n            'total_steps': len(plan['steps'])\n        }\n\n    def execute_step(self, step: Dict[str, Any], dry_run: bool = False) -> bool:\n        \"\"\"Execute a single step\"\"\"\n        action = step['action']\n\n        if dry_run:\n            # For dry run, just check if action is valid\n            return action in ['locate_object', 'navigate_to_object', 'grasp_object',\n                             'navigate_to_user', 'deliver_object', 'navigate', 'manipulate_object', 'respond']\n\n        self.node.get_logger().info(f'Executing step: {step[\"description\"]}')\n\n        if action == 'navigate':\n            return self.execute_navigation(step['parameters'])\n        elif action == 'grasp_object':\n            return self.execute_grasp(step['parameters'])\n        elif action == 'locate_object':\n            return self.execute_perception(step['parameters'])\n        elif action == 'respond':\n            return self.execute_communication(step['parameters'])\n        else:\n            self.node.get_logger().warn(f'Unknown action: {action}')\n            return False\n\n    def execute_navigation(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute navigation action\"\"\"\n        # In real implementation, this would send navigation goals\n        # For simulation, we'll just return success\n        self.node.get_logger().info(f'Navigating to {params}')\n\n        # Publish a dummy navigation goal\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.node.get_clock().now().to_msg()\n        goal_msg.header.frame_id = 'map'\n        goal_msg.pose.position.x = 1.0  # Example coordinates\n        goal_msg.pose.position.y = 1.0\n        goal_msg.pose.orientation.w = 1.0\n\n        self.nav_goal_pub.publish(goal_msg)\n        return True\n\n    def execute_grasp(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute grasp action\"\"\"\n        # In real implementation, this would send manipulation commands\n        self.node.get_logger().info(f'Grasping object: {params}')\n\n        # Publish manipulation command\n        manip_msg = String()\n        manip_msg.data = json.dumps({\n            'action': 'grasp',\n            'object': params.get('object', 'unknown')\n        })\n        self.manipulation_pub.publish(manip_msg)\n        return True\n\n    def execute_perception(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute perception action\"\"\"\n        # In real implementation, this would trigger object detection\n        self.node.get_logger().info(f'Perceiving environment for: {params}')\n        return True\n\n    def execute_communication(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute communication action\"\"\"\n        # In real implementation, this would trigger speech synthesis\n        message = params.get('message', 'Hello')\n        self.node.get_logger().info(f'Communicating: {message}')\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidAssistantNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-voice-processing-component",children:"Step 4: Voice Processing Component"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: humanoid_assistant/humanoid_assistant/voice_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\n\nclass VoiceProcessorNode(Node):\n    def __init__(self):\n        super().__init__('voice_processor_node')\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Configure recognizer\n        self.recognizer.energy_threshold = 300\n        self.recognizer.dynamic_energy_threshold = True\n\n        # Audio processing\n        self.listening = False\n        self.audio_queue = queue.Queue()\n        self.command_queue = queue.Queue()\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n        self.listening_pub = self.create_publisher(Bool, '/voice/listening', 10)\n\n        # Subscribers\n        self.wake_word_sub = self.create_subscription(\n            Bool, '/voice/wake_word_detected', self.wake_word_callback, 10)\n\n        # Start audio processing thread\n        self.audio_thread = threading.Thread(target=self.audio_processing_loop, daemon=True)\n        self.audio_thread.start()\n\n        # Timer for voice activity\n        self.voice_timer = self.create_timer(0.5, self.check_voice_activity)\n\n        self.get_logger().info('Voice Processor Node Started')\n\n    def wake_word_callback(self, msg):\n        \"\"\"Handle wake word detection\"\"\"\n        if msg.data:\n            self.listening = True\n            self.get_logger().info('Listening activated by wake word')\n\n    def audio_processing_loop(self):\n        \"\"\"Main audio processing loop\"\"\"\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        while rclpy.ok():\n            if self.listening:\n                try:\n                    with self.microphone as source:\n                        # Listen for audio with timeout\n                        audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\n\n                    # Recognize speech\n                    try:\n                        command = self.recognizer.recognize_google(audio).lower()\n                        self.get_logger().info(f'Recognized: {command}')\n\n                        # Publish command\n                        cmd_msg = String()\n                        cmd_msg.data = command\n                        self.command_pub.publish(cmd_msg)\n\n                        # Deactivate listening after successful recognition\n                        self.listening = False\n\n                    except sr.UnknownValueError:\n                        self.get_logger().info('Could not understand audio')\n                    except sr.RequestError as e:\n                        self.get_logger().error(f'Speech recognition error: {e}')\n\n                except sr.WaitTimeoutError:\n                    # No speech detected, continue listening\n                    pass\n                except Exception as e:\n                    self.get_logger().error(f'Audio processing error: {e}')\n\n            time.sleep(0.1)  # Small delay to prevent busy waiting\n\n    def check_voice_activity(self):\n        \"\"\"Check and publish listening status\"\"\"\n        listening_msg = Bool()\n        listening_msg.data = self.listening\n        self.listening_pub.publish(listening_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceProcessorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-5-vision-processing-component",children:"Step 5: Vision Processing Component"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: humanoid_assistant/humanoid_assistant/vision_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom std_msgs.msg import String\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom typing import List, Dict, Any\n\nclass VisionProcessorNode(Node):\n    def __init__(self):\n        super().__init__('vision_processor_node')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers\n        self.detections_pub = self.create_publisher(Detection2DArray, '/object/detections', 10)\n        self.visualization_pub = self.create_publisher(Image, '/vision/visualization', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n\n        # State\n        self.current_image = None\n\n        self.get_logger().info('Vision Processor Node Started')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming images for object detection\"\"\"\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.current_image = cv_image\n\n            # Perform object detection (simplified - in real system would use actual detector)\n            detections = self.detect_objects(cv_image)\n\n            # Create detection array message\n            detection_array = Detection2DArray()\n            detection_array.header = msg.header\n\n            for detection in detections:\n                detection_msg = Detection2D()\n                detection_msg.header = msg.header\n                detection_msg.bbox.center.x = detection['bbox']['center_x']\n                detection_msg.bbox.center.y = detection['bbox']['center_y']\n                detection_msg.bbox.size_x = detection['bbox']['width']\n                detection_msg.bbox.size_y = detection['bbox']['height']\n\n                # Add result\n                from vision_msgs.msg import ObjectHypothesisWithPose\n                hypothesis = ObjectHypothesisWithPose()\n                hypothesis.hypothesis.class_id = detection['class']\n                hypothesis.hypothesis.score = detection['confidence']\n                detection_msg.results.append(hypothesis)\n\n                detection_array.detections.append(detection_msg)\n\n            # Publish detections\n            self.detections_pub.publish(detection_array)\n\n            # Create and publish visualization\n            viz_image = self.create_visualization(cv_image, detections)\n            viz_msg = self.cv_bridge.cv2_to_imgmsg(viz_image, \"bgr8\")\n            viz_msg.header = msg.header\n            self.visualization_pub.publish(viz_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Vision processing error: {e}')\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in image (simplified implementation)\"\"\"\n        # In a real system, this would use a trained object detection model\n        # For this example, we'll simulate detections\n\n        # Convert to grayscale for simple blob detection\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Apply threshold to find potential objects\n        _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n\n        # Find contours (potential objects)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        detections = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 500:  # Filter small contours\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n\n                # Determine object class based on shape/size\n                aspect_ratio = w / h\n                if 0.8 <= aspect_ratio <= 1.2:\n                    obj_class = \"object\"  # Could be cup, box, etc.\n                elif aspect_ratio > 1.5:\n                    obj_class = \"book\"  # Could be book, paper\n                else:\n                    obj_class = \"object\"\n\n                detection = {\n                    'class': obj_class,\n                    'confidence': min(0.9, area / 10000),  # Normalize confidence\n                    'bbox': {\n                        'center_x': x + w / 2,\n                        'center_y': y + h / 2,\n                        'width': w,\n                        'height': h\n                    }\n                }\n                detections.append(detection)\n\n        return detections\n\n    def create_visualization(self, image, detections):\n        \"\"\"Create visualization with bounding boxes\"\"\"\n        viz_image = image.copy()\n\n        for detection in detections:\n            bbox = detection['bbox']\n            x = int(bbox['center_x'] - bbox['width'] / 2)\n            y = int(bbox['center_y'] - bbox['height'] / 2)\n            w = int(bbox['width'])\n            h = int(bbox['height'])\n\n            # Draw bounding box\n            cv2.rectangle(viz_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n            # Add label\n            label = f\"{detection['class']}: {detection['confidence']:.2f}\"\n            cv2.putText(viz_image, label, (x, y - 10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        return viz_image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionProcessorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-6-main-launch-file",children:"Step 6: Main Launch File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: humanoid_assistant/launch/humanoid_assistant.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nimport os\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    enable_voice = LaunchConfiguration('enable_voice', default='true')\n    enable_vision = LaunchConfiguration('enable_vision', default='true')\n\n    # Main system node\n    main_system = Node(\n        package='humanoid_assistant',\n        executable='main_system',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Voice processing node\n    voice_processor = Node(\n        package='humanoid_assistant',\n        executable='voice_processor',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen',\n        condition=IfCondition(enable_voice)\n    )\n\n    # Vision processing node\n    vision_processor = Node(\n        package='humanoid_assistant',\n        executable='vision_processor',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen',\n        condition=IfCondition(enable_vision)\n    )\n\n    # Navigation stack (if using Nav2)\n    navigation = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time\n        }.items()\n    )\n\n    # RViz for visualization\n    rviz = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', PathJoinSubstitution([\n            FindPackageShare('humanoid_assistant'),\n            'rviz',\n            'assistant_view.rviz'\n        ])],\n        parameters=[{'use_sim_time': use_sim_time}],\n        condition=IfCondition(LaunchConfiguration('enable_rviz', default='true'))\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time'\n        ),\n        DeclareLaunchArgument(\n            'enable_voice',\n            default_value='true',\n            description='Enable voice processing'\n        ),\n        DeclareLaunchArgument(\n            'enable_vision',\n            default_value='true',\n            description='Enable vision processing'\n        ),\n        DeclareLaunchArgument(\n            'enable_rviz',\n            default_value='true',\n            description='Enable RViz visualization'\n        ),\n        main_system,\n        voice_processor,\n        vision_processor,\n        navigation,\n        rviz\n    ])\n\n# Add the missing imports\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\nfrom launch.actions import IncludeLaunchDescription\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-7-configuration-files",children:"Step 7: Configuration Files"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# File: humanoid_assistant/config/assistant_params.yaml\nhumanoid_assistant:\n  ros__parameters:\n    # Voice processing parameters\n    voice:\n      wake_word: "hey assistant"\n      sensitivity: 0.5\n      response_delay: 0.5  # seconds\n\n    # Vision processing parameters\n    vision:\n      detection_threshold: 0.7\n      tracking_enabled: true\n      fov_horizontal: 60  # degrees\n      fov_vertical: 45\n\n    # Navigation parameters\n    navigation:\n      max_speed: 0.5  # m/s\n      min_distance: 0.3  # m from obstacles\n      goal_tolerance: 0.2  # m\n\n    # Interaction parameters\n    interaction:\n      max_conversation_length: 10  # turns\n      context_window: 30  # seconds\n      feedback_enabled: true\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-8-package-configuration",children:"Step 8: Package Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Update the package.xml file:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>humanoid_assistant</name>\n  <version>0.0.0</version>\n  <description>Autonomous humanoid assistant using VLA (Vision-Language-Action)</description>\n  <maintainer email="user@example.com">user</maintainer>\n  <license>Apache-2.0</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>vision_msgs</depend>\n  <depend>tf2_ros</depend>\n  <depend>launch</depend>\n  <depend>launch_ros</depend>\n  <depend>robot_state_publisher</depend>\n  <depend>navigation2</depend>\n  <depend>nav2_bringup</depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,i.jsx)(n.p,{children:"Update the setup.py file:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\nimport os\nfrom glob import glob\n\npackage_name = 'humanoid_assistant'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),\n        (os.path.join('share', package_name, 'config'), glob('config/*.yaml')),\n        (os.path.join('share', package_name, 'rviz'), glob('rviz/*.rviz')),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='user',\n    maintainer_email='user@example.com',\n    description='Autonomous humanoid assistant using VLA (Vision-Language-Action)',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'main_system = humanoid_assistant.main_system:main',\n            'voice_processor = humanoid_assistant.voice_processor:main',\n            'vision_processor = humanoid_assistant.vision_processor:main',\n        ],\n    },\n)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-9-running-the-system",children:"Step 9: Running the System"}),"\n",(0,i.jsx)(n.p,{children:"To run the complete humanoid assistant system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Build the package\ncd ~/humanoid_assistant_ws\ncolcon build --packages-select humanoid_assistant\nsource install/setup.bash\n\n# Run the complete system\nros2 launch humanoid_assistant humanoid_assistant.launch.py\n\n# In another terminal, send voice commands\necho \"{'data': 'Please go to the kitchen and bring me a cup'}\" | ros2 topic pub /voice/command std_msgs/String --once\n"})}),"\n",(0,i.jsx)(n.h2,{id:"physical-ai-concepts-demonstrated",children:"Physical AI Concepts Demonstrated"}),"\n",(0,i.jsx)(n.h3,{id:"1-multimodal-integration",children:"1. Multimodal Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Voice commands processed through natural language understanding"}),"\n",(0,i.jsx)(n.li,{children:"Visual perception for environment awareness"}),"\n",(0,i.jsx)(n.li,{children:"Action execution based on integrated understanding"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-cognitive-reasoning",children:"2. Cognitive Reasoning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Task decomposition from high-level commands"}),"\n",(0,i.jsx)(n.li,{children:"Context-aware decision making"}),"\n",(0,i.jsx)(n.li,{children:"Adaptive behavior based on environment"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-real-time-processing",children:"3. Real-time Processing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Synchronized processing of multiple modalities"}),"\n",(0,i.jsx)(n.li,{children:"Real-time response to user commands"}),"\n",(0,i.jsx)(n.li,{children:"Continuous environment monitoring"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-human-robot-interaction",children:"4. Human-Robot Interaction"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Natural language interface"}),"\n",(0,i.jsx)(n.li,{children:"Contextual conversation management"}),"\n",(0,i.jsx)(n.li,{children:"Appropriate feedback and responses"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"advanced-features-implementation",children:"Advanced Features Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"context-management",children:"Context Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Enhanced context manager for the assistant\nclass ContextManager:\n    def __init__(self, max_history=10):\n        self.conversation_history = []\n        self.object_memory = {}\n        self.location_memory = {}\n        self.max_history = max_history\n\n    def update_conversation(self, user_input, system_response):\n        \"\"\"Update conversation history\"\"\"\n        entry = {\n            'timestamp': time.time(),\n            'user_input': user_input,\n            'system_response': system_response,\n            'follow_up': self.extract_follow_up_info(user_input)\n        }\n\n        self.conversation_history.append(entry)\n\n        # Maintain history size\n        if len(self.conversation_history) > self.max_history:\n            self.conversation_history.pop(0)\n\n    def extract_follow_up_info(self, text):\n        \"\"\"Extract information that might be referenced later\"\"\"\n        # Look for demonstratives, spatial references, etc.\n        follow_up_indicators = ['this', 'that', 'these', 'those', 'it', 'there', 'here']\n        words = text.lower().split()\n\n        # In a real system, this would use more sophisticated NLP\n        return [word for word in words if word in follow_up_indicators]\n\n    def resolve_references(self, command):\n        \"\"\"Resolve pronouns and references in command\"\"\"\n        # Simple reference resolution\n        if 'it' in command.lower() and self.conversation_history:\n            # Assume 'it' refers to the last mentioned object\n            last_response = self.conversation_history[-1]['system_response']\n            # Extract object from last response\n            import re\n            obj_match = re.search(r'(cup|bottle|book|object)', last_response.lower())\n            if obj_match:\n                return command.lower().replace('it', obj_match.group(1))\n\n        return command\n"})}),"\n",(0,i.jsx)(n.h3,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Safety validation for the assistant\nclass SafetyValidator:\n    def __init__(self):\n        self.safety_rules = {\n            \'navigation\': [\n                lambda goal: self.check_navigation_safety(goal),\n                lambda goal: self.check_accessible(goal)\n            ],\n            \'manipulation\': [\n                lambda obj: self.check_manipulation_safety(obj),\n                lambda obj: self.check_object_safety(obj)\n            ],\n            \'communication\': [\n                lambda msg: self.check_appropriate_content(msg)\n            ]\n        }\n\n    def validate_action(self, action_type, parameters):\n        """Validate action against safety rules"""\n        if action_type in self.safety_rules:\n            for rule in self.safety_rules[action_type]:\n                try:\n                    if not rule(parameters):\n                        return False, f"Action violates safety rule for {action_type}"\n                except Exception as e:\n                    return False, f"Safety check error: {e}"\n\n        return True, "Action is safe"\n\n    def check_navigation_safety(self, goal):\n        """Check if navigation goal is safe"""\n        # In real system, check against map, obstacles, restricted areas\n        return True  # Placeholder\n\n    def check_manipulation_safety(self, obj):\n        """Check if manipulation is safe"""\n        # In real system, check object properties, location, etc.\n        return True  # Placeholder\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"caching-and-efficiency",children:"Caching and Efficiency"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Performance optimization for the assistant\nfrom functools import lru_cache\nimport time\n\nclass PerformanceOptimizer:\n    def __init__(self):\n        self.response_cache = {}\n        self.cache_ttl = 300  # 5 minutes\n\n    @lru_cache(maxsize=128)\n    def cached_command_processing(self, command_hash):\n        """Cache expensive command processing"""\n        # This would cache the results of expensive LLM calls\n        pass\n\n    def should_cache_response(self, command, response):\n        """Determine if response should be cached"""\n        # Cache responses to common, factual questions\n        common_questions = [\'what is your name\', \'what can you do\', \'hello\']\n        return any(q in command.lower() for q in common_questions)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Voice Recognition Issues"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ensure microphone permissions and quality"}),"\n",(0,i.jsx)(n.li,{children:"Adjust energy thresholds based on environment"}),"\n",(0,i.jsx)(n.li,{children:"Use noise cancellation techniques"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Vision Processing Delays"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Optimize image resolution and processing frequency"}),"\n",(0,i.jsx)(n.li,{children:"Use GPU acceleration for deep learning models"}),"\n",(0,i.jsx)(n.li,{children:"Implement efficient object detection algorithms"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"LLM Integration Problems"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Verify API keys and rate limits"}),"\n",(0,i.jsx)(n.li,{children:"Implement proper error handling and fallbacks"}),"\n",(0,i.jsx)(n.li,{children:"Optimize prompt engineering for better results"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"System Integration Issues"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Check ROS2 network configuration"}),"\n",(0,i.jsx)(n.li,{children:"Verify message type compatibility"}),"\n",(0,i.jsx)(n.li,{children:"Monitor system resource usage"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,i.jsx)(n.p,{children:"The CAPSTONE project demonstrates the complete integration of Vision-Language-Action concepts into a functional Physical AI system. The autonomous humanoid assistant showcases how multiple modalities can be combined to create natural, intuitive human-robot interaction. The system processes voice commands, perceives its environment, reasons about tasks, and executes appropriate actions while maintaining safety and context awareness."}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'Extend the system to handle multi-step commands like "Go to the kitchen, pick up the red cup, and bring it to the living room."'}),"\n",(0,i.jsx)(n.li,{children:"Implement more sophisticated object recognition and manipulation capabilities."}),"\n",(0,i.jsx)(n.li,{children:"Add emotional intelligence features that adapt responses based on user tone and context."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, we'll explore advanced topics and future directions in Physical AI, including emerging technologies, research frontiers, and practical deployment considerations."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);