"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[7872],{8312:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module3-isaac/perception-pipeline","title":"Perception Pipeline with Isaac","description":"Chapter Objectives","source":"@site/docs/module3-isaac/05-perception-pipeline.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/perception-pipeline","permalink":"/physical-ai-book/docs/module3-isaac/perception-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module3-isaac/05-perception-pipeline.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Navigation Stack (Nav2) with Isaac","permalink":"/physical-ai-book/docs/module3-isaac/navigation-stack"},"next":{"title":"Project: Warehouse Robot with Isaac","permalink":"/physical-ai-book/docs/module3-isaac/project"}}');var a=t(4848),s=t(8453);const o={sidebar_position:5},r="Perception Pipeline with Isaac",c={},l=[{value:"Chapter Objectives",id:"chapter-objectives",level:2},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:2},{value:"Overview of Isaac Perception Stack",id:"overview-of-isaac-perception-stack",level:3},{value:"Key Perception Components",id:"key-perception-components",level:3},{value:"Isaac ROS Perception Packages",id:"isaac-ros-perception-packages",level:2},{value:"Core Perception Packages",id:"core-perception-packages",level:3},{value:"Image Pipeline Configuration",id:"image-pipeline-configuration",level:3},{value:"Launch File for Image Pipeline",id:"launch-file-for-image-pipeline",level:3},{value:"Object Detection with Isaac",id:"object-detection-with-isaac",level:2},{value:"Isaac DetectNet Configuration",id:"isaac-detectnet-configuration",level:3},{value:"Object Detection Node Implementation",id:"object-detection-node-implementation",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Isaac Segmentation Configuration",id:"isaac-segmentation-configuration",level:3},{value:"Segmentation Node Implementation",id:"segmentation-node-implementation",level:3},{value:"Multi-Object Tracking",id:"multi-object-tracking",level:2},{value:"Isaac Tracking Configuration",id:"isaac-tracking-configuration",level:3},{value:"Tracking Node Implementation",id:"tracking-node-implementation",level:3},{value:"6-DOF Pose Estimation",id:"6-dof-pose-estimation",level:2},{value:"Isaac Pose Estimation Configuration",id:"isaac-pose-estimation-configuration",level:3},{value:"Pose Estimation Node",id:"pose-estimation-node",level:3},{value:"Perception Pipeline Integration",id:"perception-pipeline-integration",level:2},{value:"Complete Perception Pipeline Launch",id:"complete-perception-pipeline-launch",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"Perception Quality Metrics",id:"perception-quality-metrics",level:3},{value:"Best Practices for Isaac Perception",id:"best-practices-for-isaac-perception",level:2},{value:"Configuration Best Practices",id:"configuration-best-practices",level:3},{value:"Performance Best Practices",id:"performance-best-practices",level:3},{value:"Integration Best Practices",id:"integration-best-practices",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"perception-pipeline-with-isaac",children:"Perception Pipeline with Isaac"})}),"\n",(0,a.jsx)(n.h2,{id:"chapter-objectives",children:"Chapter Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Design and implement GPU-accelerated perception pipelines"}),"\n",(0,a.jsx)(n.li,{children:"Integrate Isaac ROS perception packages for Physical AI"}),"\n",(0,a.jsx)(n.li,{children:"Configure object detection and tracking systems"}),"\n",(0,a.jsx)(n.li,{children:"Build semantic segmentation and scene understanding systems"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate perception performance and accuracy"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"overview-of-isaac-perception-stack",children:"Overview of Isaac Perception Stack"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac perception pipeline leverages GPU acceleration for real-time processing of sensor data to enable Physical AI systems to understand their environment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Sensors       \u2502    \u2502 Isaac ROS       \u2502    \u2502   Perception    \u2502\n\u2502   (Cameras,     \u2502\u2500\u2500\u2500\u25b6\u2502   Perception    \u2502\u2500\u2500\u2500\u25b6\u2502   Results       \u2502\n\u2502   LiDAR, etc.)  \u2502    \u2502   (GPU)         \u2502    \u2502   (Objects,     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   Segmentation, \u2502\n                                              \u2502   Tracking, etc.)\u2502\n                                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.h3,{id:"key-perception-components",children:"Key Perception Components"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Image Processing"}),": GPU-accelerated image rectification, filtering, and enhancement"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Object Detection"}),": Real-time detection of objects using deep learning"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Semantic Segmentation"}),": Pixel-level scene understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Instance Segmentation"}),": Individual object segmentation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Object Tracking"}),": Multi-object tracking across frames"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pose Estimation"}),": 6-DOF pose estimation for objects"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scene Understanding"}),": 3D scene reconstruction and understanding"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-perception-packages",children:"Isaac ROS Perception Packages"}),"\n",(0,a.jsx)(n.h3,{id:"core-perception-packages",children:"Core Perception Packages"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_image_pipeline"}),": GPU-accelerated image processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_detectnet"}),": Object detection with NVIDIA DetectNet"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_segmentation"}),": Semantic segmentation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_pose_estimation"}),": 6-DOF pose estimation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_tracking"}),": Multi-object tracking"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_pointcloud_utils"}),": Point cloud processing"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"image-pipeline-configuration",children:"Image Pipeline Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# File: config/image_pipeline_params.yaml\nisaac_ros_image_pipeline:\n  ros__parameters:\n    # Input settings\n    input_width: 640\n    input_height: 480\n    input_format: "rgb8"\n\n    # Processing settings\n    enable_rectification: true\n    enable_resize: false\n    enable_format_conversion: true\n\n    # GPU settings\n    cuda_device: 0\n    enable_profiler: false\n\n    # Output settings\n    output_qos: 10\n    publish_processed_images: true\n'})}),"\n",(0,a.jsx)(n.h3,{id:"launch-file-for-image-pipeline",children:"Launch File for Image Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# File: launch/image_pipeline.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config_file = os.path.join(\n        get_package_share_directory('your_package'),\n        'config',\n        'image_pipeline_params.yaml'\n    )\n\n    # Image rectification node\n    rectify_node = Node(\n        package='isaac_ros_image_pipeline',\n        executable='image_rectification_node',\n        parameters=[config_file],\n        remappings=[\n            ('image_raw', '/camera/image_raw'),\n            ('camera_info', '/camera/camera_info'),\n            ('image_rect', '/camera/image_rect_color')\n        ]\n    )\n\n    # Image resize node\n    resize_node = Node(\n        package='isaac_ros_image_pipeline',\n        executable='image_resize_node',\n        parameters=[config_file],\n        remappings=[\n            ('image', '/camera/image_rect_color'),\n            ('resized_image', '/camera/image_resized')\n        ]\n    )\n\n    return LaunchDescription([\n        rectify_node,\n        resize_node\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"object-detection-with-isaac",children:"Object Detection with Isaac"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-detectnet-configuration",children:"Isaac DetectNet Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# File: config/detectnet_params.yaml\nisaac_ros_detectnet:\n  ros__parameters:\n    # Model settings\n    model_name: "ssd_mobilenet_v2_coco"\n    confidence_threshold: 0.7\n    enable_profiler: false\n\n    # Input settings\n    input_width: 640\n    input_height: 480\n    input_format: "rgb8"\n\n    # GPU settings\n    cuda_device: 0\n    input_tensor: "input_tensor"\n    output_coverage: "output_cov"\n    output_bbox: "output_bbox"\n\n    # Output settings\n    publish_overlay: true\n    publish_mask: false\n    mask_pool_size: 16\n\n    # Topic settings\n    image_input_topic: "/camera/image_rect_color"\n    camera_info_input_topic: "/camera/camera_info"\n    detections_output_topic: "/detectnet/detections"\n    overlay_image_output_topic: "/detectnet/overlay"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"object-detection-node-implementation",children:"Object Detection Node Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# File: perception/object_detection_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass IsaacObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_object_detection_node')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publishers\n        self.detections_pub = self.create_publisher(\n            Detection2DArray,\n            '/object_detections',\n            10\n        )\n\n        self.overlay_pub = self.create_publisher(\n            Image,\n            '/detection_overlay',\n            10\n        )\n\n        # State\n        self.camera_info = None\n        self.detection_model = None  # Would be Isaac DetectNet model\n\n        # Detection parameters\n        self.confidence_threshold = 0.7\n\n        self.get_logger().info('Isaac Object Detection Node Started')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Process camera calibration information\"\"\"\n        self.camera_info = msg\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming images for object detection\"\"\"\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Perform object detection (Isaac GPU-accelerated)\n            detections = self.perform_detection(cv_image)\n\n            # Create detection messages\n            detection_msg = self.create_detection_message(detections, msg.header)\n\n            # Create overlay image\n            overlay_image = self.create_detection_overlay(cv_image, detections)\n\n            # Publish results\n            self.detections_pub.publish(detection_msg)\n\n            overlay_msg = self.bridge.cv2_to_imgmsg(overlay_image, \"bgr8\")\n            overlay_msg.header = msg.header\n            self.overlay_pub.publish(overlay_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def perform_detection(self, image):\n        \"\"\"Perform GPU-accelerated object detection\"\"\"\n        # In real implementation, this would use Isaac DetectNet\n        # For demonstration, using a placeholder\n        detections = []\n\n        # Simulate detection results\n        if np.random.random() > 0.5:  # Random detection for demo\n            detections.append({\n                'bbox': [100, 100, 200, 150],  # [x, y, width, height]\n                'class_id': 1,\n                'class_name': 'person',\n                'confidence': 0.85\n            })\n\n        return detections\n\n    def create_detection_message(self, detections, header):\n        \"\"\"Create vision_msgs Detection2DArray message\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            detection_msg = Detection2D()\n            detection_msg.header = header\n\n            # Set bounding box\n            bbox = detection['bbox']\n            detection_msg.bbox.center.x = bbox[0] + bbox[2] / 2  # center x\n            detection_msg.bbox.center.y = bbox[1] + bbox[3] / 2  # center y\n            detection_msg.bbox.size_x = bbox[2]  # width\n            detection_msg.bbox.size_y = bbox[3]  # height\n\n            # Set hypothesis\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = str(detection['class_name'])\n            hypothesis.hypothesis.score = detection['confidence']\n            detection_msg.results.append(hypothesis)\n\n            detection_array.detections.append(detection_msg)\n\n        return detection_array\n\n    def create_detection_overlay(self, image, detections):\n        \"\"\"Create image overlay with detection bounding boxes\"\"\"\n        overlay = image.copy()\n\n        for detection in detections:\n            bbox = detection['bbox']\n            x, y, w, h = bbox\n\n            # Draw bounding box\n            cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n            # Draw label\n            label = f\"{detection['class_name']}: {detection['confidence']:.2f}\"\n            cv2.putText(overlay, label, (x, y - 10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        return overlay\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacObjectDetectionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-segmentation-configuration",children:"Isaac Segmentation Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# File: config/segmentation_params.yaml\nisaac_ros_segmentation:\n  ros__parameters:\n    # Model settings\n    model_name: "unet_coco"\n    confidence_threshold: 0.5\n    colormap: "coco_colormap"\n\n    # Input settings\n    input_width: 640\n    input_height: 480\n    input_format: "rgb8"\n\n    # GPU settings\n    cuda_device: 0\n    enable_profiler: false\n\n    # Output settings\n    publish_segmentation_map: true\n    publish_overlay: true\n    publish_mask: true\n\n    # Topic settings\n    image_input_topic: "/camera/image_rect_color"\n    segmentation_output_topic: "/segmentation/segmentation_map"\n    overlay_output_topic: "/segmentation/overlay"\n    mask_output_topic: "/segmentation/mask"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"segmentation-node-implementation",children:"Segmentation Node Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# File: perception/segmentation_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass IsaacSegmentationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_segmentation_node\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_rect_color\',\n            self.image_callback,\n            10\n        )\n\n        # Publishers\n        self.segmentation_pub = self.create_publisher(\n            Image,\n            \'/segmentation/segmentation_map\',\n            10\n        )\n\n        self.overlay_pub = self.create_publisher(\n            Image,\n            \'/segmentation/overlay\',\n            10\n        )\n\n        # State\n        self.segmentation_model = None  # Would be Isaac segmentation model\n\n        self.get_logger().info(\'Isaac Segmentation Node Started\')\n\n    def image_callback(self, msg):\n        """Process incoming images for semantic segmentation"""\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Perform segmentation (Isaac GPU-accelerated)\n            segmentation_map = self.perform_segmentation(cv_image)\n\n            # Create overlay\n            overlay = self.create_segmentation_overlay(cv_image, segmentation_map)\n\n            # Publish results\n            seg_msg = self.bridge.cv2_to_imgmsg(segmentation_map.astype(np.uint8), "mono8")\n            seg_msg.header = msg.header\n            self.segmentation_pub.publish(seg_msg)\n\n            overlay_msg = self.bridge.cv2_to_imgmsg(overlay, "bgr8")\n            overlay_msg.header = msg.header\n            self.overlay_pub.publish(overlay_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def perform_segmentation(self, image):\n        """Perform GPU-accelerated semantic segmentation"""\n        # In real implementation, this would use Isaac segmentation\n        # For demonstration, creating a simple segmentation map\n        height, width = image.shape[:2]\n\n        # Create a simple segmentation map (class IDs)\n        segmentation_map = np.zeros((height, width), dtype=np.uint8)\n\n        # Simulate some segmentation results\n        # In reality, this would come from a trained model\n        for i in range(5):  # Simulate 5 different objects\n            center_x = np.random.randint(50, width-50)\n            center_y = np.random.randint(50, height-50)\n            radius = np.random.randint(20, 50)\n\n            y, x = np.ogrid[:height, :width]\n            mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n            segmentation_map[mask] = i + 1  # Class ID\n\n        return segmentation_map\n\n    def create_segmentation_overlay(self, image, segmentation_map):\n        """Create overlay with colored segmentation"""\n        # Generate color map for different classes\n        colored_mask = self.apply_colormap(segmentation_map)\n\n        # Blend original image with segmentation mask\n        alpha = 0.6  # Transparency factor\n        overlay = cv2.addWeighted(image, alpha, colored_mask, 1 - alpha, 0)\n\n        return overlay\n\n    def apply_colormap(self, segmentation_map):\n        """Apply color map to segmentation results"""\n        # Create a color map with different colors for each class\n        height, width = segmentation_map.shape\n        colored_map = np.zeros((height, width, 3), dtype=np.uint8)\n\n        # Define colors for different classes\n        colors = [\n            [0, 0, 0],      # Class 0: Background (black)\n            [255, 0, 0],    # Class 1: Red\n            [0, 255, 0],    # Class 2: Green\n            [0, 0, 255],    # Class 3: Blue\n            [255, 255, 0],  # Class 4: Yellow\n            [255, 0, 255],  # Class 5: Magenta\n        ]\n\n        for class_id in range(len(colors)):\n            mask = segmentation_map == class_id\n            colored_map[mask] = colors[class_id]\n\n        return colored_map\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacSegmentationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"multi-object-tracking",children:"Multi-Object Tracking"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-tracking-configuration",children:"Isaac Tracking Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# File: config/tracking_params.yaml\nisaac_ros_tracking:\n  ros__parameters:\n    # Tracking settings\n    max_objects: 50\n    max_track_age: 100\n    min_track_age: 5\n    matching_threshold: 0.3\n\n    # Detection settings\n    detection_topic: "/detectnet/detections"\n    confidence_threshold: 0.7\n\n    # GPU settings\n    cuda_device: 0\n    enable_profiler: false\n\n    # Output settings\n    publish_tracked_objects: true\n    publish_trajectories: true\n\n    # Topic settings\n    image_input_topic: "/camera/image_rect_color"\n    tracked_objects_output_topic: "/tracking/tracked_objects"\n    trajectory_output_topic: "/tracking/trajectories"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"tracking-node-implementation",children:"Tracking Node Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# File: perception/tracking_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import Header\nfrom geometry_msgs.msg import Point\nimport numpy as np\nfrom collections import deque\nimport cv2\n\nclass TrackedObject:\n    def __init__(self, detection, track_id):\n        self.id = track_id\n        self.detections = deque(maxlen=50)  # Store last 50 detections\n        self.kalman_filter = self.initialize_kalman_filter(detection)\n        self.age = 0\n        self.hits = 1\n        self.hit_streak = 1\n        self.time_since_update = 0\n\n    def initialize_kalman_filter(self, detection):\n        \"\"\"Initialize Kalman filter for tracking\"\"\"\n        # Simple Kalman filter for position and velocity\n        # In practice, would use more sophisticated tracking\n        bbox = detection.bbox\n        x = bbox.center.x\n        y = bbox.center.y\n\n        # State: [x, y, vx, vy]\n        kf = {\n            'state': np.array([x, y, 0, 0], dtype=np.float32),\n            'covariance': np.eye(4, dtype=np.float32) * 100,\n            'process_noise': np.eye(4, dtype=np.float32) * 0.1,\n            'measurement_noise': np.eye(2, dtype=np.float32) * 10\n        }\n        return kf\n\n    def predict(self):\n        \"\"\"Predict next state\"\"\"\n        # Simple constant velocity model\n        dt = 1.0  # Time step\n        F = np.array([\n            [1, 0, dt, 0],\n            [0, 1, 0, dt],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1]\n        ], dtype=np.float32)\n\n        self.kalman_filter['state'] = F @ self.kalman_filter['state']\n        P = self.kalman_filter['covariance']\n        Q = self.kalman_filter['process_noise']\n        self.kalman_filter['covariance'] = F @ P @ F.T + Q\n\n        return self.kalman_filter['state'][:2]  # Return position [x, y]\n\n    def update(self, measurement):\n        \"\"\"Update with new measurement\"\"\"\n        # Extract position from measurement\n        z = np.array([measurement.bbox.center.x, measurement.bbox.center.y], dtype=np.float32)\n\n        # Measurement matrix\n        H = np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0]\n        ], dtype=np.float32)\n\n        # Predicted measurement\n        x = self.kalman_filter['state']\n        P = self.kalman_filter['covariance']\n        R = self.kalman_filter['measurement_noise']\n\n        y = z - H @ x  # Innovation\n        S = H @ P @ H.T + R  # Innovation covariance\n        K = P @ H.T @ np.linalg.inv(S)  # Kalman gain\n\n        # Update state and covariance\n        self.kalman_filter['state'] = x + K @ y\n        I = np.eye(len(x))\n        self.kalman_filter['covariance'] = (I - K @ H) @ P\n\n        # Store detection\n        self.detections.append(measurement)\n        self.hits += 1\n        self.hit_streak += 1\n        self.time_since_update = 0\n\nclass IsaacTrackingNode(Node):\n    def __init__(self):\n        super().__init__('isaac_tracking_node')\n\n        # Subscribers\n        self.detections_sub = self.create_subscription(\n            Detection2DArray,\n            '/object_detections',\n            self.detections_callback,\n            10\n        )\n\n        # Publishers\n        self.tracked_objects_pub = self.create_publisher(\n            Detection2DArray,\n            '/tracking/tracked_objects',\n            10\n        )\n\n        # State\n        self.tracks = []\n        self.next_track_id = 0\n        self.max_disappeared = 10  # Frames before deleting track\n\n        self.get_logger().info('Isaac Tracking Node Started')\n\n    def detections_callback(self, msg):\n        \"\"\"Process new detections and update tracks\"\"\"\n        # Convert detections to format suitable for tracking\n        detections = []\n        for detection in msg.detections:\n            detections.append(detection)\n\n        # Update existing tracks\n        self.update_tracks(detections)\n\n        # Create message with tracked objects\n        tracked_msg = Detection2DArray()\n        tracked_msg.header = msg.header\n\n        for track in self.tracks:\n            if track.time_since_update == 0:  # Only include active tracks\n                # Create detection message with track ID\n                detection = Detection2D()\n                detection.header = msg.header\n                detection.bbox = track.detections[-1].bbox  # Use latest detection\n\n                # Add track ID as class name\n                hypothesis = track.detections[-1].results[0] if track.detections else None\n                if hypothesis:\n                    hypothesis.hypothesis.class_id = f\"track_{track.id}\"\n                    detection.results.append(hypothesis)\n\n                tracked_msg.detections.append(detection)\n\n        self.tracked_objects_pub.publish(tracked_msg)\n\n    def update_tracks(self, detections):\n        \"\"\"Update tracks with new detections\"\"\"\n        # Predict new positions for all tracks\n        for track in self.tracks:\n            track.predict()\n\n        # If no tracks exist, create new ones for all detections\n        if len(self.tracks) == 0:\n            for detection in detections:\n                new_track = TrackedObject(detection, self.next_track_id)\n                self.tracks.append(new_track)\n                self.next_track_id += 1\n            return\n\n        # Calculate distance matrix between tracks and detections\n        if len(detections) > 0:\n            # Simple distance-based association\n            track_predictions = [track.predict() for track in self.tracks]\n            detection_positions = [\n                [detection.bbox.center.x, detection.bbox.center.y]\n                for detection in detections\n            ]\n\n            # Associate tracks with detections (simple nearest neighbor)\n            used_detections = set()\n            for i, track_pos in enumerate(track_predictions):\n                min_dist = float('inf')\n                best_det_idx = -1\n\n                for j, det_pos in enumerate(detection_positions):\n                    if j in used_detections:\n                        continue\n\n                    dist = np.sqrt(\n                        (track_pos[0] - det_pos[0])**2 +\n                        (track_pos[1] - det_pos[1])**2\n                    )\n\n                    if dist < min_dist and dist < 50:  # Threshold for association\n                        min_dist = dist\n                        best_det_idx = j\n\n                if best_det_idx != -1:\n                    # Update track with associated detection\n                    self.tracks[i].update(detections[best_det_idx])\n                    used_detections.add(best_det_idx)\n                else:\n                    # No detection associated, increment time since update\n                    self.tracks[i].time_since_update += 1\n\n            # Create new tracks for unassociated detections\n            for i, detection in enumerate(detections):\n                if i not in used_detections:\n                    new_track = TrackedObject(detection, self.next_track_id)\n                    self.tracks.append(new_track)\n                    self.next_track_id += 1\n\n        # Remove old tracks\n        self.tracks = [\n            track for track in self.tracks\n            if track.time_since_update <= self.max_disappeared\n        ]\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacTrackingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"6-dof-pose-estimation",children:"6-DOF Pose Estimation"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-pose-estimation-configuration",children:"Isaac Pose Estimation Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# File: config/pose_estimation_params.yaml\nisaac_ros_pose_estimation:\n  ros__parameters:\n    # Model settings\n    model_name: "6dof_pose_model"\n    confidence_threshold: 0.8\n    enable_refinement: true\n\n    # Input settings\n    input_width: 640\n    input_height: 480\n    input_format: "rgb8"\n\n    # GPU settings\n    cuda_device: 0\n    enable_profiler: false\n\n    # Object settings\n    object_models_path: "/path/to/object/models"\n    enable_multi_object: true\n\n    # Output settings\n    publish_poses: true\n    publish_visualization: true\n\n    # Topic settings\n    image_input_topic: "/camera/image_rect_color"\n    camera_info_input_topic: "/camera/camera_info"\n    poses_output_topic: "/pose_estimation/poses"\n    visualization_output_topic: "/pose_estimation/visualization"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"pose-estimation-node",children:"Pose Estimation Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# File: perception/pose_estimation_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, PoseArray\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom tf_transformations import quaternion_from_euler\n\nclass IsaacPoseEstimationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_pose_estimation_node\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_rect_color\',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publishers\n        self.poses_pub = self.create_publisher(\n            PoseArray,\n            \'/pose_estimation/poses\',\n            10\n        )\n\n        self.visualization_pub = self.create_publisher(\n            Image,\n            \'/pose_estimation/visualization\',\n            10\n        )\n\n        # State\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.pose_model = None  # Would be Isaac pose estimation model\n\n        self.get_logger().info(\'Isaac Pose Estimation Node Started\')\n\n    def camera_info_callback(self, msg):\n        """Process camera calibration information"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        """Process incoming images for pose estimation"""\n        if self.camera_matrix is None:\n            return\n\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Perform pose estimation\n            poses = self.estimate_poses(cv_image)\n\n            # Create PoseArray message\n            pose_array = PoseArray()\n            pose_array.header = msg.header\n            pose_array.poses = poses\n\n            # Create visualization\n            viz_image = self.create_pose_visualization(cv_image, poses)\n\n            # Publish results\n            self.poses_pub.publish(pose_array)\n\n            viz_msg = self.bridge.cv2_to_imgmsg(viz_image, "bgr8")\n            viz_msg.header = msg.header\n            self.visualization_pub.publish(viz_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def estimate_poses(self, image):\n        """Estimate 6-DOF poses of objects in image"""\n        # In real implementation, this would use Isaac pose estimation\n        # For demonstration, returning placeholder poses\n        poses = []\n\n        # Detect objects first (simplified)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        _, thresh = cv2.threshold(gray, 127, 255, 0)\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            if cv2.contourArea(contour) > 500:  # Filter small contours\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n\n                # Estimate 3D pose (simplified)\n                center_x = x + w / 2\n                center_y = y + h / 2\n\n                # Convert pixel coordinates to 3D (simplified)\n                z = 1.0  # Assume fixed depth for demo\n                x_3d = (center_x - self.camera_matrix[0, 2]) * z / self.camera_matrix[0, 0]\n                y_3d = (center_y - self.camera_matrix[1, 2]) * z / self.camera_matrix[1, 1]\n\n                # Create pose\n                pose = PoseStamped()\n                pose.pose.position.x = x_3d\n                pose.pose.position.y = y_3d\n                pose.pose.position.z = z\n\n                # Set orientation (simplified)\n                quat = quaternion_from_euler(0, 0, 0)\n                pose.pose.orientation.x = quat[0]\n                pose.pose.orientation.y = quat[1]\n                pose.pose.orientation.z = quat[2]\n                pose.pose.orientation.w = quat[3]\n\n                poses.append(pose.pose)\n\n        return poses\n\n    def create_pose_visualization(self, image, poses):\n        """Create visualization with pose information"""\n        viz_image = image.copy()\n\n        for i, pose in enumerate(poses):\n            # Convert 3D position back to 2D for visualization\n            point_3d = np.array([pose.position.x, pose.position.y, pose.position.z])\n            rvec = np.array([0, 0, 0])  # Rotation vector (simplified)\n            tvec = np.array([0, 0, 0])  # Translation vector (simplified)\n\n            # Project 3D point to 2D\n            point_2d, _ = cv2.projectPoints(\n                point_3d.reshape(1, 1, 3),\n                rvec, tvec,\n                self.camera_matrix,\n                self.distortion_coeffs if self.distortion_coeffs is not None else np.array([])\n            )\n\n            # Draw pose indicator\n            center = (int(point_2d[0][0][0]), int(point_2d[0][0][1]))\n            cv2.circle(viz_image, center, 10, (0, 255, 0), 2)\n            cv2.putText(viz_image, f\'Object {i}\', center,\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        return viz_image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacPoseEstimationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"perception-pipeline-integration",children:"Perception Pipeline Integration"}),"\n",(0,a.jsx)(n.h3,{id:"complete-perception-pipeline-launch",children:"Complete Perception Pipeline Launch"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# File: launch/perception_pipeline.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nimport os\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    config_dir = LaunchConfiguration('config_dir', default='config')\n\n    # Get package path\n    pkg_perception = FindPackageShare('isaac_perception_demos')\n\n    # Image pipeline\n    image_pipeline = Node(\n        package='isaac_ros_image_pipeline',\n        executable='image_pipeline_node',\n        parameters=[\n            PathJoinSubstitution([pkg_perception, config_dir, 'image_pipeline_params.yaml'])\n        ],\n        remappings=[\n            ('image_raw', '/camera/image_raw'),\n            ('camera_info', '/camera/camera_info'),\n            ('image_rect', '/camera/image_rect_color')\n        ]\n    )\n\n    # Object detection\n    object_detection = Node(\n        package='isaac_ros_detectnet',\n        executable='detectnet_node',\n        parameters=[\n            PathJoinSubstitution([pkg_perception, config_dir, 'detectnet_params.yaml'])\n        ],\n        remappings=[\n            ('image', '/camera/image_rect_color'),\n            ('camera_info', '/camera/camera_info'),\n            ('detections', '/object_detections')\n        ]\n    )\n\n    # Semantic segmentation\n    segmentation = Node(\n        package='isaac_ros_segmentation',\n        executable='segmentation_node',\n        parameters=[\n            PathJoinSubstitution([pkg_perception, config_dir, 'segmentation_params.yaml'])\n        ],\n        remappings=[\n            ('image', '/camera/image_rect_color'),\n            ('segmentation_map', '/segmentation/map')\n        ]\n    )\n\n    # Object tracking\n    tracking = Node(\n        package='isaac_perception_demos',\n        executable='tracking_node',\n        parameters=[\n            PathJoinSubstitution([pkg_perception, config_dir, 'tracking_params.yaml'])\n        ],\n        remappings=[\n            ('detections', '/object_detections'),\n            ('tracked_objects', '/tracked_objects')\n        ]\n    )\n\n    # Pose estimation\n    pose_estimation = Node(\n        package='isaac_perception_demos',\n        executable='pose_estimation_node',\n        parameters=[\n            PathJoinSubstitution([pkg_perception, config_dir, 'pose_estimation_params.yaml'])\n        ],\n        remappings=[\n            ('image', '/camera/image_rect_color'),\n            ('camera_info', '/camera/camera_info'),\n            ('poses', '/object_poses')\n        ]\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation time'\n        ),\n        DeclareLaunchArgument(\n            'config_dir',\n            default_value='config',\n            description='Configuration directory'\n        ),\n        image_pipeline,\n        object_detection,\n        segmentation,\n        tracking,\n        pose_estimation\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,a.jsx)(n.h3,{id:"perception-quality-metrics",children:"Perception Quality Metrics"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# File: perception/perception_evaluator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom vision_msgs.msg import Detection2DArray\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Float32, Int32\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass PerceptionEvaluator(Node):\n    def __init__(self):\n        super().__init__('perception_evaluator')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribers\n        self.detections_sub = self.create_subscription(\n            Detection2DArray,\n            '/object_detections',\n            self.detections_callback,\n            10\n        )\n\n        # Publishers\n        self.detection_rate_pub = self.create_publisher(Float32, '/perception/detection_rate', 10)\n        self.accuracy_pub = self.create_publisher(Float32, '/perception/accuracy', 10)\n        self.object_count_pub = self.create_publisher(Int32, '/perception/object_count', 10)\n\n        # State\n        self.detection_history = []\n        self.frame_count = 0\n        self.detection_count = 0\n        self.last_time = self.get_clock().now()\n\n        # Timer for evaluation\n        self.eval_timer = self.create_timer(1.0, self.evaluate_performance)\n\n        self.get_logger().info('Perception Evaluator Node Started')\n\n    def detections_callback(self, msg):\n        \"\"\"Process detection results\"\"\"\n        self.detection_count += len(msg.detections)\n        self.frame_count += 1\n\n        # Store detection confidence for accuracy calculation\n        for detection in msg.detections:\n            if detection.results:\n                confidence = detection.results[0].hypothesis.score\n                self.detection_history.append(confidence)\n\n    def evaluate_performance(self):\n        \"\"\"Evaluate perception performance metrics\"\"\"\n        current_time = self.get_clock().now()\n        dt = (current_time - self.last_time).nanoseconds / 1e9\n\n        if dt > 0 and self.frame_count > 0:\n            # Detection rate (detections per second)\n            detection_rate = self.detection_count / dt\n            rate_msg = Float32()\n            rate_msg.data = detection_rate\n            self.detection_rate_pub.publish(rate_msg)\n\n            # Average confidence (proxy for accuracy)\n            avg_confidence = np.mean(self.detection_history) if self.detection_history else 0.0\n            accuracy_msg = Float32()\n            accuracy_msg.data = avg_confidence\n            self.accuracy_pub.publish(accuracy_msg)\n\n            # Object count\n            obj_count_msg = Int32()\n            obj_count_msg.data = len(self.detection_history)\n            self.object_count_pub.publish(obj_count_msg)\n\n            # Log metrics\n            self.get_logger().info(\n                f'Perception Metrics - Rate: {detection_rate:.2f} det/s, '\n                f'Accuracy: {avg_confidence:.3f}, Objects: {len(self.detection_history)}'\n            )\n\n        # Reset counters\n        self.last_time = current_time\n        self.frame_count = 0\n        self.detection_count = 0\n        self.detection_history = []  # Keep only recent history\n\ndef main(args=None):\n    rclpy.init(args=args)\n    evaluator = PerceptionEvaluator()\n\n    try:\n        rclpy.spin(evaluator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        evaluator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-for-isaac-perception",children:"Best Practices for Isaac Perception"}),"\n",(0,a.jsx)(n.h3,{id:"configuration-best-practices",children:"Configuration Best Practices"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use appropriate models for your specific use case"}),"\n",(0,a.jsx)(n.li,{children:"Configure input resolution based on GPU capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Set confidence thresholds to balance precision and recall"}),"\n",(0,a.jsx)(n.li,{children:"Use proper camera calibration for accurate measurements"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-best-practices",children:"Performance Best Practices"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Optimize pipeline for your specific GPU"}),"\n",(0,a.jsx)(n.li,{children:"Use appropriate batch sizes for deep learning models"}),"\n",(0,a.jsx)(n.li,{children:"Implement multi-threading for CPU-bound operations"}),"\n",(0,a.jsx)(n.li,{children:"Monitor GPU utilization and memory usage"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"integration-best-practices",children:"Integration Best Practices"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Validate perception results against ground truth when possible"}),"\n",(0,a.jsx)(n.li,{children:"Implement fallback strategies for perception failures"}),"\n",(0,a.jsx)(n.li,{children:"Use multiple sensors for redundancy"}),"\n",(0,a.jsx)(n.li,{children:"Plan for graceful degradation when perception fails"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac perception pipeline provides GPU-accelerated processing for real-time Physical AI applications. By leveraging Isaac ROS packages for object detection, segmentation, tracking, and pose estimation, robots can achieve sophisticated scene understanding capabilities. Proper configuration and integration of these components enable robust perception in complex environments."}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement a complete perception pipeline with object detection and tracking."}),"\n",(0,a.jsx)(n.li,{children:"Evaluate perception performance in different lighting conditions."}),"\n",(0,a.jsx)(n.li,{children:"Integrate perception with navigation for obstacle avoidance."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"In the next chapter, we'll work on a project that integrates all the Isaac concepts learned in Module 3, creating a complete intelligent robot system with perception, navigation, and interaction capabilities."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);