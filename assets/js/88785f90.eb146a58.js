"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[1088],{5242:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module2-simulation/simulating-sensors","title":"Simulating Sensors (LiDAR, Cameras, IMU)","description":"Chapter Objectives","source":"@site/docs/module2-simulation/03-simulating-sensors.md","sourceDirName":"module2-simulation","slug":"/module2-simulation/simulating-sensors","permalink":"/physical-ai-book/docs/module2-simulation/simulating-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/syedaareebashah/physical-ai-book/edit/main/docs/module2-simulation/03-simulating-sensors.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Fundamentals","permalink":"/physical-ai-book/docs/module2-simulation/gazebo-fundamentals"},"next":{"title":"Unity for High-Fidelity Rendering","permalink":"/physical-ai-book/docs/module2-simulation/unity-rendering"}}');var i=a(4848),r=a(8453);const o={sidebar_position:3},t="Simulating Sensors (LiDAR, Cameras, IMU)",l={},c=[{value:"Chapter Objectives",id:"chapter-objectives",level:2},{value:"Sensor Simulation Overview",id:"sensor-simulation-overview",level:2},{value:"Camera Simulation",id:"camera-simulation",level:2},{value:"Basic Camera Configuration",id:"basic-camera-configuration",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"Camera Processing Node",id:"camera-processing-node",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"2D LiDAR Configuration",id:"2d-lidar-configuration",level:3},{value:"3D LiDAR Configuration (Velodyne-style)",id:"3d-lidar-configuration-velodyne-style",level:3},{value:"LiDAR Processing Node",id:"lidar-processing-node",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Sensor Configuration",id:"imu-sensor-configuration",level:3},{value:"IMU Processing Node",id:"imu-processing-node",level:3},{value:"GPS Simulation",id:"gps-simulation",level:2},{value:"GPS Sensor Configuration",id:"gps-sensor-configuration",level:3},{value:"Force/Torque Sensor Simulation",id:"forcetorque-sensor-simulation",level:2},{value:"Force/Torque Sensor in Joints",id:"forcetorque-sensor-in-joints",level:3},{value:"Multi-Sensor Fusion for Physical AI",id:"multi-sensor-fusion-for-physical-ai",level:2},{value:"Sensor Fusion Node Example",id:"sensor-fusion-node-example",level:3},{value:"Physical AI Sensor Considerations",id:"physical-ai-sensor-considerations",level:2},{value:"Realism vs. Performance",id:"realism-vs-performance",level:3},{value:"Environmental Effects",id:"environmental-effects",level:3},{value:"Validation and Calibration",id:"validation-and-calibration",level:2},{value:"Comparing Simulated vs. Real Sensors",id:"comparing-simulated-vs-real-sensors",level:3},{value:"Best Practices for Physical AI Sensor Simulation",id:"best-practices-for-physical-ai-sensor-simulation",level:2},{value:"Accuracy Considerations",id:"accuracy-considerations",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"simulating-sensors-lidar-cameras-imu",children:"Simulating Sensors (LiDAR, Cameras, IMU)"})}),"\n",(0,i.jsx)(n.h2,{id:"chapter-objectives",children:"Chapter Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Configure and simulate various sensor types in Gazebo"}),"\n",(0,i.jsx)(n.li,{children:"Understand the characteristics and limitations of simulated sensors"}),"\n",(0,i.jsx)(n.li,{children:"Integrate simulated sensors with ROS 2 for Physical AI applications"}),"\n",(0,i.jsx)(n.li,{children:"Validate sensor data accuracy and performance"}),"\n",(0,i.jsx)(n.li,{children:"Create realistic sensor noise models for Physical AI systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"sensor-simulation-overview",children:"Sensor Simulation Overview"}),"\n",(0,i.jsx)(n.p,{children:"In Physical AI, accurate sensor simulation is crucial for developing robust perception and control systems. Simulated sensors must replicate real-world characteristics including:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Physical limitations and constraints"}),"\n",(0,i.jsx)(n.li,{children:"Noise and uncertainty models"}),"\n",(0,i.jsx)(n.li,{children:"Temporal characteristics"}),"\n",(0,i.jsx)(n.li,{children:"Environmental dependencies"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"basic-camera-configuration",children:"Basic Camera Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="camera" type="camera">\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>\n  <camera name="head_camera">\n    <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev>\n    </noise>\n  </camera>\n  <plugin filename="libgazebo_ros_camera.so" name="camera_controller">\n    <frame_name>camera_frame</frame_name>\n    <topic_name>/camera/image_raw</topic_name>\n    <camera_info_topic_name>/camera/camera_info</camera_info_topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>\n  <camera name="depth_head">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n  </camera>\n  <plugin filename="libgazebo_ros_openni_kinect.so" name="depth_camera_controller">\n    <baseline>0.2</baseline>\n    <distortion_k1>0.0</distortion_k1>\n    <distortion_k2>0.0</distortion_k2>\n    <distortion_k3>0.0</distortion_k3>\n    <distortion_t1>0.0</distortion_t1>\n    <distortion_t2>0.0</distortion_t2>\n    <point_cloud_cutoff>0.1</point_cloud_cutoff>\n    <frame_name>depth_camera_frame</frame_name>\n    <point_cloud_topic_name>/camera/depth/points</point_cloud_topic_name>\n    <depth_image_topic_name>/camera/depth/image_raw</depth_image_topic_name>\n    <depth_image_camera_info_topic_name>/camera/depth/camera_info</depth_image_camera_info_topic_name>\n    <camera_name>depth_camera</camera_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"camera-processing-node",children:"Camera Processing Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: sensor_processing/camera_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass CameraProcessor(Node):\n    def __init__(self):\n        super().__init__('camera_processor')\n\n        self.subscription = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n\n        self.info_subscription = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.info_callback, 10)\n\n        self.object_pub = self.create_publisher(Image, '/camera/processed', 10)\n\n        self.bridge = CvBridge()\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n    def info_callback(self, msg):\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Process image (example: object detection)\n            processed_image = self.detect_objects(cv_image)\n\n            # Publish processed image\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, \"bgr8\")\n            processed_msg.header = msg.header\n            self.object_pub.publish(processed_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detect_objects(self, image):\n        # Simple color-based object detection for demonstration\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Upper red range\n        lower_red = np.array([170, 50, 50])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw bounding boxes\n        result = image.copy()\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CameraProcessor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"2d-lidar-configuration",children:"2D LiDAR Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="lidar_2d" type="ray">\n  <always_on>true</always_on>\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>360</samples>\n        <resolution>1.0</resolution>\n        <min_angle>-3.14159</min_angle> \x3c!-- -\u03c0 --\x3e\n        <max_angle>3.14159</max_angle>   \x3c!-- \u03c0 --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>10.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin filename="libgazebo_ros_ray_sensor.so" name="lidar_2d_controller">\n    <ros_topic>/scan</ros_topic>\n    <frame_name>lidar_frame</frame_name>\n    <update_rate>10</update_rate>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3d-lidar-configuration-velodyne-style",children:"3D LiDAR Configuration (Velodyne-style)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="velodyne_VLP_16" type="ray">\n  <always_on>true</always_on>\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1800</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>16</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.261799</min_angle> \x3c!-- -15 degrees --\x3e\n        <max_angle>0.261799</max_angle>   \x3c!-- 15 degrees --\x3e\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>100.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin filename="libgazebo_ros_velodyne_gpu_lidar.so" name="velodyne_VLP_16_controller">\n    <topic_name>/velodyne_points</topic_name>\n    <frame_name>velodyne</frame_name>\n    <min_range>0.1</min_range>\n    <max_range>100.0</max_range>\n    <gaussian_noise>0.01</gaussian_noise>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"lidar-processing-node",children:"LiDAR Processing Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: sensor_processing/lidar_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom std_msgs.msg import String\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\n\nclass LiDARProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n\n        self.subscription = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10)\n\n        self.obstacle_pub = self.create_publisher(String, '/obstacle_detection', 10)\n\n    def scan_callback(self, msg):\n        # Convert laser scan to points\n        points = self.laser_to_points(msg)\n\n        # Detect obstacles\n        obstacles = self.detect_obstacles(points)\n\n        # Publish obstacle information\n        obstacle_msg = String()\n        obstacle_msg.data = f'Detected {len(obstacles)} obstacles'\n        self.obstacle_pub.publish(obstacle_msg)\n\n        self.get_logger().info(f'Processed scan: {len(obstacles)} obstacles detected')\n\n    def laser_to_points(self, scan):\n        points = []\n        angle = scan.angle_min\n\n        for range_val in scan.ranges:\n            if scan.range_min <= range_val <= scan.range_max:\n                x = range_val * np.cos(angle)\n                y = range_val * np.sin(angle)\n                points.append([x, y])\n            angle += scan.angle_increment\n\n        return np.array(points)\n\n    def detect_obstacles(self, points):\n        if len(points) < 2:\n            return []\n\n        # Use DBSCAN clustering to group nearby points\n        clustering = DBSCAN(eps=0.3, min_samples=3).fit(points)\n        labels = clustering.labels_\n\n        # Group points by cluster (obstacle)\n        obstacles = {}\n        for i, label in enumerate(labels):\n            if label != -1:  # -1 is noise in DBSCAN\n                if label not in obstacles:\n                    obstacles[label] = []\n                obstacles[label].append(points[i])\n\n        return obstacles\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LiDARProcessor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"imu-sensor-configuration",children:"IMU Sensor Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>2e-4</stddev>\n          <bias_mean>0.0000075</bias_mean>\n          <bias_stddev>0.0000008</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>2e-4</stddev>\n          <bias_mean>0.0000075</bias_mean>\n          <bias_stddev>0.0000008</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>2e-4</stddev>\n          <bias_mean>0.0000075</bias_mean>\n          <bias_stddev>0.0000008</bias_stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n          <bias_mean>0.1</bias_mean>\n          <bias_stddev>0.001</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n          <bias_mean>0.1</bias_mean>\n          <bias_stddev>0.001</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>1.7e-2</stddev>\n          <bias_mean>0.1</bias_mean>\n          <bias_stddev>0.001</bias_stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin filename="libgazebo_ros_imu_sensor.so" name="imu_controller">\n    <topic_name>/imu/data</topic_name>\n    <body_name>base_link</body_name>\n    <update_rate>100</update_rate>\n    <gaussian_noise>0.01</gaussian_noise>\n    <frame_name>imu_link</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"imu-processing-node",children:"IMU Processing Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: sensor_processing/imu_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom std_msgs.msg import Float32\nimport numpy as np\nfrom tf_transformations import euler_from_quaternion\n\nclass IMUProcessor(Node):\n    def __init__(self):\n        super().__init__('imu_processor')\n\n        self.subscription = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n\n        self.orientation_pub = self.create_publisher(Float32, '/robot_orientation', 10)\n        self.acceleration_pub = self.create_publisher(Float32, '/robot_acceleration', 10)\n\n        self.prev_time = None\n        self.prev_orientation = None\n\n    def imu_callback(self, msg):\n        # Extract orientation from quaternion\n        orientation_q = msg.orientation\n        orientation_list = [orientation_q.x, orientation_q.y, orientation_q.z, orientation_q.w]\n        (roll, pitch, yaw) = euler_from_quaternion(orientation_list)\n\n        # Publish orientation (using yaw as primary orientation)\n        orientation_msg = Float32()\n        orientation_msg.data = yaw\n        self.orientation_pub.publish(orientation_msg)\n\n        # Calculate acceleration magnitude\n        accel = msg.linear_acceleration\n        accel_mag = np.sqrt(accel.x**2 + accel.y**2 + accel.z**2)\n\n        # Publish acceleration magnitude\n        accel_msg = Float32()\n        accel_msg.data = accel_mag\n        self.acceleration_pub.publish(accel_msg)\n\n        # Calculate angular velocity magnitude\n        ang_vel = msg.angular_velocity\n        ang_vel_mag = np.sqrt(ang_vel.x**2 + ang_vel.y**2 + ang_vel.z**2)\n\n        self.get_logger().info(f'Yaw: {yaw:.3f} rad, Accel: {accel_mag:.3f} m/s\xb2, Ang Vel: {ang_vel_mag:.3f} rad/s')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IMUProcessor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"gps-simulation",children:"GPS Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"gps-sensor-configuration",children:"GPS Sensor Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="gps_sensor" type="gps">\n  <always_on>true</always_on>\n  <update_rate>1</update_rate>\n  <plugin filename="libgazebo_ros_gps.so" name="gps_controller">\n    <topic_name>/gps/fix</topic_name>\n    <frame_name>gps_link</frame_name>\n    <update_rate>1</update_rate>\n    <gaussian_noise>0.1</gaussian_noise>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"forcetorque-sensor-simulation",children:"Force/Torque Sensor Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"forcetorque-sensor-in-joints",children:"Force/Torque Sensor in Joints"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<joint name="wrist_ft_sensor" type="revolute">\n  <parent link="forearm_link"/>\n  <child link="wrist_link"/>\n  <axis xyz="0 0 1"/>\n  <limit lower="-3.14" upper="3.14" effort="100" velocity="1.0"/>\n\n  <sensor name="wrist_force_torque" type="force_torque">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <force_torque>\n      <frame>sensor</frame>\n      <measure_direction>child_to_parent</measure_direction>\n    </force_torque>\n  </sensor>\n</joint>\n\n<plugin filename="libgazebo_ros_ft_sensor.so" name="ft_sensor_controller">\n  <topic_name>/wrist/force_torque</topic_name>\n  <joint_name>wrist_ft_sensor</joint_name>\n</plugin>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"multi-sensor-fusion-for-physical-ai",children:"Multi-Sensor Fusion for Physical AI"}),"\n",(0,i.jsx)(n.h3,{id:"sensor-fusion-node-example",children:"Sensor Fusion Node Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# File: sensor_processing/sensor_fusion.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu, Image\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom collections import deque\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n\n        # Subscribers for different sensors\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.camera_callback, 10)\n\n        # Publisher for fused state\n        self.state_pub = self.create_publisher(PoseStamped, '/fused_state', 10)\n\n        # Buffer for sensor data\n        self.scan_buffer = deque(maxlen=5)\n        self.imu_buffer = deque(maxlen=10)\n\n        self.bridge = CvBridge()\n        self.last_pose = None\n\n    def scan_callback(self, msg):\n        self.scan_buffer.append(msg)\n        self.update_fused_state()\n\n    def imu_callback(self, msg):\n        self.imu_buffer.append(msg)\n        self.update_fused_state()\n\n    def camera_callback(self, msg):\n        # Process camera data if needed\n        pass\n\n    def update_fused_state(self):\n        if not self.scan_buffer or not self.imu_buffer:\n            return\n\n        # Example: simple state estimation using sensor data\n        latest_scan = self.scan_buffer[-1]\n        latest_imu = self.imu_buffer[-1]\n\n        # Calculate position from IMU integration (simplified)\n        # In practice, you'd use more sophisticated fusion algorithms\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = 'map'\n\n        # For now, just publish a placeholder\n        pose_msg.pose.position.x = 0.0\n        pose_msg.pose.position.y = 0.0\n        pose_msg.pose.position.z = 0.0\n\n        # Publish the fused state\n        self.state_pub.publish(pose_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorFusionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"physical-ai-sensor-considerations",children:"Physical AI Sensor Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"realism-vs-performance",children:"Realism vs. Performance"}),"\n",(0,i.jsx)(n.p,{children:"When simulating sensors for Physical AI:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Noise Modeling"}),": Include realistic noise patterns that match real sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency"}),": Account for sensor processing and communication delays"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Field of View"}),": Match real sensor specifications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Update Rates"}),": Use rates that match real hardware capabilities"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"environmental-effects",children:"Environmental Effects"}),"\n",(0,i.jsx)(n.p,{children:"Sensors in Physical AI systems must account for environmental conditions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example: Camera with environmental effects --\x3e\n<sensor name="weather_camera" type="camera">\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>\n  <camera name="weather_cam">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>50.0</far>\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>\n    </noise>\n  </camera>\n  \x3c!-- Add environmental effects plugin --\x3e\n  <plugin filename="libgazebo_ros_camera.so" name="weather_camera_controller">\n    <frame_name>weather_camera_frame</frame_name>\n    <topic_name>/weather_camera/image_raw</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"validation-and-calibration",children:"Validation and Calibration"}),"\n",(0,i.jsx)(n.h3,{id:"comparing-simulated-vs-real-sensors",children:"Comparing Simulated vs. Real Sensors"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Example: Sensor validation node\nclass SensorValidator(Node):\n    def __init__(self):\n        super().__init__('sensor_validator')\n\n        # Subscribe to both simulated and real sensors\n        self.sim_scan_sub = self.create_subscription(\n            LaserScan, '/sim_scan', self.sim_scan_callback, 10)\n        self.real_scan_sub = self.create_subscription(\n            LaserScan, '/real_scan', self.real_scan_callback, 10)\n\n        self.error_pub = self.create_publisher(Float32, '/sensor_error', 10)\n\n        self.scan_pairs = []\n\n    def sim_scan_callback(self, msg):\n        # Store simulated scan with timestamp\n        pass\n\n    def real_scan_callback(self, msg):\n        # Store real scan and compare with closest simulated scan\n        pass\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-physical-ai-sensor-simulation",children:"Best Practices for Physical AI Sensor Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"accuracy-considerations",children:"Accuracy Considerations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use realistic noise models based on real sensor specifications"}),"\n",(0,i.jsx)(n.li,{children:"Include sensor limitations (range, resolution, field of view)"}),"\n",(0,i.jsx)(n.li,{children:"Model environmental effects (lighting, weather, etc.)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use appropriate update rates for different sensor types"}),"\n",(0,i.jsx)(n.li,{children:"Optimize sensor configurations based on application needs"}),"\n",(0,i.jsx)(n.li,{children:"Balance realism with simulation performance"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Validate sensor data consistency across modalities"}),"\n",(0,i.jsx)(n.li,{children:"Test sensor failure scenarios"}),"\n",(0,i.jsx)(n.li,{children:"Ensure proper coordinate frame transformations"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,i.jsx)(n.p,{children:"Simulating sensors accurately is crucial for Physical AI development. This chapter covered configuring cameras, LiDAR, IMU, and other sensors in Gazebo, integrating them with ROS 2, and processing their data. Proper sensor simulation enables safe testing of perception and control algorithms before deployment to real hardware."}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Create a simulation world with multiple sensor types and validate their ROS 2 topics."}),"\n",(0,i.jsx)(n.li,{children:"Implement a simple sensor fusion algorithm combining camera and LiDAR data."}),"\n",(0,i.jsx)(n.li,{children:"Add realistic noise models to your simulated sensors based on real hardware specifications."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, we'll explore Unity for high-fidelity rendering and how it complements Gazebo for Physical AI applications requiring photorealistic simulation."})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>t});var s=a(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);